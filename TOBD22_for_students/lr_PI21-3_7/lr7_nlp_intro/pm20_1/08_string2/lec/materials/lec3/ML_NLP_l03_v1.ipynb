{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция NLP 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расстояние между словами\n",
    "\n",
    "* Расстояние Левенштейна\n",
    "* Задача динамического программирования\n",
    "* Алгоритм поиска расстояния Левинштайна (The minimum edit distance algorithm was named by Wagner and Fischer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расстояние Левенштейна\n",
    "\n",
    "Часто требуется понять, насколько близкими являются два не совпадающих слова (строки). Это может потребоваться для:\n",
    "* поиска ошибок и опечаток в слове\n",
    "* поиска словоформ слова\n",
    "* для сравнения предложений и текстов\n",
    "* в других областях (в биоинформатике для сравнения генов, хромосом и белков)\n",
    "\n",
    "__Расстояние Левенштейна__ (редакционное расстояние, дистанция редактирования) - __минимальное__ количество операций необходимых для превращения одной строки в другую. Рассматриваются следующие операции:\n",
    "* вставка одного символа\n",
    "* удаление одного символа \n",
    "* замена одного символа на другим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> \n",
    "__Пример выполнения операций вставки, удаления и замены для слова \"intention\" :__\n",
    "\n",
    "<img src=\"levinst1.png\" alt=\"Пример выполнения операций вставки, удаления и замены\" style=\"width: 650px;\"/>\n",
    "\n",
    "\n",
    "__Пример преобразования слова \"intention\" в \"execution\" с помощью операций вставки, удаления и замены:__\n",
    "\n",
    "<img src=\"levinst2.png\" alt=\"Пример закона Ципфа\" style=\"width: 400px;\"/>\n",
    "</center>\n",
    "\n",
    "В общем случае __стоимость различных операций__ может быть различной. Цена операций могут зависеть от вида операции (вставка, удаление, замена) и/или от участвующих в ней символов, отражая разную вероятность этих событий.\n",
    "\n",
    "Если к списку разрешённых операций добавить __транспозицию__ (два соседних символа меняются местами), получается __расстояние Дамерау - Левенштейна__. Дамерау показал, что 80 % ошибок при наборе текста человеком являются транспозициями. Кроме того, это расстояние используется и в биоинформатике. Для поиска расстояния Левинштайна и расстояния Дамрау - Левинштайна существуют эффективные алгоритм, требующий $O(MN)$ операций ($M$ и $N$ это длины первой и второй строки соответственно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\qquad\\operatorname{D}(i,j) = \\begin{cases}   \\max(i,j) & \\text{ if } \\min(i,j)=0, \\\\   \\min \\begin{cases}           \\operatorname{D}(i-1,j) + 1 \\\\           \\operatorname{D}(i,j-1) + 1 \\\\           \\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j])        \\end{cases} & \\text{ otherwise.} \\end{cases}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Здесь и далее считается, что элементы строк нумеруются с первого, как принято в математике.\n",
    "\n",
    "Пусть $S_1$ и $S_2$ - две строки (длиной $M$ и $N$ соответственно) над некоторым алфавитом, тогда расстояние Левенштейна $\\operatorname{d}(S_1,S_2) можно подсчитать по следующей рекуррентной формуле:\n",
    "\n",
    "$$\\ \\operatorname{d}(S_1, S_2) = \\operatorname{D}(M,N)$$\n",
    "\n",
    "$$\\qquad\\operatorname{D}(i,j) = \\begin{cases}\n",
    "  \\max(i,j) & \\text{ if } \\min(i,j)=0, \\\\\n",
    "  \\min \\begin{cases}\n",
    "          \\operatorname{D}(i-1,j) + 1 \\\\\n",
    "          \\operatorname{D}(i,j-1) + 1 \\\\\n",
    "          \\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j])\n",
    "       \\end{cases} & \\text{ otherwise.}\n",
    "\\end{cases}$$\n",
    "\n",
    "Цена операции замены зависит от заменяемых символов:\n",
    "\n",
    "$$\\operatorname{m}(s_1, s_2) = \\begin{cases}\n",
    "0 \\text{ , if } s_1 = s_2 \\\\\n",
    "2 \\text{ , if } s_1 \\neq s_2 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Очевидно, что для расстояния Левинштайна справедливы следующие утверждения:\n",
    "* $\\operatorname{d}(S_1,S_2) \\geqslant \\bigl| |S_1| - |S_2| \\bigr|$\n",
    "* $\\operatorname{d}(S_1,S_2) \\leqslant \\max\\bigl( |S_1| , |S_2| \\bigr)$\n",
    "* $\\operatorname{d}(S_1,S_2) = 0 \\Leftrightarrow S_1 = S_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Редакционным предписанием__ называется последовательность действий, необходимых для получения из первой строки второй кратчайшим образом. Обычно действия обозначаются так: D (англ. delete) — удалить, I (англ. insert) — вставить, R (replace) — заменить, M (match) — совпадение.\n",
    "\n",
    "По сути редакционное предписание это кратчайшие пути на графе с весами, в котором существует 3 вида ориентированных ребер (D, I, M), а вершинами являются строки (слова). В общем случае для конкретной пары слов может существовать несколько редакционных предписаний (кратчайших путей на графе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Динамическое программирование\n",
    "\n",
    "__Динамическое программирование__ - способ решения сложных задач путём разбиения их на более простые подзадачи. Он применим к _задачам с оптимальной подструктурой_, выглядящим как _набор перекрывающихся подзадач_, сложность которых чуть меньше исходной. В этом случае время вычислений, по сравнению с «наивными» методами, можно значительно сократить.\n",
    "\n",
    "__Идея динамического программирования:__\n",
    "\n",
    "_Оптимальная подструктура_ в динамическом программировании означает, что оптимальное решение подзадач меньшего размера может быть использовано для решения исходной задачи. \n",
    "\n",
    "В общем случае мы можем решить задачу, в которой присутствует оптимальная подструктура, проделывая следующие три шага.\n",
    "\n",
    "1. Разбиение задачи на подзадачи меньшего размера.\n",
    "2. Нахождение оптимального решения подзадач рекурсивно, проделывая такой же трехшаговый алгоритм.\n",
    "3. Использование полученного решения подзадач для конструирования решения исходной задачи.\n",
    "\n",
    "Часто многие из рассматриваемых подзадач одинаковы. Подход динамического программирования состоит в том, чтобы _решить каждую подзадачу только один раз_, сократив тем самым количество вычислений. Это особенно полезно в случаях, когда число повторяющихся подзадач экспоненциально велико.\n",
    "\n",
    "* Метод динамического программирования __сверху-вниз__ (top-down approach) - это простое _запоминание результатов решения тех подзадач_, которые могут повторно встретиться в дальнейшем. \n",
    "* Динамическое программирование __снизу-вверх__ (bottom-up approach) включает в себя переформулирование сложной задачи в виде рекурсивной последовательности более простых подзадач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм Вагнера - Фишера\n",
    "\n",
    "Используя рекурсивное определение расстояния Левинштайна $\\operatorname{D}(i,j)$ через расстояния для слов меньшей длины: $\\operatorname{D}(i-1,j) \\text{ , } \\operatorname{D}(i,j-1) \\text{ , } \\operatorname{D}(i-1,j-1)$ мы применим принцип динамического программирования снизу-вверх, комбинируя решения подзадач, для решения более сложной задачи. \n",
    "\n",
    "1. Для получения базового решения когда конечная строка длины 0 или исходная строка длинны 0:\n",
    "    * $\\operatorname{D}(i, 0) = i $ - используется $i$ операций удаления (на схеме операция вставки обозначается, как: \"$\\uparrow$\")\n",
    "    * $\\operatorname{D}(0, j) = j$ - используется $j$ операций вставки (на схеме операция вставки обозначается, как: \"$\\leftarrow$\")\n",
    "2. После расчета $\\operatorname{D}(i, j)$ для малых $i$ и $j$ мы рассчитываем значения расстояния для бОльших $i$ и $j$ на основе рекурсивной формулы: \n",
    "\n",
    "$$\\qquad\\operatorname{D}(i,j) =  \\min \\begin{cases}\n",
    "          \\operatorname{D}(i-1,j) + 1 \\text{, операция удаления, на схеме обозначается как: } \\uparrow\\\\\n",
    "          \\operatorname{D}(i,j-1) + 1 \\text{, операция вставки, на схеме обозначается как: } \\leftarrow\\\\\n",
    "          \\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j]) \\text{, операция замены, на схеме обозначается как: } \\nwarrow\n",
    "       \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример поиска расстояния Левинштейна для слов \"intention\" и \"execution\" с помощью алгоритма Вагнера - Фишера:__\n",
    "\n",
    "<img src=\"levinst3.png\" alt=\"Пример поиска расстояния Левинштейна для слов intention и execution\" style=\"width: 750px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Алгоритм Вагнера - Фишера для поиска расстояния Левинштейна:__\n",
    "\n",
    "<img src=\"levinst4.png\" alt=\"Алгоритм Вагнера - Фишера для поиска расстояния Левинштейна\" style=\"width: 600px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('intention', 'execution', substitution_cost=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# результат при substitution_cost=1\n",
    "edit_distance('intention', 'execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('пирвет', 'привет', substitution_cost=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# расстояние Домрау-Левинштайна:\n",
    "edit_distance('пирвет', 'привет', substitution_cost=2, transpositions=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения приложений определение расстояния Левенштейна между словами или строками обладает следующими недостатками:\n",
    "* При перестановке местами слов или частей слов получаются сравнительно большие расстояния.\n",
    "* Расстояния между совершенно разными короткими словами оказываются небольшими, в то время как расстояния между очень похожими длинными словами оказываются значительными.\n",
    "\n",
    "Другие метрики в NLTK: http://www.nltk.org/howto/metrics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задача определения частей речи (POS-tagging)\n",
    "\n",
    "* Постановка задачи\n",
    "* Формулировка\n",
    "* Алгоритм Витерби"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center> \n",
    "__Пример тегов частей речи в pymorphy2__\n",
    "</center>\n",
    "\n",
    "<table border=\"1\" class=\"docutils\">\n",
    "<colgroup>\n",
    "<col width=\"14%\">\n",
    "<col width=\"40%\">\n",
    "<col width=\"46%\">\n",
    "</colgroup>\n",
    "<thead valign=\"bottom\">\n",
    "<tr class=\"row-odd\"><th class=\"head\">Граммема</th>\n",
    "<th class=\"head\">Значение</th>\n",
    "<th class=\"head\">Примеры</th>\n",
    "</tr>\n",
    "</thead>\n",
    "<tbody valign=\"top\">\n",
    "<tr class=\"row-even\"><td>NOUN</td>\n",
    "<td>имя существительное</td>\n",
    "<td>хомяк</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>ADJF</td>\n",
    "<td>имя прилагательное (полное)</td>\n",
    "<td>хороший</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>ADJS</td>\n",
    "<td>имя прилагательное (краткое)</td>\n",
    "<td>хорош</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>COMP</td>\n",
    "<td>компаратив</td>\n",
    "<td>лучше, получше, выше</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>VERB</td>\n",
    "<td>глагол (личная форма)</td>\n",
    "<td>говорю, говорит, говорил</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>INFN</td>\n",
    "<td>глагол (инфинитив)</td>\n",
    "<td>говорить, сказать</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>PRTF</td>\n",
    "<td>причастие (полное)</td>\n",
    "<td>прочитавший, прочитанная</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>PRTS</td>\n",
    "<td>причастие (краткое)</td>\n",
    "<td>прочитана</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>GRND</td>\n",
    "<td>деепричастие</td>\n",
    "<td>прочитав, рассказывая</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>NUMR</td>\n",
    "<td>числительное</td>\n",
    "<td>три, пятьдесят</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>ADVB</td>\n",
    "<td>наречие</td>\n",
    "<td>круто</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>NPRO</td>\n",
    "<td>местоимение-существительное</td>\n",
    "<td>он</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>PRED</td>\n",
    "<td>предикатив</td>\n",
    "<td>некогда</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>PREP</td>\n",
    "<td>предлог</td>\n",
    "<td>в</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>CONJ</td>\n",
    "<td>союз</td>\n",
    "<td>и</td>\n",
    "</tr>\n",
    "<tr class=\"row-odd\"><td>PRCL</td>\n",
    "<td>частица</td>\n",
    "<td>бы, же, лишь</td>\n",
    "</tr>\n",
    "<tr class=\"row-even\"><td>INTJ</td>\n",
    "<td>междометие</td>\n",
    "<td>ой</td>\n",
    "</tr>\n",
    "</tbody>\n",
    "</table>\n",
    "\n",
    "Граммемы Opencorpora: http://opencorpora.org/dict.php?act=gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример тегов частей речи в MyStam__\n",
    "<br/>\n",
    "\n",
    "<table>\n",
    "\n",
    "<tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">A</td> <td class=\"entry doc-c-table__td\">прилагательное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">ADV</td> <td class=\"entry doc-c-table__td\">наречие</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">ADVPRO</td> <td class=\"entry doc-c-table__td\">местоименное наречие</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">ANUM</td> <td class=\"entry doc-c-table__td\">числительное-прилагательное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">APRO</td> <td class=\"entry doc-c-table__td\">местоимение-прилагательное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">COM</td> <td class=\"entry doc-c-table__td\">часть композита - сложного слова</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">CONJ</td> <td class=\"entry doc-c-table__td\">союз</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">INTJ</td> <td class=\"entry doc-c-table__td\">междометие</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">NUM</td> <td class=\"entry doc-c-table__td\">числительное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">PART</td> <td class=\"entry doc-c-table__td\">частица</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">PR</td> <td class=\"entry doc-c-table__td\">предлог</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">S</td> <td class=\"entry doc-c-table__td\">существительное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">SPRO</td> <td class=\"entry doc-c-table__td\">местоимение-существительное</td> </tr> <tr class=\"row doc-c-table__tr\"> <td class=\"entry doc-c-table__td\">V</td> <td class=\"entry doc-c-table__td\">глагол</td> </tr> \n",
    "</table>    \n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эффективное определение части речи __по виду самого слова неэффективно__ из-за лексико-морфологической омонимии, которая очень часто встречается в т.ч. в русском языке. \n",
    "\n",
    "__Лексико-морфологическая омонимия__ – совпадение словоформ двух разных лексем. Пример омонима: \n",
    "* дуло - глагол несовершенного вида, изъявительного наклонения, в единственном числе среднем роде, в прошедшем времени\n",
    "* дуло - существительное среднего рода в единственном числе, именительном падеже.\n",
    "\n",
    "Обычно контекст предложения позволяет разрешнить неоднозначность, пример: _Из окна сильно дуло._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='дуло', tag=OpencorporaTag('VERB,impf,tran neut,sing,past,indc'), normal_form='дуть', score=0.5, methods_stack=((<DictionaryAnalyzer>, 'дуло', 1211, 9),)),\n",
       " Parse(word='дуло', tag=OpencorporaTag('NOUN,inan,neut sing,nomn'), normal_form='дуло', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'дуло', 54, 0),)),\n",
       " Parse(word='дуло', tag=OpencorporaTag('NOUN,inan,neut sing,accs'), normal_form='дуло', score=0.25, methods_stack=((<DictionaryAnalyzer>, 'дуло', 54, 3),))]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('дуло')\n",
    "p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Постановка задачи POS-tagging и скрытые марковские модели\n",
    "\n",
    "$$\\hat{t}_1^n = \\underset{t_1^n}{\\arg\\max} \\ P(t_1^n \\mid w_1^n) \\text{ ,}$$\n",
    "где $w_1^n$ - последовательность слов с 1-го по n-e (обычно предложение, часто с символами начала и конца предложения); $t_1^n$ - последовательность тегов для слов.\n",
    "\n",
    "Формула Байеса: \n",
    "$$P(A \\mid B) = \\frac{P(B \\mid A)\\, P(A)}{P(B)}$$\n",
    "\n",
    "Применим формулу Байеса:\n",
    "$$\\hat{t}_1^n =  \\underset{t_1^n}{\\arg\\max} \\ \\frac{P(w_1^n \\mid t_1^n) \\, P(t_1^n)}{P(w_1^n)}   \\text{ ,}$$\n",
    "\n",
    "Т.к. набор слов в предложении $w_1^n$ не меняется при поиске $\\arg\\max$:\n",
    "$$\\hat{t}_1^n =  \\underset{t_1^n}{\\arg\\max} \\ P(w_1^n \\mid t_1^n) \\, P(t_1^n) \\text{ ,}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цепь Маркова - последовательность случайных событий с конечным (или счётным) числом исходов, для которых предполагается что будущее событие зависит только от текущего события и не зависит от более ранних событий.\n",
    "\n",
    "Например, у нас имеется имеется дискретная последовательность (цепь) дискретных случайных величин, значение каждой из случайных величин принадлежит множеству заданных состояний системы. Это множество, может быть множеством слов, тэгов частей речи, или, например, символов, кодирующих состояния погоды. В цепи Маркова делается очень сильное предположение, что на вероятность появления следующего звена цепи влияет только предшествующее звено и не влияют все более ранние события.\n",
    "\n",
    "$$P(X_{n} = i_{n} \\mid X_{n-1},\\ldots,  X_1) = P(X_{n} = i_{n} \\mid X_{n-1})$$\n",
    "\n",
    "Таким образом, в простейшем случае условное распределение последующего состояния цепи Маркова зависит только от текущего состояния и не зависит от всех предыдущих состояний (в отличие от цепей Маркова высших порядков)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Например мы рассматриваем последовательность дней и погоду в каждый из дней определяем как \"холодную\", \"теплую\" или \"жаркую\". Тогда в предположении случайной марковской цепи вероятностяь определенного типа погоды в следующий день будет определяться только типом погоды в предшествующий день.\n",
    "\n",
    "<center> \n",
    "__Пример марковской модели для трех состояний погоды__\n",
    "\n",
    "<img src=\"hmm1.png\" alt=\"Пример марковской модели для трех состояний погоды\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для задачи POS-tagging марковское предположение выливается в:\n",
    "\n",
    "$$P(t_1^n) = P(t_n \\ldots t_1)=P(t_n \\mid t_{n-1} \\ldots t_1)P(t_{n-1} \\ldots t_1) = \\prod_{i=1}^n P(t_i \\mid t_{i-1} \\ldots t_1) \\approx \\prod_{i=1}^n P(t_i \\mid t_{i-1}) \\text{ ,}$$\n",
    "\n",
    "т.е. мы допускаем, что вероятность появления тэга следующего слова зависит только от тега предыдущего слва."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Цепи Маркова полезны когда мы хотим вычислить вероятность __последовательности наблюдаемых событий__. Но во многих случаях __события, которые нас интересют, скрыты__: мы не наблюдаем их напрямую. Например, мы в явном виде не наблюдаем части речи в предложении, вместо этого мы наблюдаем в нем слова. На основании последовательности наблюдений (слов) нам нужо вывести последовательность интересующих нас скрытых событий (тэгов частей речи). Мы называем эти события скрытими, т.к. в явном виде не наблюдаем их. Решить такую задачу позволяет __скрытая марковская модель__ (hidden Markov model (HMM))\n",
    "\n",
    "Скрытая марковская модель определяется:\n",
    "* Набором скрытых состояний. В нашем случае множеством тегов частей речи $\\left \\{ T_k \\right \\}$, $k \\in 1,\\ldots, N_T$.\n",
    "* Матрицей вероятностей переходов $A$ вероятность перехода из скрытого состояния $k$ в скрытое состояние $l$. В нашем случае: $P(t_i=T_l \\mid t_{i-1}=T_k)=a_{kl}$, для $\\sum_{l=1}^{N_T} a_{kl} = 1\\text{ для }\\forall k$.\n",
    "* Вероятностями порождения наблюдений при определенных скрытых состояниях. В нашем случае наблюдения это слова из словаря $\\left \\{ W_l \\right \\}$, $l \\in 1,\\ldots, N_W$., тогда вероятность порождения слова $W_l$ для тега $T_k$: $P(w_i=W_l \\mid t_i=T_k)=b_{kl}$, для $\\sum_{l=1}^{N_W} b_{kl} = 1\\text{ для }\\forall k$.\n",
    "* Начальным распределением вероятностей для скрытых состоянияй: $\\left \\{ \\pi_k \\right \\}$, $k \\in 1,\\ldots, N_T$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример скрытой марковской модели порождения текста для трех частей речи__\n",
    "<br/>VB - глагол в основной форме, MD - модальный глагол, NN - существительное в ед. числе\n",
    "\n",
    "<img src=\"hmm2.png\" alt=\"Пример марковской модели для трех состояний погоды\" style=\"width: 550px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постановка задачи POS-tagging:\n",
    "\n",
    "$$\\hat{t}_1^n =  \\underset{t_1^n}{\\arg\\max} \\ P(w_1^n \\mid t_1^n) \\, P(t_1^n) \\text{ ,}$$\n",
    "\n",
    "в которой $_1^n$ - последовательность слов, а $t_1^n$ - последовательность тегов с 1-го по n-ый с учетом предположений скрытой марковской модели будет преобразована. \n",
    "\n",
    "Марковское допущение (вероятность появления тега заисит только от предыдущего тега и не зависит от других предыдущих тегов):\n",
    "\n",
    "$$P(t_1^n) \\approx \\prod_{i=1}^n P(t_i \\mid t_{i-1}) \\text{ ,}$$\n",
    "\n",
    "допущение скрытой модели (вероятность появления слова зависит только от собственного тега и не зависит от соседних слов и соседних тегов):\n",
    "\n",
    "$$P(w_i^n \\mid t_1^n) \\approx \\prod_{i=1}^n P(w_i \\mid t_i) \\text{ ,}$$\n",
    "\n",
    "\n",
    "таким образом:\n",
    "\n",
    "$$\\hat{t}_1^n =  \\underset{t_1^n}{\\arg\\max} \\ P(w_1^n \\mid t_1^n) \\, P(t_1^n) \\approx \\underset{t_1^n}{\\arg\\max} \\ \\prod_{i=1}^n  P(w_i \\mid t_i) P(p_i \\mid p_{i-1}) \\text{ ,}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.е. нам нужно выбрать наиболее вероятную цепочку тегов (скрытых состояний) для наблюдаемой цепочки слов. Например:\n",
    "\n",
    "\n",
    "<center> \n",
    "__Пример скрытой марковской модели для предложения \"Из окна сильно дуло\"__\n",
    "\n",
    "<img src=\"hmm3.png\" alt=\"Пример скрытой марковской модели для предложения\" style=\"width: 400px;\"/>\n",
    "</center>\n",
    "\n",
    "Простой перебор цепочек тегов (скрытых состояний) для поиска наиболее вероятной цепочки дает __экспоненциальный рост сложности__ задачи при увеличении длины предложения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для поиска решения нам нужно знать коэффициенты матрицы вероятностей переходов между тегами (скрытыми состояниями) и вероятности порождения слов (наблюдений) при определенных тегах (скрытых состояниях). Для этого нам необходимо воспользоваться корпусом с размеченными тегами. Тогда:\n",
    "\n",
    "Вероятности переходов между тегами :\n",
    "\n",
    "$$P(t_i \\mid t_{i-1}) = \\frac {C(t_{i-1}, t_i)} {C(t_{i-1})}$$\n",
    "\n",
    "Вероятности порождения слов при определенных тегах:\n",
    "\n",
    "$$P(w_i \\mid t_i) = \\frac {C(t_i, w_i)} {C(t_i)}$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Алгоритм Витерби адаптирует принцип динамического программирования для поиска наиболее вероятной цепочки скрытых состояний в скрытой марковской модели. Он позволяет искать наиболее вероятную цепочку за линейной веремя относительно длины предложения.\n",
    "\n",
    "<center> \n",
    "<br/>\n",
    "__Алгоритм Витерби__\n",
    "\n",
    "<img src=\"hmm4.png\" alt=\"Алгоритм Витерби\" style=\"width: 550px;\"/>\n",
    "</center>\n",
    "\n",
    "Поиск параметра максимизирующего произведение удобно переформулировать в виде поиска параметра максимизирующего сумму логарифмов:\n",
    "\n",
    "$$\\hat{t}_1^n = \\underset{t_1^n}{\\arg\\max} \\ \\prod_{i=1}^n  P(w_i \\mid t_i) P(p_i \\mid p_{i-1}) = \\underset{t_1^n}{\\arg\\max} \\ \\sum_{i=1}^n  \\left ( \\log P(w_i \\mid t_i) + \\log P(p_i \\mid p_{i-1}) \\right )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Рассмотрим пример применения алгоритма Витерби для предложения: \"The bear is on the move\"\n",
    "\n",
    "<center> \n",
    "<br/>\n",
    "__Пример исходных данных для Алгоритма Витерби__\n",
    "\n",
    "<img src=\"hmm5.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__0й Шаг Алгоритма Витерби (пример)__\n",
    "<br/> заполнение начальных значений    \n",
    "\n",
    "<img src=\"hmm6.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__1й Шаг Алгоритма Витерби для первого тега (пример)__\n",
    "<br/> выбор для первого слова $\\max$ и $\\arg\\max$ для одного из тегов\n",
    "\n",
    "<img src=\"hmm7.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__1й Шаг Алгоритма Витерби (пример)__\n",
    "<br/> выбор для первого слова $\\max$ и $\\arg\\max$ для всех тегов \n",
    "\n",
    "<img src=\"hmm8.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Результат прямого прохода Алгоритма Витерби (пример)__\n",
    "<br/> найдена вероятность для самой вероятной цепочки  \n",
    "\n",
    "<img src=\"hmm9.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Результат Алгоритма Витерби (пример)__\n",
    "<br/> выведена самая вероятная цепочка\n",
    "\n",
    "<img src=\"hmm10.png\" alt=\"Шаг Алгоритма Витерби\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "The Hidden Markov Model\n",
    "A Markov chain is useful when we need to compute a probability for a sequence\n",
    "of observable events. In many cases, however, the events we are interested in are\n",
    "hidden hidden: we don’t observe them directly. For example we don’t normally observe\n",
    "part-of-speech tags in a text. Rather, we see words, and must infer the tags from the\n",
    "word sequence. We call the tags hidden because they are not observed.\n",
    "MarkovHidden A hidden Markov model (HMM) allows us to talk about both observed events\n",
    "\n",
    "model\n",
    "\n",
    "(like words that we see in the input) and hidden events (like part-of-speech tags) that\n",
    "we think of as causal factors in our probabilistic model. An HMM is speciﬁed by\n",
    "the following components:"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
