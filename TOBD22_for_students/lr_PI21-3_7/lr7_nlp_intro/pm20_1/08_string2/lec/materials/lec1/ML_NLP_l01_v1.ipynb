{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сегментация текста\n",
    "\n",
    "* Определение границ предложения \n",
    "* Токенизация "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Определение границ предложений\n",
    "\n",
    "* Мотивация\n",
    "* Как автоматически определять границы предложений? \n",
    "    * Обычно определяются по точке \n",
    "    * Точка - имеет много значений: \n",
    "        * граница предложения\n",
    "        * сокращение: “Dr.”, “U.S.A.” \n",
    "        * Разделитель в числах 3.14 \n",
    "        * ... \n",
    "    * Предложение может заканчиваться не только точкой:\n",
    "        * ./!/?/!?/!!!/... и т.д.\n",
    "        * разбор предложений с прямой и косвенной речью\n",
    "        * разбор предложений со списками \n",
    "        * на конце предложения может вообще не быть знаков препинания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Определение границ предложений\n",
    "\n",
    "* Рассмотрим задачу разрешения многозначности точки \n",
    "* Задача сводится к классификации точки на два класса: конец предложения или нет. Есть два подхода:\n",
    "    * Подход основанный на правилах (rule-based). Пример правил:\n",
    "        * перед точкой и после нее стоят цифры -> не к.п.\n",
    "        * слово перед точкой есть в словаре сокращений -> не к.п.\n",
    "        * ... правил может быть очень много и постоянно будут находится новые исключения.\n",
    "    * Подход  основанный на машинном обучении (machine learning):\n",
    "        * решение задачи классификации одним из методов ML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение задач NLP на основе правил\n",
    "\n",
    "* Исторически __первый подход__ к задаче\n",
    "* В ранних системах правила были встроены в программный код, сейчас для записи правил __используется__ либо уже готовый __формальный язык__, либо подобный язык специально создаётся для разрабатываемого приложения.\n",
    "* (-) Правила создаются лингвистами или специалистами по проблемной области обрабатываемых текстов. Трудоемкий процесс, требующий специалистов высокой кваилификации.\n",
    "* (+) Правила обычно декларативны и легко понимаемы, поэтому их просто поддерживать: модифицировать и расширять.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Решение задач NLP на основе машинного обучения\n",
    "\n",
    "Среди методов, применяемых в рамках подхода, выделяют методы обучения с учителем (supervised), методы обучения\n",
    "без учителя (unsupervised), методы частичного обучения с учителем (bootstrapping).\n",
    "\n",
    "* (+) ML не требует ручного труда по составлению правил и сокращает время разработки систем\n",
    "* (-) ML обчно предполагает наличие подходящего размеченного корпуса текстов, что не всегда возможно. Создание такого корпуса в любом случае требует значительных объемов ручного труда.\n",
    "* (-) модели (классификаторы) непрозрачны для понимания, т. к. не имеют явной лингвистической интерпретации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Постгуманизм — рациональное мировоззрение, основанное на представлении, что эволюция человека не завершена и может быть продолжена в будущем. Эволюционное развитие должно привести к становлению постчеловека — гипотетической стадии эволюции человеческого вида, строение и возможности которого стали бы отличными от современных человеческих в результате активного использования передовых технологий преобразования человека. Постгуманизм признаёт неотъемлемыми правами совершенствование человеческих возможностей (физиологических, интеллектуальных и т. п.) и достижение физического бессмертия. В отличие от трансгуманизма, под определением постгуманизма также понимается критика классического гуманизма, подчёркивающая изменение отношения человека к себе, обществу, окружающей среде и бурно развивающимся технологиям, но окончательно разница между транс- и постгуманизмом не определена и остаётся предметом дискуссий.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('phm.txt ') as f:\n",
    "    lines = [l for l in f]\n",
    "print(len(lines))\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Natural Language Toolkit - русский язык поддерживается только в некоторых модулях"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка модулей и наборов данных NLTK:\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Постгуманизм — рациональное мировоззрение, основанное на представлении, что эволюция человека не завершена и может быть продолжена в будущем.',\n",
       " 'Эволюционное развитие должно привести к становлению постчеловека — гипотетической стадии эволюции человеческого вида, строение и возможности которого стали бы отличными от современных человеческих в результате активного использования передовых технологий преобразования человека.',\n",
       " 'Постгуманизм признаёт неотъемлемыми правами совершенствование человеческих возможностей (физиологических, интеллектуальных и т.',\n",
       " 'п.)',\n",
       " 'и достижение физического бессмертия.',\n",
       " 'В отличие от трансгуманизма, под определением постгуманизма также понимается критика классического гуманизма, подчёркивающая изменение отношения человека к себе, обществу, окружающей среде и бурно развивающимся технологиям, но окончательно разница между транс- и постгуманизмом не определена и остаётся предметом дискуссий.']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Даже сегментация строки на предложения очень зависит от языка.\n",
    "# Так выглядит подключение токинизаторов для других языков в NLTK \n",
    "# (к сожалению в nltk.tokenize нет модуля для русского языка):\n",
    "spanish_tokenizer = nltk.data.load('tokenizers/punkt/spanish.pickle')\n",
    "spanish_tokenizer.tokenize('Hola amigo. Estoy bien.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install razdel\n",
    "# разделение на основе правил\n",
    "# https://github.com/natasha/razdel\n",
    "from razdel import sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           141,\n",
       "           'Постгуманизм — рациональное мировоззрение, основанное на представлении, что эволюция человека не завершена и может быть продолжена в будущем.'),\n",
       " Substring(142,\n",
       "           421,\n",
       "           'Эволюционное развитие должно привести к становлению постчеловека — гипотетической стадии эволюции человеческого вида, строение и возможности которого стали бы отличными от современных человеческих в результате активного использования передовых технологий преобразования человека.'),\n",
       " Substring(422,\n",
       "           590,\n",
       "           'Постгуманизм признаёт неотъемлемыми правами совершенствование человеческих возможностей (физиологических, интеллектуальных и т. п.) и достижение физического бессмертия.'),\n",
       " Substring(591,\n",
       "           914,\n",
       "           'В отличие от трансгуманизма, под определением постгуманизма также понимается критика классического гуманизма, подчёркивающая изменение отношения человека к себе, обществу, окружающей среде и бурно развивающимся технологиям, но окончательно разница между транс- и постгуманизмом не определена и остаётся предметом дискуссий.')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_0 = list(sentenize(lines[0]))\n",
    "sent_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Постгуманизм — рациональное мировоззрение, основанное на представлении, что эволюция человека не завершена и может быть продолжена в будущем.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_0[0].text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В каком виде лучше представлять результат сегментации? \n",
    "\n",
    "Простая модель: список сегментов.\n",
    "* (-) если применяется несколько способов сегментации?\n",
    "* (-) как искать в тексте исходное расположение сегмента?\n",
    "\n",
    "Более общий способ представления результата анализа текстов - __модель аннотаций__. \n",
    "\n",
    "Аннотация - в общем случае тройка: \n",
    "* начало \n",
    "* конец \n",
    "* значение (не обязательно) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Токенизация \n",
    "\n",
    "Токенизация - разбиение строки на подстроки, которые мы рассматриваем как интересующие нас группы символов (токены).\n",
    "\n",
    "В NLP под токенизацией обычно понимают разбиение текста на слова, знаки препинания и т.д."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 12, 'Постгуманизм'),\n",
       " Substring(13, 14, '—'),\n",
       " Substring(15, 27, 'рациональное'),\n",
       " Substring(28, 41, 'мировоззрение'),\n",
       " Substring(41, 42, ',')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# токенизация русскоязычного текста с помощью библиотеки razdel:\n",
    "from razdel import tokenize\n",
    "tokens = list(tokenize(lines[0]))\n",
    "tokens[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Постгуманизм',\n",
       " '—',\n",
       " 'рациональное',\n",
       " 'мировоззрение',\n",
       " ',',\n",
       " 'основанное',\n",
       " 'на',\n",
       " 'представлении',\n",
       " ',',\n",
       " 'что',\n",
       " 'эволюция',\n",
       " 'человека',\n",
       " 'не',\n",
       " 'завершена',\n",
       " 'и',\n",
       " 'может',\n",
       " 'быть',\n",
       " 'продолжена',\n",
       " 'в',\n",
       " 'будущем',\n",
       " '.',\n",
       " 'Эволюционное',\n",
       " 'развитие',\n",
       " 'должно',\n",
       " 'привести',\n",
       " 'к',\n",
       " 'становлению',\n",
       " 'постчеловека',\n",
       " '—',\n",
       " 'гипотетической',\n",
       " 'стадии',\n",
       " 'эволюции',\n",
       " 'человеческого',\n",
       " 'вида',\n",
       " ',',\n",
       " 'строение',\n",
       " 'и',\n",
       " 'возможности',\n",
       " 'которого',\n",
       " 'стали',\n",
       " 'бы',\n",
       " 'отличными',\n",
       " 'от',\n",
       " 'современных',\n",
       " 'человеческих',\n",
       " 'в',\n",
       " 'результате',\n",
       " 'активного',\n",
       " 'использования',\n",
       " 'передовых',\n",
       " 'технологий',\n",
       " 'преобразования',\n",
       " 'человека',\n",
       " '.',\n",
       " 'Постгуманизм',\n",
       " 'признаёт',\n",
       " 'неотъемлемыми',\n",
       " 'правами',\n",
       " 'совершенствование',\n",
       " 'человеческих',\n",
       " 'возможностей',\n",
       " '(',\n",
       " 'физиологических',\n",
       " ',',\n",
       " 'интеллектуальных',\n",
       " 'и',\n",
       " 'т',\n",
       " '.',\n",
       " 'п',\n",
       " '.',\n",
       " ')',\n",
       " 'и',\n",
       " 'достижение',\n",
       " 'физического',\n",
       " 'бессмертия',\n",
       " '.',\n",
       " 'В',\n",
       " 'отличие',\n",
       " 'от',\n",
       " 'трансгуманизма',\n",
       " ',',\n",
       " 'под',\n",
       " 'определением',\n",
       " 'постгуманизма',\n",
       " 'также',\n",
       " 'понимается',\n",
       " 'критика',\n",
       " 'классического',\n",
       " 'гуманизма',\n",
       " ',',\n",
       " 'подчёркивающая',\n",
       " 'изменение',\n",
       " 'отношения',\n",
       " 'человека',\n",
       " 'к',\n",
       " 'себе',\n",
       " ',',\n",
       " 'обществу',\n",
       " ',',\n",
       " 'окружающей',\n",
       " 'среде',\n",
       " 'и',\n",
       " 'бурно',\n",
       " 'развивающимся',\n",
       " 'технологиям',\n",
       " ',',\n",
       " 'но',\n",
       " 'окончательно',\n",
       " 'разница',\n",
       " 'между',\n",
       " 'транс-',\n",
       " 'и',\n",
       " 'постгуманизмом',\n",
       " 'не',\n",
       " 'определена',\n",
       " 'и',\n",
       " 'остаётся',\n",
       " 'предметом',\n",
       " 'дискуссий',\n",
       " '.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Токинизатор tok-tok простой токинизатор общего назначения. Он рассматривает только одно предложение в строке. Таким образом, только последняя точка в предложении рассматривается как токен.\n",
    "\n",
    "Tok-tok был протестирован и показал приемлемые результаты на следующих языках: English, Persian, Russian, Czech, French, German, Vietnamese, Tajik, и некоторых других. Tok-tok принимает строку в кодировке UTF-8."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Постгуманизм',\n",
       " '—',\n",
       " 'рациональное',\n",
       " 'мировоззрение',\n",
       " ',',\n",
       " 'основанное',\n",
       " 'на',\n",
       " 'представлении',\n",
       " ',',\n",
       " 'что',\n",
       " 'эволюция',\n",
       " 'человека',\n",
       " 'не',\n",
       " 'завершена',\n",
       " 'и',\n",
       " 'может',\n",
       " 'быть',\n",
       " 'продолжена',\n",
       " 'в',\n",
       " 'будущем',\n",
       " '.']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "toktok = ToktokTokenizer()\n",
    "toktok.tokenize(sent_0[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Многозначность определения токена:\n",
    "* I’m - один токен или два? \n",
    "* won’t - один токен или два? \n",
    "* т.к. - один токен или два? \n",
    "\n",
    "Разрешение таких воп зависит от целей токенизации. Или (что хуже) от применяемой библиотеки."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Задачи сегментации текста:\n",
    "\n",
    "Языки, где слова не разделяются пробелами: \n",
    "* \"\"\"两个月前遭受恐怖袭击的法国巴黎的犹太超市在装修之后周日重新开放，法国内政部长以及超市的管理者都表示，这显示了生命力要比野蛮行为更强大。\n",
    "该超市1月9日遭受枪手袭击，导致4人死亡，据悉这起事件与法国《查理周刊》杂志社恐怖袭击案有关。\n",
    "\"\"\" -> WordList(['两', '个', '月', '前', '遭受', '恐怖', '袭击', '的', '法国', '巴黎', '的', '犹太', '超市', '在', '装修', '之后', '周日', '重新', '开放', '，', '法国', '内政', '部长', '以及', '超市', '的', '管理者', '都', '表示', '，', '这', '显示', '了', '生命力', '要', '比', '野蛮', '行为', '更', '强大', '。', '该', '超市', '1', '月', '9', '日', '遭受', '枪手', '袭击', '，', '导致', '4', '人', '死亡', '，', '据悉', '这', '起', '事件', '与', '法国', '《', '查理', '周刊', '》', '杂志', '社', '恐怖', '袭击', '案', '有关', '。'])\n",
    "* В немецком языке возможны слова типа Donaudampfschifffahrtskapitän (капитан рейса, выполняемого пароходом по Дунаю), по сути состоящего из слов Dona (Дунай), Dampfschiff (пароход), Fahrt (рейс) и Kapitän (капитан). Если не выполнять разделение таких слов на составляющие слова, то документ не будет найден по запросам, содержащим слова, входящие в \"склеенные\" слова.\n",
    "\n",
    "Другие случаи:\n",
    "* #поставьмнелайк, #делайкакя, #серьгискристалламиростовнадону, ...\n",
    "* supernaturalflavors.com, babybirthdaygift.com, crosswordmagazines.com, купитьрозы.рф, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стемминг и лемматизация\n",
    "\n",
    "\n",
    "Часто необходимо обрабатывать разные формы слова одинаково. В этом случае поможет переход от словоформ к их леммам (словарным формам лексем) или основам (ядерным частям слова, за вычетом словоизменительных морфем)\n",
    "\n",
    "Например, при поиске: по запросам “кошками” и “кошкам” ожидаются одинаковые ответы.\n",
    "\n",
    "__Стемминг__ - это процесс нахождения основы слова, которая не обязательно совпадает с корнем слова.\n",
    "\n",
    "__Лемматизация__ - приведение слова к словарной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Морфология__ - это раздел лингвистики, который изучает структуру слов и их морфологические характеристики. Классическая морфология проанализирует слово _собака_ примерно так: это существительное женского рода, оно состоит из _корня_ собак и _окончания_ а, окончание показывает, что слово употреблено в единственном числе и в именительном падеже. \n",
    "\n",
    "__Компьютерная морфология__ анализирует и синтезирует слова программными средствами. В наиболее привычной формулировке под морфологическим анализом слова подразумевается:\n",
    "* определение леммы (базовой, канонической формы слова)\n",
    "* определение грамматических характеристик слова. \n",
    "\n",
    "В области автоматической обработки данных также используется термин __нормализация__, обозначающий постановку слова или словосочетания в __каноническую форму__ (грамматические характеристики исходной формы при этом не выдаются). Обратная задача, т. е . постановка леммы в нужную грамматическую форму, называется __порождением словоформы__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Стемминг отбрасывает суффиксы и окончания до неизменяемой формы слова \n",
    "\n",
    "Примеры: \n",
    "* кошка -> кошк \n",
    "* кошками -> кошк \n",
    "* пылесосы -> пылесос\n",
    "\n",
    "В школьной грамматике основой считается часть слова без окончания. В большинстве случаев она не меняется при грамматических изменениях самого слова — так ведет себя, например, основа _слон_ в словоформах: _слон, слону, слонами, слонов_. Но в некоторых словах основа может изменяться. Например, для словоформ _день, дню и дне_ основами будут ден-, дн- и дн-, такое явление называется __чередованием__. Поэтому самый популярный на сегодня подход использует псевдоосновы (или машинные основы). Это неизменяемые начальные части слов. Для слова день такой неизменяемой частью будет _д-_. Формы некоторых слов могут образовываться от разных корней. Например, у слова _ходить_ есть форма _шел_. Это называется _супплетивизмом_. \n",
    "\n",
    "__В русском языке__ супплетивизм и чередования очень распространены, поэтому __псевдоосновы часто получаются очень короткими__. __Для русского языка стемминг работает гораздо хуже, чем лемматизация__. \n",
    "\n",
    "В стемминге есть только правила обрабатывания суффиксов и, возможно, небольшие словари исключений. Существует бесплатный инструмент для написания стеммеров — Snowball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball - Наиболее распространенный стеммер из проекта Apache Lucene \n",
    "# Работает для нескольких языков, включая русский\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кошк\n",
      "кошечк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "snb_stemmer_ru = SnowballStemmer('russian')\n",
    "print(snb_stemmer_ru.stem('кошку'))\n",
    "print(snb_stemmer_ru.stem('кошечки'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['постгуманизм',\n",
       " 'рациональн',\n",
       " 'мировоззрен',\n",
       " 'основа',\n",
       " 'на',\n",
       " 'представлен',\n",
       " 'что',\n",
       " 'эволюц',\n",
       " 'человек',\n",
       " 'не',\n",
       " 'заверш',\n",
       " 'и',\n",
       " 'может',\n",
       " 'быт',\n",
       " 'продолж',\n",
       " 'в',\n",
       " 'будущ']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt = list(sentenize(lines[0]))\n",
    "tok = list(tokenize(snt[0].text))\n",
    "w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# предложение превращено в последовательность стем русских слов:\n",
    "[snb_stemmer_ru.stem(t.text) for t in tok if w.search(t.text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball использует __систему суффиксов и окончаний__ для предсказания части речи и грамматических параметров. Так как одно и\n",
    "то же окончание может принадлежать разным частям речи или различным парадигмам, его оказывается недостаточно для точного предсказания. Применение суффиксов позволяет повысить точность.\n",
    "\n",
    "Система реализовывается на языке программирования в виде большого количества условных операторов, анализирующих самый длинный постфикс и его контекст. По окончании анализа слову приписывается часть речи и набор параметров, а найденное окончание (или псевдоокончание) отрезается. В итоге, помимо параметров, система возвращает стем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "У разных слов часто совпадает основа: \n",
    "* пол : полу , пола , поле , полю , поля , пол , полем , полях , полям \n",
    "* лев : левый, левая, лев \n",
    "\n",
    "Из-за этого увеличивается многозначность и ухудшаются результаты работы приложений.\n",
    "\n",
    "Лемматизация - приведение слова к словарной форме, например: \n",
    "* кошки -> кошка \n",
    "* кошками -> кошка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Морфологические анализаторы для русского языка:\n",
    "\n",
    "|Название |Open |Доб. словари |Объем слов. |Скорость | Python? |\n",
    "|------|------|------|------|------|------|\n",
    "|AOT | Y | N | 160 тыс.| 60-90 | N |\n",
    "|MyStem | N | Y/N | >250 тыс.| 100-120 | Есть оболочка на Python |\n",
    "|Pymorphy2 | Y | N | 250 тыс.| 80-100 | Y|\n",
    "|TreeTagger | N | Y | 210 тыс.| 20-25 | N |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__MyStem__\n",
    "\n",
    "https://github.com/nlpub/pymystem3 \n",
    "\n",
    "_pip install pymystem3_\n",
    "\n",
    "Лемматизация:\n",
    "\n",
    "```python\n",
    ">>> from pymystem3 import Mystem\n",
    ">>> text = \"Красивая мама красиво мыла раму\"\n",
    ">>> m = Mystem()\n",
    ">>> lemmas = m.lemmatize(text)\n",
    ">>> print(''.join(lemmas))```\n",
    "\n",
    "Получение грамматической информации и лемм:\n",
    "\n",
    "```python\n",
    ">>> import json\n",
    ">>> from pymystem3 import Mystem\n",
    "\n",
    ">>> text = \"Красивая мама красиво мыла раму\"\n",
    ">>> m = Mystem()\n",
    ">>> lemmas = m.lemmatize(text)\n",
    "\n",
    ">>> print(\"lemmas:\", ''.join(lemmas))\n",
    ">>> print(\"full info:\", json.dumps(m.analyze(text), ensure_ascii=False, encoding='utf8'))\n",
    "\n",
    "lemmas: красивый мама красиво мыть рама\n",
    "\n",
    "full info: [{\"text\": \"Красивая\", \"analysis\": [{\"lex\": \"красивый\", \"gr\": \"A=им,ед,полн,жен\"}]}, {\"text\": \" \"}, {\"text\": \"мама\", \"analysis\": [{\"lex\": \"мама\", \"gr\": \"S,жен,од=им,ед\"}]}, {\"text\": \" \"}, {\"text\": \"красиво\", \"analysis\": [{\"lex\": \"красиво\", \"gr\": \"ADV=\"}]}, {\"text\": \" \"}, {\"text\": \"мыла\", \"analysis\": [{\"lex\": \"мыть\", \"gr\": \"V,несов,пе=прош,ед,изъяв,жен\"}]}, {\"text\": \" \"}, {\"text\": \"раму\", \"analysis\": [{\"lex\": \"рама\", \"gr\": \"S,жен,неод=вин,ед\"}]}, {\"text\": \"\\n\"}]```\n",
    "\n",
    "Морфологический анализатор, разработанный компанией Яндекс. MyStem версии 3.0 предоставляет все функции полного морфологического анализа, однако не имеет функции синтеза. Морфоанализатор MyStem базируется на словаре НКРЯ, который содержит более 200 тыс. лемм. Исходные коды MyStem являются закрытыми. MyStem производит __разрешение морфологической омонимии__ и делает разбор несловарных словоформ. Для решения этой задачи используются различные методы машинного обучения. В зависимости от входных данных MyStem снимает омонимию двумя способами: с учетом контекста и без учета контекста. Контекстное снятие омонимии является подключаемым и использует технологию MatrixNet. Основной идеей является ранжирование разборов на основе ближайших к разбираемому слов (контекстов)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__pymorphy2__ \n",
    "\n",
    "https://github.com/kmike/pymorphy2\n",
    "\n",
    "_pip install pymorphy2_\n",
    "\n",
    "Словари распространяются отдельными пакетами. Для русского языка:\n",
    "\n",
    "_pip install -U pymorphy2-dicts-ru_\n",
    "\n",
    "Есть оптимизированная версия, потребуется настроенное окружение для сборки (компилятор C/C++ и т.д.).\n",
    "\n",
    "Морфологический процессор с открытым исходным кодом, предоставляет все функции полного морфологического анализа и\n",
    "синтеза словоформ. Он умеет:\n",
    "* приводить слово к нормальной форме (например, “люди -> человек”, или “гулял -> гулять”).\n",
    "* ставить слово в нужную форму. Например, ставить слово во множественное число, менять падеж слова и т.д.\n",
    "* возвращать грамматическую информацию о слове (число, род, падеж, часть речи и т.д.)\n",
    "\n",
    "При работе используется словарь OpenCorpora; для незнакомых слов строятся гипотезы. Библиотека достаточно быстрая: в настоящий момент скорость работы - от нескольких тыс слов/сек до > 100тыс слов/сек (в зависимости от выполняемой операции, интерпретатора и установленных пакетов); потребление памяти - 10…20Мб; полностью поддерживается буква ё. Словарь OpenCorpora содержит около 250 тыс. лемм, а также является полностью открытым и регулярно пополняемым.\n",
    "\n",
    "Для анализа неизвестных слов в Pymorphy2 используются несколько методов, которые применяются последовательно. Изначально от слова отсекается префикс из набора известных префиксов и если остаток слова был найден в словаре, то отсеченный префикс приписывается к результатам разбора. Если этот метод не сработал, то аналогичные действия выполняются для префикса слова длиной от 1 до 5, даже если такой префикс является неизвестным. Затем, в случае неудачи, словоформа разбирается по окончанию. Для этого используется дополнительный автомат всех окончаний, встречающихся в словаре с имеющимися разборами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpencorporaTag('VERB,perf,intr plur,past,indc')"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод MorphAnalyzer.parse() возвращает один или несколько объектов типа Parse с информацией о том, как слово может быть разобрано.\n",
    "\n",
    "Тег - это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово - глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc). Доступные граммемы описаны тут: https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html#grammeme-docs.\n",
    "\n",
    "Далее: https://pymorphy2.readthedocs.io/en/latest/user/guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score - это оценка P(tag|word), оценка вероятности того, что данный разбор правильный.\n",
    "\n",
    "Разборы сортируются по убыванию score, поэтому везде в примерах берется первый вариант разбора из возможных. Оценки P(tag|word) помогают улучшить разбор, но их недостаточно для надежного снятия неоднозначности, как минимум по следующим причинам:\n",
    "\n",
    "то, как нужно разбирать слово, зависит от соседних слов; pymorphy2 работает только на уровне отдельных слов;\n",
    "условная вероятность P(tag|word) оценена на основе сбалансированного набора текстов; в специализированных текстах вероятности могут быть другими - например, возможно, что в металлургических текстах P(NOUN|стали) > P(VERB|стали);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='стать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='стать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стать', 904, 0),))"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#у каждого разбора есть нормальная форма, которую можно получить, обратившись к атрибутам normal_form или normalized:\n",
    "p[0].normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['постгуманизм',\n",
       " 'рациональный',\n",
       " 'мировоззрение',\n",
       " 'основать',\n",
       " 'на',\n",
       " 'представление',\n",
       " 'что',\n",
       " 'эволюция',\n",
       " 'человек',\n",
       " 'не',\n",
       " 'завершить',\n",
       " 'и',\n",
       " 'мочь',\n",
       " 'быть',\n",
       " 'продолжить',\n",
       " 'в',\n",
       " 'будущее']"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt = list(sentenize(lines[0]))\n",
    "tok = list(tokenize(snt[0].text))\n",
    "w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# предложение превращено в последовательность стем русских слов:\n",
    "pt = [morph.parse(t.text) for t in tok if w.search(t.text)] \n",
    "[w[0].normalized.word for w in pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
