{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Пример закона Ципфа](ZipfsLaw.png)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "# Image(url= \"http://my_site.com/my_picture.jpg\")\n",
    "# Image(url= \"http://my_site.com/my_picture.jpg\", width=100, height=100)\n",
    "PATH =\"./\"\n",
    "# PATH = \"/Users/reblochonMasque/Documents/Drawings/\"\n",
    "# Image(filename = PATH + \"My_picture.jpg\", width=100, height=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статистические методы в NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистика словоупотребления в текстах\n",
    "\n",
    "* Закон Ципфа\n",
    "* Стоп-слова\n",
    "* Языковые корпусы\n",
    "* Униграммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Ципфа\n",
    "\n",
    "__Закон Ципфа (Zipf's law ):__ («ранг-частота») -- эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (так называемому рангу этого слова. \n",
    "\n",
    "Например: \n",
    "* второе по используемости слово встречается примерно в два раза реже, чем первое\n",
    "* третье - в три раза реже, чем первое (и так далее ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример (распределение частот слов в статьях русской Википедии):__\n",
    "\n",
    "<img src=\"ZipfsLaw.png\" alt=\"Пример закона Ципфа\" style=\"width: 500px;\"/>\n",
    "\n",
    "__Пример (распределение частот слов в крупном художесвтенном произведении):__\n",
    "\n",
    "<img src=\"ZipfsLaw2.png\" alt=\"Пример закона Ципфа\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law (/zɪf/) is an empirical law formulated using mathematical statistics that refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. Zipf distribution is related to the zeta distribution, but is not identical.\n",
    "\n",
    "Закон Ципфа - эмперический закон, наблюдаемый для различных объектов в области физики, социологии, лингвистики и т.д., указывающий на то, что характеристики объектов (в частности, частота появлвения) имеют вид близкий к распределению Ципфа. \n",
    "\n",
    "Распределение Ципфа - это дискретный закон распределения, имеющий степенную природу и близкий (но не идентичный) Дзета-распределени. \n",
    "\n",
    "Пусть:\n",
    "* $N$ - количестов различных объектов (например, различных слов в тексте);\n",
    "* $k$ - ранг, т.е. порядоквый номер объекта (например, слова), в отсортированной по частоте последовательности объектов;\n",
    "* $s$ - параметр распределения, отражающий степень убывания частоты.\n",
    "\n",
    "тогда распрпеделение имеет вид:\n",
    "\n",
    "$$f(k;s,N)=\\frac{1/k^s}{\\sum\\limits_{n=1}^N (1/n^s)}$$\n",
    "\n",
    "Свойство объектов распределенных по этому закону:\n",
    "* $P_k$ - частота встречаемости объекта с рангом k\n",
    "\n",
    "$$P_k=P_1/k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В естественных языках частоты слов имеют очень тяжелые ховосты и могут описываться распределением Ципфа с $s \\to 1$ при $N \\to \\infty$ в случае если $s > 1$:\n",
    "$$\\zeta (s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}<\\infty$$\n",
    "где $\\zeta$ это Дзета-функция Римана.\n",
    "\n",
    "В этом случае распределение Ципфа можно заменить Дзета распределением (дискретным распределением, в котором $k \\in [1, \\infty])$): \n",
    "$$P(x=k; s) = \\frac {k^{-s}} {\\zeta(s)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова\n",
    "\n",
    "* Для крупных текстов большинство слов из головы распределения обычно характеризуют язык, а не текст\n",
    "* Обычно это служебные слова, определяющие стрктуру предложения (например: предлоги, артикли, частицы), местоимения (фактически, универсальные указатели) и  самые общие понятия используемые в письменной речи\n",
    "* Во многих задачах использование наиболее частотных слов создает шум и их выгодно исключать из рассмотрения. За такими словами закрепился теримн __стоп-слова__(stop words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример стоп-слов русского языка:__\n",
    "<br/>\n",
    "(конкретный состав стоп-слов зависит от рассматриваемого корпуса текстов, длинны списка и т.д.)\n",
    "</center>\n",
    "\n",
    "<table>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt; width: 48pt;\" width=\"64\">-</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">еще</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">него</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">сказать</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">а</td>   <td style=\"background-color: #eeeeee;\">ж</td>   <td style=\"background-color: #eeeeee;\">нее</td>   <td style=\"background-color: #eeeeee;\">со</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">без</td>   <td style=\"background-color: #eeeeee;\">же</td>   <td style=\"background-color: #eeeeee;\">ней</td>   <td style=\"background-color: #eeeeee;\">совсем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">более</td>   <td style=\"background-color: #eeeeee;\">жизнь</td>   <td style=\"background-color: #eeeeee;\">нельзя</td>   <td style=\"background-color: #eeeeee;\">так</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">больше</td>   <td style=\"background-color: #eeeeee;\">за</td>   <td style=\"background-color: #eeeeee;\">нет</td>   <td style=\"background-color: #eeeeee;\">такой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будет</td>   <td style=\"background-color: #eeeeee;\">зачем</td>   <td style=\"background-color: #eeeeee;\">ни</td>   <td style=\"background-color: #eeeeee;\">там</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будто</td>   <td style=\"background-color: #eeeeee;\">здесь</td>   <td style=\"background-color: #eeeeee;\">нибудь</td>   <td style=\"background-color: #eeeeee;\">тебя</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">бы</td>   <td style=\"background-color: #eeeeee;\">и</td>   <td style=\"background-color: #eeeeee;\">никогда</td>   <td style=\"background-color: #eeeeee;\">тем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">был</td>   <td style=\"background-color: #eeeeee;\">из</td>   <td style=\"background-color: #eeeeee;\">ним</td>   <td style=\"background-color: #eeeeee;\">теперь</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">была</td>   <td style=\"background-color: #eeeeee;\">из-за</td>   <td style=\"background-color: #eeeeee;\">них</td>   <td style=\"background-color: #eeeeee;\">то</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">были</td>   <td style=\"background-color: #eeeeee;\">или</td>   <td style=\"background-color: #eeeeee;\">ничего</td>   <td style=\"background-color: #eeeeee;\">тогда</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">было</td>   <td style=\"background-color: #eeeeee;\">им</td>   <td style=\"background-color: #eeeeee;\">но</td>   <td style=\"background-color: #eeeeee;\">того</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">быть</td>   <td style=\"background-color: #eeeeee;\">иногда</td>   <td style=\"background-color: #eeeeee;\">ну</td>   <td style=\"background-color: #eeeeee;\">тоже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">в</td>   <td style=\"background-color: #eeeeee;\">их</td>   <td style=\"background-color: #eeeeee;\">о</td>   <td style=\"background-color: #eeeeee;\">только</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вам</td>   <td style=\"background-color: #eeeeee;\">к</td>   <td style=\"background-color: #eeeeee;\">об</td>   <td style=\"background-color: #eeeeee;\">том</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вас</td>   <td style=\"background-color: #eeeeee;\">кажется</td>   <td style=\"background-color: #eeeeee;\">один</td>   <td style=\"background-color: #eeeeee;\">тот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вдруг</td>   <td style=\"background-color: #eeeeee;\">как</td>   <td style=\"background-color: #eeeeee;\">он</td>   <td style=\"background-color: #eeeeee;\">три</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ведь</td>   <td style=\"background-color: #eeeeee;\">какая</td>   <td style=\"background-color: #eeeeee;\">она</td>   <td style=\"background-color: #eeeeee;\">тут</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">во</td>   <td style=\"background-color: #eeeeee;\">какой</td>   <td style=\"background-color: #eeeeee;\">они</td>   <td style=\"background-color: #eeeeee;\">ты</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вот</td>   <td style=\"background-color: #eeeeee;\">когда</td>   <td style=\"background-color: #eeeeee;\">опять</td>   <td style=\"background-color: #eeeeee;\">у</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">впрочем</td>   <td style=\"background-color: #eeeeee;\">конечно</td>   <td style=\"background-color: #eeeeee;\">от</td>   <td style=\"background-color: #eeeeee;\">уж</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">все</td>   <td style=\"background-color: #eeeeee;\">которого</td>   <td style=\"background-color: #eeeeee;\">перед</td>   <td style=\"background-color: #eeeeee;\">уже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всегда</td>   <td style=\"background-color: #eeeeee;\">которые</td>   <td style=\"background-color: #eeeeee;\">по</td>   <td style=\"background-color: #eeeeee;\">хорошо</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всего</td>   <td style=\"background-color: #eeeeee;\">кто</td>   <td style=\"background-color: #eeeeee;\">под</td>   <td style=\"background-color: #eeeeee;\">хоть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всех</td>   <td style=\"background-color: #eeeeee;\">куда</td>   <td style=\"background-color: #eeeeee;\">после</td>   <td style=\"background-color: #eeeeee;\">чего</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всю</td>   <td style=\"background-color: #eeeeee;\">ли</td>   <td style=\"background-color: #eeeeee;\">потом</td>   <td style=\"background-color: #eeeeee;\">человек</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вы</td>   <td style=\"background-color: #eeeeee;\">лучше</td>   <td style=\"background-color: #eeeeee;\">потому</td>   <td style=\"background-color: #eeeeee;\">чем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">г</td>   <td style=\"background-color: #eeeeee;\">между</td>   <td style=\"background-color: #eeeeee;\">почти</td>   <td style=\"background-color: #eeeeee;\">через</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">где</td>   <td style=\"background-color: #eeeeee;\">меня</td>   <td style=\"background-color: #eeeeee;\">при</td>   <td style=\"background-color: #eeeeee;\">что</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">говорил</td>   <td style=\"background-color: #eeeeee;\">мне</td>   <td style=\"background-color: #eeeeee;\">про</td>   <td style=\"background-color: #eeeeee;\">чтоб</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">да</td>   <td style=\"background-color: #eeeeee;\">много</td>   <td style=\"background-color: #eeeeee;\">раз</td>   <td style=\"background-color: #eeeeee;\">чтобы</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">даже</td>   <td style=\"background-color: #eeeeee;\">может</td>   <td style=\"background-color: #eeeeee;\">разве</td>   <td style=\"background-color: #eeeeee;\">чуть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">два</td>   <td style=\"background-color: #eeeeee;\">можно</td>   <td style=\"background-color: #eeeeee;\">с</td>   <td style=\"background-color: #eeeeee;\">эти</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">для</td>   <td style=\"background-color: #eeeeee;\">мой</td>   <td style=\"background-color: #eeeeee;\">сам</td>   <td style=\"background-color: #eeeeee;\">этого</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">до</td>   <td style=\"background-color: #eeeeee;\">моя</td>   <td style=\"background-color: #eeeeee;\">свое</td>   <td style=\"background-color: #eeeeee;\">этой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">другой</td>   <td style=\"background-color: #eeeeee;\">мы</td>   <td style=\"background-color: #eeeeee;\">свою</td>   <td style=\"background-color: #eeeeee;\">этом</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">его</td>   <td style=\"background-color: #eeeeee;\">на</td>   <td style=\"background-color: #eeeeee;\">себе</td>   <td style=\"background-color: #eeeeee;\">этот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ее</td>   <td style=\"background-color: #eeeeee;\">над</td>   <td style=\"background-color: #eeeeee;\">себя</td>   <td style=\"background-color: #eeeeee;\">эту</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ей</td>   <td style=\"background-color: #eeeeee;\">надо</td>   <td style=\"background-color: #eeeeee;\">сегодня</td>   <td style=\"background-color: #eeeeee;\">я</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ему</td>   <td style=\"background-color: #eeeeee;\">наконец</td>   <td style=\"background-color: #eeeeee;\">сейчас</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">если</td>   <td style=\"background-color: #eeeeee;\">нас</td>   <td style=\"background-color: #eeeeee;\">сказал</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">есть</td>   <td style=\"background-color: #eeeeee;\">не</td>   <td style=\"background-color: #eeeeee;\">сказала</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U nltk\n",
    "import nltk\n",
    "nltk.__version__\n",
    "# # nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alpha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка списков стоп-слов в NLTK:\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words = stopwords.words('russian')\n",
    "print(ru_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('phm.txt ') as f:\n",
    "#     lines = [l for l in f]\n",
    "# print(len(lines))\n",
    "# print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools as it\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ]*$') # re.compile('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens = [t.text.lower() for t in tokenize(f.read()) if w_regex.search(t.text)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266954 ['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'мне', 'отмщение', 'и', 'аз', 'воздам', 'часть', 'первая', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'все', 'смешалось', 'в', 'доме', 'облонских', 'жена', 'узнала', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'и', 'объявила', 'мужу', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'уже', 'третий', 'день', 'и', 'мучительно', 'чувствовалось', 'и', 'самими', 'супругами', 'и', 'всеми', 'членами', 'семьи', 'и', 'домочадцами', 'все', 'члены', 'семьи', 'и', 'домочадцы', 'чувствовали', 'что', 'нет', 'смысла', 'в', 'их', 'сожительстве', 'и', 'что', 'на', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'более', 'связаны', 'между', 'собой', 'чем', 'они', 'члены', 'семьи', 'и', 'домочадцы', 'облонских', 'жена', 'не', 'выходила', 'из', 'своих', 'комнат', 'мужа', 'третий', 'день', 'не', 'было', 'дома', 'дети', 'бегали', 'по', 'всему', 'дому', 'как', 'потерянные', 'англичанка', 'поссорилась', 'с', 'экономкой', 'и', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'ей', 'новое', 'место', 'повар', 'ушел', 'еще', 'вчера', 'со', 'двора', 'во', 'время', 'обеда', 'черная', 'кухарка', 'и', 'кучер', 'просили', 'расчета', 'на']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(len(book_tokens), book_tokens[:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано токенов:266954; найдено различных токенов:32569\n",
      "Содержимое: [('лев', 1), ('николаевич', 2), ('толстой', 2), ('анна', 499), ('каренина', 45), ('мне', 682), ('отмщение', 1), ('и', 12916), ('аз', 1), ('воздам', 1)]\n",
      "Самое частое слово:и,  частота слова \"анна\":499\n",
      "[('и', 12916), ('не', 6537), ('что', 5765), ('в', 5720), ('он', 5551), ('на', 3594), ('она', 3434), ('с', 3327), ('я', 3212), ('как', 2660), ('но', 2581), ('его', 2578), ('это', 2223), ('к', 1983), ('ее', 1805), ('все', 1671), ('было', 1656), ('так', 1415), ('сказал', 1412), ('а', 1391), ('то', 1388), ('же', 1325), ('ему', 1252), ('о', 1243), ('за', 1139), ('левин', 1135), ('только', 1017), ('ты', 993), ('у', 913), ('был', 901), ('по', 834), ('когда', 831), ('для', 827), ('сказала', 827), ('бы', 822), ('от', 813), ('да', 812), ('теперь', 810), ('вы', 756), ('из', 735), ('была', 728), ('еще', 699), ('ей', 689), ('мне', 682), ('кити', 661), ('они', 646), ('него', 622), ('уже', 601), ('нет', 592), ('очень', 573)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Обработано токенов:{fdist.N()}; найдено различных токенов:{fdist.B()}')\n",
    "# kvi = iter(fdist.items())\n",
    "print('Содержимое:', list(it.islice(fdist.items(), 10)))\n",
    "print(f'Самое частое слово:{fdist.max()},  частота слова \"анна\":{fdist.get(\"анна\")}')\n",
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_l0 = list(sentenize(lines[0]))\n",
    "# tokens_l0 = [list(tokenize(sent.text)) for sent in sent_l0]\n",
    "# tokens_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# # делаем плоский список и оставляем только слова приведенные к ниженму регистру:\n",
    "# wtokens_l0 = [t.text.lower() for tokens in tokens_l0 for t in tokens if w.search(t.text)]\n",
    "# print(wtokens_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'отмщение', 'аз', 'воздам', 'часть', 'первая', 'счастливые', 'семьи', 'похожи', 'друг', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'смешалось', 'доме', 'облонских', 'жена', 'узнала', 'муж', 'связи', 'бывшею', 'доме', 'объявила', 'мужу', 'жить', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'третий', 'день', 'мучительно', 'чувствовалось', 'самими', 'супругами', 'всеми', 'членами', 'семьи', 'домочадцами', 'члены', 'семьи', 'домочадцы', 'чувствовали', 'смысла', 'сожительстве', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'связаны', 'собой', 'члены', 'семьи', 'домочадцы', 'облонских', 'жена', 'выходила', 'своих', 'комнат', 'мужа', 'третий', 'день', 'дома', 'дети', 'бегали', 'всему', 'дому', 'потерянные', 'англичанка', 'поссорилась', 'экономкой', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'новое', 'место', 'повар', 'ушел', 'вчера', 'двора', 'время', 'обеда', 'черная', 'кухарка', 'кучер', 'просили', 'расчета']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words_s = set(ru_stop_words)\n",
    "# фильтруем стоп слова:\n",
    "wtokens_wostw = [w for w in book_tokens[:150] if w not in ru_stop_words_s]\n",
    "print(wtokens_wostw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 'это', 2223), (18, 'сказал', 1412), (25, 'левин', 1135), (33, 'сказала', 827), (44, 'кити', 661), (49, 'очень', 573), (53, 'вронский', 509), (56, 'анна', 499), (66, 'алексей', 429), (68, 'степан', 423), (69, 'аркадьич', 422), (72, 'александрович', 395), (81, 'время', 366), (82, 'мог', 357), (83, 'говорил', 357), (89, 'руку', 309), (90, 'долли', 302), (92, 'которые', 295), (97, 'лицо', 277), (98, 'сказать', 276), (102, 'дело', 272), (103, 'левина', 272), (108, 'который', 263), (111, 'своей', 251), (113, 'знал', 249), (116, 'жизни', 235), (117, 'говорить', 234), (118, 'знаю', 233), (121, 'которое', 231), (124, 'пред', 224), (125, 'хотел', 219), (127, 'сергей', 219), (129, 'нужно', 217), (130, 'человек', 215), (131, 'прежде', 215), (132, 'глаза', 214), (134, 'могу', 214), (135, 'видел', 214), (137, 'тебе', 213), (139, 'тотчас', 211), (141, 'чувствовал', 210), (143, 'вронского', 205), (145, 'одно', 202), (146, 'своего', 199), (147, 'могла', 199), (148, 'свое', 198), (149, 'иванович', 191), (153, 'думал', 189), (154, 'глядя', 189), (156, 'говорила', 184)]\n"
     ]
    }
   ],
   "source": [
    "print(list(it.islice(((i, w, f) for i, (w, f) in enumerate(fdist.most_common(500)) \\\n",
    "                      if w not in ru_stop_words_s), 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Языковые корпусы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус - собрание текстов, объединенных общим признаком\n",
    "\n",
    "Зачем используются корпуcы в  лингвистике? \n",
    "1. На основе корпусов создаются словари и грамматики. \n",
    "2. Корпусы помогают и в теоретической лингвистике: при исследовании какого-то явления можно быстро собрать нужные данные в их естественном контексте и проанализировать. \n",
    "3. На основе корпусов проводится машинное обучение для самых разных областей прикладной лингвистики. \n",
    "Корпусы могут пригодиться и для любых других задач, связанных с языком - однажды созданный и подготовленный корпус может использоваться многократно, различными исследователями и в разных целях. По сути, корпус - инфраструктруа для существующих и еще не появившихся задач изучения языка.\n",
    "\n",
    "Свойствак корпуса: \n",
    "1. __электронный__ - оформленный в удобном для работе формате\n",
    "2. __репрезентативный__ - корпус должен хорошо «представлять» тот объект, который он моделирует, т.е. язык)\n",
    "    * сбалансированность (balance) корпуса: если корпус - это уменьшенная модель языка, то в нем пропорционально (_какие пропорции?_) должны быть представлены текстов различных периодов, жанров, стилей, авторов и т. д. \n",
    "3. __размеченный__ - разметка заключается в приписывании текстам корпуса и их компонентам дополнительной информации, метаданных\n",
    "4. __прагматически ориентированный__ - создаваемый для определенных целей\n",
    "\n",
    "Специальные виды коропусов:\n",
    "* Параллельные корпусы - корпуса на нескольких языках\n",
    "* Устные корпусы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типы разметки корпусов: \n",
    "* __морфологическая__ - part-of-speech tagging (POS-tagging), дословно - частеречная разметка. В действительности морфологические метки включают не только признак части речи, но и признаки грамматических категорий, свойственных данной части речи, а также нормальную форму слова, лемму. \n",
    "* __синтаксическая__ - описывает синтаксические связи между лексическими единицами и/или различные синтаксические конструкции (например, придаточное предложение, именное сказуемое и т.д.) \n",
    "* __семантическая__ - обозначают семантические категории, к которым относится данное слово или слово сочетание, и более узкие подкатегории, определяющие его значение. Семантическая разметка корпусов предусматривает спецификацию значений слов, разрешение омонимии и синонимии, категоризацию слов (разряды), выделение тематических классов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример морфологической разметки корпуса:\n",
    "```xml \n",
    "<s> \n",
    "    <w>Звонили<ana lemma=\"ЗВОНИТЬ\" pos=\"Г\" gram=\"мн,нс,нп,дст,прш,\"/></w> \n",
    "    <w>к<ana lemma=\"К\" p os=\"ПР ЕДЛ\" gram=\"\" /></w> \n",
    "    <w>вечерне\n",
    "        <ana lemma=\"ВЕЧЕРНЯ\" p os=\"С\" gram=\"жр ,ед,дт,пр,но,\" /> \n",
    "        <ana lemma=\"ВЕЧЕРНИЙ\" p os=\"П\" gram=\"ср,ед,кр,\" /></w> \n",
    "    <pun>.</pun> \n",
    "</s> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные корпусы русского языка: \n",
    "* http://ruscorpora.ru - Национальный корпус русского языка (НКРЯ)\n",
    "* http://www.ling.helsinki.fi/projects/hanco - ХЕЛЬСИНКСКИЙ АННОТИРОВАННЫЙ КОРПУС (ХАНКО)\n",
    "* http://corpus.leeds.ac.uk/ruscorpora.html - Russian corpora (Ruscorpora)\n",
    "* http://aot.ru/search1.html - Автоматическая Обработка Текста (АОТ)\n",
    "* http://sketchengine.co.uk \n",
    "\n",
    "Неразмеченные текстовые корпусы:\n",
    "* Project Guttenberg (англоязычный)\n",
    "* Reuters corpora (англоязычный)\n",
    "* lib.ru\n",
    "* Web..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковые модели, N-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые приписывают последовательности слов вероятность ее появления в тексте, называются __языковыми моделями__.\n",
    "\n",
    "Задача языкового моделирования формально может быть сведена к вычислению вероятности появления слова $w_i$ при условии, что до этого появилась цепочка слов $w_1, \\dotso, w_{i−1}$ (история). \n",
    "\n",
    "Например, для изречения _\"Враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\"_ вероятностная языковая модель должна уметь предсказывать, например,  вероятность:\n",
    "\n",
    "_P = (\"преднамеренно\" | \"враг не тот, кто наносит обиду, а тот, кто делает это\")_\n",
    "\n",
    "$P(A|B)=P(AB)/P(B)$\n",
    "\n",
    "Простые методы порождения вероятностных языковых моделей, вероятность $P$ напрямую: \n",
    "\n",
    "_P = F(\"враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\")/F(\"враг не тот, кто наносит обиду, а тот, кто делает это\")_\n",
    "\n",
    "где F – абсолютная частота встречаемости выражения в корпусе текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-граммой (n-gram) называется последовательность из $n$ структурных единиц (токенов), на которые сегментирован исходный текст (слова, символы алфавита и т.д.). N-граммы, извлеченные из корпусов, чаще всего _состоят из словоформ, лексем или стем_.\n",
    "\n",
    "Если рассматривать n-граммы как последовательности слов, то:\n",
    "* отдельное слово называется __униграммой__ (\"хозяин\")\n",
    "* два слова - __биграммой__ (\"хозяин ушел\")\n",
    "* три - __триграммой__ (\"хозяин ушел домой\") \n",
    "* и т.д. \n",
    "\n",
    "Слова внутри n-граммы могут не иметь синтаксических связей между собой, единственное, что их связывает – это совместная встречаемость.\n",
    "\n",
    "Кроме n-грамм существуют коллокации:\n",
    "* __n-грамма__ - (статистически устойчивая) последовательность из n соседних слов\n",
    "* __коллокация__ - устойчивое выражение (словосочетание), слова в котором связаны друг с другом, при этом они не обязательно располагаются друг за другом. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборы данных с N-граммами:\n",
    "\n",
    "* https://books.google.com/ngrams , пример: https://books.google.com/ngrams/graph?content=%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80+%D0%9F%D1%83%D1%88%D0%BA%D0%B8%D0%BD%2C+%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80+%D0%9B%D0%B5%D0%BD%D0%B8%D0%BD%2C+%D0%AE%D1%80%D0%B8%D0%B9+%D0%93%D0%B0%D0%B3%D0%B0%D1%80%D0%B8%D0%BD&year_start=1800&year_end=2000&corpus=25&smoothing=3&share=&direct_url=t1%3B%2C%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%20%D0%9F%D1%83%D1%88%D0%BA%D0%B8%D0%BD%3B%2Cc0%3B.t1%3B%2C%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80%20%D0%9B%D0%B5%D0%BD%D0%B8%D0%BD%3B%2Cc0%3B.t1%3B%2C%D0%AE%D1%80%D0%B8%D0%B9%20%D0%93%D0%B0%D0%B3%D0%B0%D1%80%D0%B8%D0%BD%3B%2Cc0\n",
    "\n",
    "* http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
    "* http://www.ruscorpora.ru/corpora-freq.html\n",
    "* https://www.sketchengine.eu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающем корпусе мы можем:\n",
    "* для каждой n-граммы мы можем посчитать __частоту__ - т.е. сколько раз она встретилась в корпусе\n",
    "* на основе полученных данных построить __вероятностную модель__, которая затем может быть использована для оценки вероятности n-грамм в некотором тестовом корпусе\n",
    "\n",
    "Какова вероятность встретить определенное предложение в тексте? Формулировка:\n",
    "\n",
    "_P(\"враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\") = ?_\n",
    "\n",
    "Вероятность этого события можно расписать через произведение условных вероятностей:\n",
    "$$P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2) \\cdot \\dotso \\cdot P(w_n|w_1^{n−1}) = \\prod_{k=1}^{n}P(w_k|w_1^{k−1})$$\n",
    "где: $w_i$ - i-е слово; $w_i^j$ - последовательность слов с i-го до j-го.\n",
    "\n",
    "Допущение Маркова:\n",
    "$$P(w_k|w_1^{k−1}) \\approx P(w_k|w_{k−1})$$\n",
    "тогда:\n",
    "$$P(w_1^n) \\approx \\prod_{k=1}^{n}P(w_k|w_{k−1})$$\n",
    "\n",
    "Как найти условные вероятности $P(w_k|w_{k−1})$? Используем частоты:\n",
    "$$P(w_k|w_{k−1}) = \\frac{F(w_{k−1}w_k)}{\\sum_{w}F(w_{k−1}w)} = \\frac{F(w_{k−1}w_k)}{F(w_{k−1})}$$\n",
    "т.е. нам нужны частоты биграмм и униграмм в корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример. Пусть корпус состоит из трех предложений:\n",
    "* ```<s> I am Sam </s>```\n",
    "* ```<s> Sam I am </s>```\n",
    "* ```<s> I do not like green eggs and ham </s>```\n",
    "\n",
    "Тогда: P(I|<s\\>) = 2/3 ; P(am|I)=2/2; P(Sam|am)=1/2; P(</s\\>|Sam)=1/2; P(do|I)=1/3; P(Sam|<s\\>)=1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel.substring import Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ_]*$') # ('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens_ss =  [t.text.lower() for sent in list(sentenize(f.read())) \\\n",
    "    for t in it.chain([Substring(None, None, '_')], tokenize(sent.text), [Substring(None, None, '__')]) \\\n",
    "    if w_regex.search(t.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'лев', 'николаевич', 'толстой', 'анна', 'каренина', 'мне', 'отмщение', 'и', 'аз', 'воздам', 'часть', 'первая', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', '__', '_', 'все', 'смешалось', 'в', 'доме', 'облонских', '__', '_', 'жена', 'узнала', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'и', 'объявила', 'мужу', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', '__', '_', 'положение', 'это', 'продолжалось', 'уже', 'третий', 'день', 'и', 'мучительно', 'чувствовалось', 'и', 'самими', 'супругами', 'и', 'всеми', 'членами', 'семьи', 'и', 'домочадцами', '__', '_', 'все', 'члены', 'семьи', 'и', 'домочадцы', 'чувствовали', 'что', 'нет', 'смысла', 'в', 'их', 'сожительстве', 'и', 'что', 'на', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди']\n"
     ]
    }
   ],
   "source": [
    "print(book_tokens_ss[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "# from nltk.probability import FreqDist\n",
    "# fdist = FreqDist(b_t)\n",
    "from nltk.util import everygrams, skipgrams\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_', 19386), ('и', 12916), ('в', 5720), ('с', 3327), ('я', 3212), ('к', 1983), ('а', 1391), ('о', 1243), ('у', 913), ('ж', 129)]\n",
      "[('было', 1656), ('была', 728), ('кити', 661), ('него', 622), ('быть', 565), ('меня', 533), ('себя', 501), ('анна', 499), ('себе', 499), ('были', 499)]\n"
     ]
    }
   ],
   "source": [
    "cfdist = ConditionalFreqDist((len(word), word) for word in book_tokens_ss)\n",
    "print(cfdist[1].most_common()[:10])\n",
    "print(cfdist[4].most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна'),\n",
       " ('анна', 'каренина'),\n",
       " ('каренина', 'мне'),\n",
       " ('мне', 'отмщение'),\n",
       " ('отмщение', 'и'),\n",
       " ('и', 'аз'),\n",
       " ('аз', 'воздам')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Биграммы можно строить так:\n",
    "list(it.islice(zip(book_tokens_ss[:-1], book_tokens_ss[1:]), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_',),\n",
       " ('лев',),\n",
       " ('николаевич',),\n",
       " ('толстой',),\n",
       " ('анна',),\n",
       " ('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна'),\n",
       " ('_', 'лев', 'николаевич'),\n",
       " ('лев', 'николаевич', 'толстой'),\n",
       " ('николаевич', 'толстой', 'анна')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(book_tokens_ss[:5], max_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(book_tokens_ss[:5], min_len=2, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfdist2 = ConditionalFreqDist(w_2 for w_2 in  everygrams(book_tokens_ss, min_len=2, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__', 64), ('аркадьевна', 28), ('не', 21), ('и', 20), ('с', 12), ('как', 9), ('павловна', 8), ('я', 6), ('была', 6), ('вышла', 6), ('в', 6), ('уже', 6), ('ни', 4), ('ничего', 4), ('улыбнулась', 4), ('вдруг', 4), ('чувствовала', 4), ('на', 4), ('вошла', 3), ('что', 3), ('все', 3), ('очевидно', 3), ('быстро', 3), ('сказала', 3), ('села', 3), ('встала', 3), ('но', 3), ('говорила', 3), ('ради', 3), ('сказал', 3), ('опять', 3), ('спросил', 2), ('взялась', 2), ('подняла', 2), ('пожимая', 2), ('провела', 2), ('знала', 2), ('улыбаясь', 2), ('вглядываясь', 2), ('сейчас', 2)]\n",
      "\n",
      "[('__', 68), ('и', 28), ('не', 25), ('был', 13), ('с', 12), ('в', 9), ('вошел', 8), ('знал', 7), ('чувствовал', 7), ('взглянул', 6), ('улыбаясь', 5), ('уже', 5), ('еще', 5), ('но', 5), ('сказал', 4), ('подошел', 4), ('никогда', 4), ('мог', 4), ('имел', 4), ('понял', 4), ('остановился', 3), ('вышел', 3), ('глядя', 3), ('оба', 3), ('смотрел', 3), ('поехал', 3), ('видел', 3), ('она', 2), ('подумал', 2), ('встал', 2), ('со', 2), ('внимательно', 2), ('пожимая', 2), ('к', 2), ('взял', 2), ('увидал', 2), ('для', 2), ('ничего', 2), ('закричал', 2), ('весело', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(cfdist2['анна'].most_common()[:40])\n",
    "print()\n",
    "print(cfdist2['вронский'].most_common()[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfdist2['анна'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__', 64), ('аркадьевна', 28), ('павловна', 8), ('вышла', 6), ('улыбнулась', 4), ('чувствовала', 4), ('вошла', 3), ('очевидно', 3), ('быстро', 3), ('сказала', 3), ('села', 3), ('встала', 3), ('говорила', 3), ('ради', 3), ('сказал', 3), ('спросил', 2), ('взялась', 2), ('подняла', 2), ('пожимая', 2), ('провела', 2), ('знала', 2), ('улыбаясь', 2), ('вглядываясь', 2), ('вернулась', 2), ('оглядываясь', 2), ('посмотрела', 2), ('почувствовала', 2), ('очень', 2), ('видела', 2), ('смотрела', 2), ('прочла', 2), ('приехала', 2), ('тотчас', 2), ('краснея', 2), ('рядом', 2), ('заметила', 2), ('глядя', 2), ('спросила', 2), ('сощурившись', 2), ('взяла', 2)]\n",
      "\n",
      "[('__', 68), ('вошел', 8), ('знал', 7), ('чувствовал', 7), ('взглянул', 6), ('улыбаясь', 5), ('сказал', 4), ('подошел', 4), ('мог', 4), ('имел', 4), ('понял', 4), ('остановился', 3), ('вышел', 3), ('глядя', 3), ('оба', 3), ('смотрел', 3), ('поехал', 3), ('видел', 3), ('подумал', 2), ('встал', 2), ('внимательно', 2), ('пожимая', 2), ('взял', 2), ('увидал', 2), ('закричал', 2), ('весело', 2), ('слушал', 2), ('испытывал', 2), ('хмурясь', 2), ('оглянулся', 2), ('хотел', 2), ('выпьешь', 2), ('действительно', 2), ('несмотря', 2), ('получив', 2), ('назвал', 2), ('вспоминая', 2), ('поклонился', 2), ('желал', 2), ('тотчас', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(list(it.islice(((w, f) for w, f in cfdist2['анна'].most_common() if w not in ru_stop_words_s), 40)))\n",
    "print()\n",
    "print(list(it.islice(((w, f) for w, f in cfdist2['вронский'].most_common() if w not in ru_stop_words_s), 40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfdist.tabulate(['и', 'за'], ['анна', 'вронский']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_prob(s1, s2, cfd):\n",
    "    common_frq = cfd[s1][s2]\n",
    "    s1_frq = cfd[s1].N()\n",
    "    print(f'''2 слова встретились совместно (в данном порядке):{common_frq}, \n",
    "частота предшеcтвовашего слова: {s1_frq}''')    \n",
    "    return common_frq/s1_frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 слова встретились совместно (в данном порядке):28, \n",
      "частота предшеcтвовашего слова: 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.056112224448897796"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cond_prob('анна','аркадьевна', cfdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'c', 'a', 'b', 'c', 'b', 'c', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "# ['a', 'b', 'c']\n",
    "lm = MLE(2)\n",
    "lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
    "lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
    "# print(lm.generate(random_seed=3))\n",
    "print(lm.generate(num_words=10, text_seed=['a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = MLE(2)\n",
    "vals = book_tokens_ss[:]\n",
    "vocabulary_text = list(set(vals))\n",
    "ngram_2 = list(everygrams(vals, min_len=2, max_len=2))\n",
    "# print(vals,text, vocabulary_text, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.fit([ngram_2], vocabulary_text)\n",
    "lm2.fit([[(t,) for t in vals]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['левин', 'точно', 'перламутровую', 'раковину', 'которою', 'он', '__', '_', 'опять', 'сложив', 'письмо', '__', '_', 'ну', 'да', 'я', 'хотел', 'вспоминать', 'об', 'удобствах']\n"
     ]
    }
   ],
   "source": [
    "print(lm2.generate(num_words=20, text_seed=['_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = MLE(3)\n",
    "vals = book_tokens_ss[:]\n",
    "vocabulary_text = list(set(vals))\n",
    "ngram_3 = list(everygrams(vals, min_len=3, max_len=3))\n",
    "# print(vals,text, vocabulary_text, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.fit([ngram_3], vocabulary_text)\n",
    "lm3.fit([[(t,) for t in vals]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ввел', '_', 'в', 'четверг', 'ветер', 'затих', 'и', 'надвинулся', 'густой', 'серый', 'туман', 'как', 'бы', 'снимал', 'с', 'нее', 'глаз', '__', '_', 'тихо', 'и', 'медленно', 'выговаривая', 'слова', 'но', 'с', 'твоим', 'монашеством', 'и', 'строгим', 'лицом', 'приказывавшую', 'слуге', '__', '_', 'нет', 'все', 'более', 'и', 'более', 'озлобляясь', 'кричал', 'громче', 'и', 'громче', '__', '_', 'я', 'все', 'сделаю']\n"
     ]
    }
   ],
   "source": [
    "print(lm3.generate(num_words=50, text_seed=['_']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимость в сглаживании:\n",
    "* Из-за разреженности языка\n",
    "* Огранниченность размера корпуса\n",
    "    * занижена вероятность\n",
    "    * вероятность равна нулю\n",
    "    \n",
    "Сглаживание - повышение вероятности некоторых n-грам, за счет понижения вероятности других.\n",
    "* (*) Сглаживание Лапласа\n",
    "* (*) Откат (backoff)\n",
    "* Сглаживание Кнесера-Нея (Kneser-Ney)\n",
    "* Сглаживание Виттена-Белла (Witten-Bell)\n",
    "* Сглаживание Гуда-Тьюринга (Good-Turing)\n",
    "* Интерполяция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание Лапласа:\n",
    "\n",
    "Добавим 1 к встречаемости каждой n-граммы. Пусть в словаре V слов, тогда:\n",
    "\n",
    "$$P(w_k|w_{k−1}) = \\frac{F(w_{k−1}w_k)+1}{F(w_{k−1})+V}$$\n",
    "\n",
    "Практическое применение Сглаживания Лапласа:\n",
    "* Метод провоцирует сильную погрешность в вычислениях \n",
    "* Тесты показываюь, что не сглаженная модель часто показывает более точные результаты\n",
    "* Мтод интересен только с теоретической точки зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откат (backoff):\n",
    "\n",
    "* Основная идея: можно оценивать вероятности n-грамм с помощью вероятностей (n-k)-грамм (0<k<n).\n",
    "* Особенность: метод можно сочетать с другими алгоритмами сглаживания (Witten-Bell, Good-Turing и т. д.)\n",
    "\n",
    "Оценка вероятности в случае триграмм:\n",
    "\n",
    "$$\\hat{P}(w_k|w_{k−2}w_{k−1}) = \\begin{cases}\n",
    " & P(w_k|w_{k−2}w_{k−1}) \\text{ , } F(w_{k−2}w_{k−1}w_k)>0 \\\\ \n",
    " & \\alpha(w_{k−2}^{k-1})\\hat{P}(w_k|w_{k−1}) \\text{ , } F(w_{k−2}w_{k−1}w_k)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Коэффициент $\\alpha$ необходим для корректного распределения остаточной вероятности n-грамм в соответствии с распределением вероятности (n-1)-грамм.\n",
    "* Если не вводить $\\alpha$, оценка будет ошибочной, т.к. не будет выполняться равенство:\n",
    "$$\\sum_{i,j} P(w_k|w_i w_j) = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оценки качества моделей\n",
    "\n",
    "Как понять, что одна модель лучше другой?\n",
    "* Внешняя оценка (in vivo): как изменение параметра модели влияет на качество решения задачи\n",
    "* Внутренняя оценка (in vitro): коэффициент неопределенности (perplexity)\n",
    "\n",
    "Коэффициент неопределенности\n",
    "* Основан на теории информации\n",
    "* Лучше та модель, которая лучше предсказывает детали тестовой коллекции\n",
    "\n",
    "$$ PP(w) = P(w_1 w_2 \\dotso w_N )^{-1/N}$$\n",
    "\n",
    "Для биграмм:\n",
    "$$ PP(w) = \\left ( \\prod_{i=1}^{N} P(w_i | w_{i-1} )\\right )^{-1/N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.nltk.org/api/nltk.lm.html#module-nltk.lm.models\n",
    "lm2.counts['анна']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.counts[['анна']]['была']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you get the score for a word given some preceding context. For example we want to know what is the chance that “b” is preceded by “a”.\n",
    "\n",
    "```python\n",
    ">>> lm.score(\"b\", [\"a\"])\n",
    "0.5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016321804491603593"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, the real purpose of training a language model is to have it score how probable words are in certain contexts. \n",
    "# This being MLE, the model returns the item’s relative frequency as its score.\n",
    "lm2.score(\"анна\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.258983718334655"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.logscore(\"анна\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012024048096192385"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here’s how you get the score for a word given some preceding context. \n",
    "# For example we want to know what is the chance that “была” is preceded by [\"анна\"].\n",
    "lm2.score('была', ['анна'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.0596133731282"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [('анна', 'была'), ('была', 'замужем')]\n",
    "lm2.perplexity(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.948623364641533"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.entropy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [(\"замужем\",'анна'), ('анна', 'была')]\n",
    "lm2.perplexity(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
