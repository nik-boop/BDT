{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#![Пример закона Ципфа](ZipfsLaw.png)\n",
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML \n",
    "# Image(url= \"http://my_site.com/my_picture.jpg\")\n",
    "# Image(url= \"http://my_site.com/my_picture.jpg\", width=100, height=100)\n",
    "PATH =\"./\"\n",
    "# PATH = \"/Users/reblochonMasque/Documents/Drawings/\"\n",
    "# Image(filename = PATH + \"My_picture.jpg\", width=100, height=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Статистические методы в NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Статистика словоупотребления в текстах\n",
    "\n",
    "* Закон Ципфа\n",
    "* Языковые корпусы \n",
    "* Стоп-слова\n",
    "* Языковые корпусы\n",
    "* Языковые модели, N-граммы\n",
    "* Поиск словосочетаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Закон Ципфа\n",
    "\n",
    "__Закон Ципфа (Zipf's law ):__ («ранг-частота») -- эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (так называемому рангу этого слова. \n",
    "\n",
    "Например: \n",
    "* второе по используемости слово встречается примерно в два раза реже, чем первое\n",
    "* третье - в три раза реже, чем первое (и так далее ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример (распределение частот слов в статьях русской Википедии):__\n",
    "\n",
    "<img src=\"ZipfsLaw.png\" alt=\"Пример закона Ципфа\" style=\"width: 500px;\"/>\n",
    "\n",
    "__Пример (распределение частот слов в крупном художесвтенном произведении):__\n",
    "\n",
    "<img src=\"ZipfsLaw2.png\" alt=\"Пример закона Ципфа\" style=\"width: 400px;\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zipf's law (/zɪf/) is an empirical law formulated using mathematical statistics that refers to the fact that many types of data studied in the physical and social sciences can be approximated with a Zipfian distribution, one of a family of related discrete power law probability distributions. Zipf distribution is related to the zeta distribution, but is not identical.\n",
    "\n",
    "Закон Ципфа - эмперический закон, наблюдаемый для различных объектов в области физики, социологии, лингвистики и т.д., указывающий на то, что характеристики объектов (в частности, частота появлвения) имеют вид близкий к распределению Ципфа. \n",
    "\n",
    "Распределение Ципфа - это дискретный закон распределения, имеющий степенную природу и близкий (но не идентичный) Дзета-распределени. \n",
    "\n",
    "Пусть:\n",
    "* $N$ - количестов различных объектов (например, различных слов в тексте);\n",
    "* $k$ - ранг, т.е. порядоквый номер объекта (например, слова), в отсортированной по частоте последовательности объектов;\n",
    "* $s$ - параметр распределения, отражающий степень убывания частоты.\n",
    "\n",
    "тогда распрпеделение имеет вид:\n",
    "\n",
    "$$f(k;s,N)=\\frac{1/k^s}{\\sum\\limits_{n=1}^N (1/n^s)}$$\n",
    "\n",
    "Свойство объектов распределенных по этому закону:\n",
    "* $P_k$ - частота встречаемости объекта с рангом k\n",
    "\n",
    "$$P_k=P_1/k$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В естественных языках частоты слов имеют очень тяжелые ховосты и могут описываться распределением Ципфа с $s \\to 1$ при $N \\to \\infty$ в случае если $s > 1$:\n",
    "$$\\zeta (s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}<\\infty$$\n",
    "где $\\zeta$ это Дзета-функция Римана.\n",
    "\n",
    "В этом случае распределение Ципфа можно заменить Дзета распределением (дискретным распределением, в котором $k \\in [1, \\infty])$): \n",
    "$$P(x=k; s) = \\frac {k^{-s}} {\\zeta(s)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Стоп-слова\n",
    "\n",
    "* Для крупных текстов большинство слов из головы распределения обычно характеризуют язык, а не текст\n",
    "* Обычно это служебные слова, определяющие стрктуру предложения (например: предлоги, артикли, частицы), местоимения (фактически, универсальные указатели) и  самые общие понятия используемые в письменной речи\n",
    "* Во многих задачах использование наиболее частотных слов создает шум и их выгодно исключать из рассмотрения. За такими словами закрепился теримн __стоп-слова__(stop words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "__Пример стоп-слов русского языка:__\n",
    "<br/>\n",
    "(конкретный состав стоп-слов зависит от рассматриваемого корпуса текстов, длинны списка и т.д.)\n",
    "</center>\n",
    "\n",
    "<table>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt; width: 48pt;\" width=\"64\">-</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">еще</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">него</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">сказать</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">а</td>   <td style=\"background-color: #eeeeee;\">ж</td>   <td style=\"background-color: #eeeeee;\">нее</td>   <td style=\"background-color: #eeeeee;\">со</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">без</td>   <td style=\"background-color: #eeeeee;\">же</td>   <td style=\"background-color: #eeeeee;\">ней</td>   <td style=\"background-color: #eeeeee;\">совсем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">более</td>   <td style=\"background-color: #eeeeee;\">жизнь</td>   <td style=\"background-color: #eeeeee;\">нельзя</td>   <td style=\"background-color: #eeeeee;\">так</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">больше</td>   <td style=\"background-color: #eeeeee;\">за</td>   <td style=\"background-color: #eeeeee;\">нет</td>   <td style=\"background-color: #eeeeee;\">такой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будет</td>   <td style=\"background-color: #eeeeee;\">зачем</td>   <td style=\"background-color: #eeeeee;\">ни</td>   <td style=\"background-color: #eeeeee;\">там</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будто</td>   <td style=\"background-color: #eeeeee;\">здесь</td>   <td style=\"background-color: #eeeeee;\">нибудь</td>   <td style=\"background-color: #eeeeee;\">тебя</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">бы</td>   <td style=\"background-color: #eeeeee;\">и</td>   <td style=\"background-color: #eeeeee;\">никогда</td>   <td style=\"background-color: #eeeeee;\">тем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">был</td>   <td style=\"background-color: #eeeeee;\">из</td>   <td style=\"background-color: #eeeeee;\">ним</td>   <td style=\"background-color: #eeeeee;\">теперь</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">была</td>   <td style=\"background-color: #eeeeee;\">из-за</td>   <td style=\"background-color: #eeeeee;\">них</td>   <td style=\"background-color: #eeeeee;\">то</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">были</td>   <td style=\"background-color: #eeeeee;\">или</td>   <td style=\"background-color: #eeeeee;\">ничего</td>   <td style=\"background-color: #eeeeee;\">тогда</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">было</td>   <td style=\"background-color: #eeeeee;\">им</td>   <td style=\"background-color: #eeeeee;\">но</td>   <td style=\"background-color: #eeeeee;\">того</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">быть</td>   <td style=\"background-color: #eeeeee;\">иногда</td>   <td style=\"background-color: #eeeeee;\">ну</td>   <td style=\"background-color: #eeeeee;\">тоже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">в</td>   <td style=\"background-color: #eeeeee;\">их</td>   <td style=\"background-color: #eeeeee;\">о</td>   <td style=\"background-color: #eeeeee;\">только</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вам</td>   <td style=\"background-color: #eeeeee;\">к</td>   <td style=\"background-color: #eeeeee;\">об</td>   <td style=\"background-color: #eeeeee;\">том</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вас</td>   <td style=\"background-color: #eeeeee;\">кажется</td>   <td style=\"background-color: #eeeeee;\">один</td>   <td style=\"background-color: #eeeeee;\">тот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вдруг</td>   <td style=\"background-color: #eeeeee;\">как</td>   <td style=\"background-color: #eeeeee;\">он</td>   <td style=\"background-color: #eeeeee;\">три</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ведь</td>   <td style=\"background-color: #eeeeee;\">какая</td>   <td style=\"background-color: #eeeeee;\">она</td>   <td style=\"background-color: #eeeeee;\">тут</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">во</td>   <td style=\"background-color: #eeeeee;\">какой</td>   <td style=\"background-color: #eeeeee;\">они</td>   <td style=\"background-color: #eeeeee;\">ты</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вот</td>   <td style=\"background-color: #eeeeee;\">когда</td>   <td style=\"background-color: #eeeeee;\">опять</td>   <td style=\"background-color: #eeeeee;\">у</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">впрочем</td>   <td style=\"background-color: #eeeeee;\">конечно</td>   <td style=\"background-color: #eeeeee;\">от</td>   <td style=\"background-color: #eeeeee;\">уж</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">все</td>   <td style=\"background-color: #eeeeee;\">которого</td>   <td style=\"background-color: #eeeeee;\">перед</td>   <td style=\"background-color: #eeeeee;\">уже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всегда</td>   <td style=\"background-color: #eeeeee;\">которые</td>   <td style=\"background-color: #eeeeee;\">по</td>   <td style=\"background-color: #eeeeee;\">хорошо</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всего</td>   <td style=\"background-color: #eeeeee;\">кто</td>   <td style=\"background-color: #eeeeee;\">под</td>   <td style=\"background-color: #eeeeee;\">хоть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всех</td>   <td style=\"background-color: #eeeeee;\">куда</td>   <td style=\"background-color: #eeeeee;\">после</td>   <td style=\"background-color: #eeeeee;\">чего</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всю</td>   <td style=\"background-color: #eeeeee;\">ли</td>   <td style=\"background-color: #eeeeee;\">потом</td>   <td style=\"background-color: #eeeeee;\">человек</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вы</td>   <td style=\"background-color: #eeeeee;\">лучше</td>   <td style=\"background-color: #eeeeee;\">потому</td>   <td style=\"background-color: #eeeeee;\">чем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">г</td>   <td style=\"background-color: #eeeeee;\">между</td>   <td style=\"background-color: #eeeeee;\">почти</td>   <td style=\"background-color: #eeeeee;\">через</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">где</td>   <td style=\"background-color: #eeeeee;\">меня</td>   <td style=\"background-color: #eeeeee;\">при</td>   <td style=\"background-color: #eeeeee;\">что</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">говорил</td>   <td style=\"background-color: #eeeeee;\">мне</td>   <td style=\"background-color: #eeeeee;\">про</td>   <td style=\"background-color: #eeeeee;\">чтоб</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">да</td>   <td style=\"background-color: #eeeeee;\">много</td>   <td style=\"background-color: #eeeeee;\">раз</td>   <td style=\"background-color: #eeeeee;\">чтобы</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">даже</td>   <td style=\"background-color: #eeeeee;\">может</td>   <td style=\"background-color: #eeeeee;\">разве</td>   <td style=\"background-color: #eeeeee;\">чуть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">два</td>   <td style=\"background-color: #eeeeee;\">можно</td>   <td style=\"background-color: #eeeeee;\">с</td>   <td style=\"background-color: #eeeeee;\">эти</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">для</td>   <td style=\"background-color: #eeeeee;\">мой</td>   <td style=\"background-color: #eeeeee;\">сам</td>   <td style=\"background-color: #eeeeee;\">этого</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">до</td>   <td style=\"background-color: #eeeeee;\">моя</td>   <td style=\"background-color: #eeeeee;\">свое</td>   <td style=\"background-color: #eeeeee;\">этой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">другой</td>   <td style=\"background-color: #eeeeee;\">мы</td>   <td style=\"background-color: #eeeeee;\">свою</td>   <td style=\"background-color: #eeeeee;\">этом</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">его</td>   <td style=\"background-color: #eeeeee;\">на</td>   <td style=\"background-color: #eeeeee;\">себе</td>   <td style=\"background-color: #eeeeee;\">этот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ее</td>   <td style=\"background-color: #eeeeee;\">над</td>   <td style=\"background-color: #eeeeee;\">себя</td>   <td style=\"background-color: #eeeeee;\">эту</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ей</td>   <td style=\"background-color: #eeeeee;\">надо</td>   <td style=\"background-color: #eeeeee;\">сегодня</td>   <td style=\"background-color: #eeeeee;\">я</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ему</td>   <td style=\"background-color: #eeeeee;\">наконец</td>   <td style=\"background-color: #eeeeee;\">сейчас</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">если</td>   <td style=\"background-color: #eeeeee;\">нас</td>   <td style=\"background-color: #eeeeee;\">сказал</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">есть</td>   <td style=\"background-color: #eeeeee;\">не</td>   <td style=\"background-color: #eeeeee;\">сказала</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U nltk\n",
    "import nltk\n",
    "nltk.__version__\n",
    "# # nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\alpha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка списков стоп-слов в NLTK:\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words = stopwords.words('russian')\n",
    "print(ru_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('phm.txt ') as f:\n",
    "#     lines = [l for l in f]\n",
    "# print(len(lines))\n",
    "# print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools as it\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ]*$') # re.compile('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens = [t.text.lower() for t in tokenize(f.read()) if w_regex.search(t.text)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266954 ['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'мне', 'отмщение', 'и', 'аз', 'воздам', 'часть', 'первая', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'все', 'смешалось', 'в', 'доме', 'облонских', 'жена', 'узнала', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'и', 'объявила', 'мужу', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'уже', 'третий', 'день', 'и', 'мучительно', 'чувствовалось', 'и', 'самими', 'супругами', 'и', 'всеми', 'членами', 'семьи', 'и', 'домочадцами', 'все', 'члены', 'семьи', 'и', 'домочадцы', 'чувствовали', 'что', 'нет', 'смысла', 'в', 'их', 'сожительстве', 'и', 'что', 'на', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'более', 'связаны', 'между', 'собой', 'чем', 'они', 'члены', 'семьи', 'и', 'домочадцы', 'облонских', 'жена', 'не', 'выходила', 'из', 'своих', 'комнат', 'мужа', 'третий', 'день', 'не', 'было', 'дома', 'дети', 'бегали', 'по', 'всему', 'дому', 'как', 'потерянные', 'англичанка', 'поссорилась', 'с', 'экономкой', 'и', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'ей', 'новое', 'место', 'повар', 'ушел', 'еще', 'вчера', 'со', 'двора', 'во', 'время', 'обеда', 'черная', 'кухарка', 'и', 'кучер', 'просили', 'расчета', 'на']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(len(book_tokens), book_tokens[:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано токенов:266954; найдено различных токенов:32569\n",
      "Содержимое: [('лев', 1), ('николаевич', 2), ('толстой', 2), ('анна', 499), ('каренина', 45), ('мне', 682), ('отмщение', 1), ('и', 12916), ('аз', 1), ('воздам', 1)]\n",
      "Самое частое слово:и,  частота слова \"анна\":499\n",
      "[('и', 12916), ('не', 6537), ('что', 5765), ('в', 5720), ('он', 5551), ('на', 3594), ('она', 3434), ('с', 3327), ('я', 3212), ('как', 2660), ('но', 2581), ('его', 2578), ('это', 2223), ('к', 1983), ('ее', 1805), ('все', 1671), ('было', 1656), ('так', 1415), ('сказал', 1412), ('а', 1391), ('то', 1388), ('же', 1325), ('ему', 1252), ('о', 1243), ('за', 1139), ('левин', 1135), ('только', 1017), ('ты', 993), ('у', 913), ('был', 901), ('по', 834), ('когда', 831), ('для', 827), ('сказала', 827), ('бы', 822), ('от', 813), ('да', 812), ('теперь', 810), ('вы', 756), ('из', 735), ('была', 728), ('еще', 699), ('ей', 689), ('мне', 682), ('кити', 661), ('они', 646), ('него', 622), ('уже', 601), ('нет', 592), ('очень', 573)]\n"
     ]
    }
   ],
   "source": [
    "print(f'Обработано токенов:{fdist.N()}; найдено различных токенов:{fdist.B()}')\n",
    "# kvi = iter(fdist.items())\n",
    "print('Содержимое:', list(it.islice(fdist.items(), 10)))\n",
    "print(f'Самое частое слово:{fdist.max()},  частота слова \"анна\":{fdist.get(\"анна\")}')\n",
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sent_l0 = list(sentenize(lines[0]))\n",
    "# tokens_l0 = [list(tokenize(sent.text)) for sent in sent_l0]\n",
    "# tokens_l0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# # делаем плоский список и оставляем только слова приведенные к ниженму регистру:\n",
    "# wtokens_l0 = [t.text.lower() for tokens in tokens_l0 for t in tokens if w.search(t.text)]\n",
    "# print(wtokens_l0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'отмщение', 'аз', 'воздам', 'часть', 'первая', 'счастливые', 'семьи', 'похожи', 'друг', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'смешалось', 'доме', 'облонских', 'жена', 'узнала', 'муж', 'связи', 'бывшею', 'доме', 'объявила', 'мужу', 'жить', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'третий', 'день', 'мучительно', 'чувствовалось', 'самими', 'супругами', 'всеми', 'членами', 'семьи', 'домочадцами', 'члены', 'семьи', 'домочадцы', 'чувствовали', 'смысла', 'сожительстве', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'связаны', 'собой', 'члены', 'семьи', 'домочадцы', 'облонских', 'жена', 'выходила', 'своих', 'комнат', 'мужа', 'третий', 'день', 'дома', 'дети', 'бегали', 'всему', 'дому', 'потерянные', 'англичанка', 'поссорилась', 'экономкой', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'новое', 'место', 'повар', 'ушел', 'вчера', 'двора', 'время', 'обеда', 'черная', 'кухарка', 'кучер', 'просили', 'расчета']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words_s = set(ru_stop_words)\n",
    "# фильтруем стоп слова:\n",
    "wtokens_wostw = [w for w in book_tokens[:150] if w not in ru_stop_words_s]\n",
    "print(wtokens_wostw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 'это', 2223), (18, 'сказал', 1412), (25, 'левин', 1135), (33, 'сказала', 827), (44, 'кити', 661), (49, 'очень', 573), (53, 'вронский', 509), (56, 'анна', 499), (66, 'алексей', 429), (68, 'степан', 423), (69, 'аркадьич', 422), (72, 'александрович', 395), (81, 'время', 366), (82, 'мог', 357), (83, 'говорил', 357), (89, 'руку', 309), (90, 'долли', 302), (92, 'которые', 295), (97, 'лицо', 277), (98, 'сказать', 276), (102, 'дело', 272), (103, 'левина', 272), (108, 'который', 263), (111, 'своей', 251), (113, 'знал', 249), (116, 'жизни', 235), (117, 'говорить', 234), (118, 'знаю', 233), (121, 'которое', 231), (124, 'пред', 224), (125, 'хотел', 219), (127, 'сергей', 219), (129, 'нужно', 217), (130, 'человек', 215), (131, 'прежде', 215), (132, 'глаза', 214), (134, 'могу', 214), (135, 'видел', 214), (137, 'тебе', 213), (139, 'тотчас', 211), (141, 'чувствовал', 210), (143, 'вронского', 205), (145, 'одно', 202), (146, 'своего', 199), (147, 'могла', 199), (148, 'свое', 198), (149, 'иванович', 191), (153, 'думал', 189), (154, 'глядя', 189), (156, 'говорила', 184)]\n"
     ]
    }
   ],
   "source": [
    "print(list(it.islice(((i, w, f) for i, (w, f) in enumerate(fdist.most_common(500)) \\\n",
    "                      if w not in ru_stop_words_s), 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Языковые корпусы "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Корпус - собрание текстов, объединенных общим признаком\n",
    "\n",
    "Зачем используются корпуcы в  лингвистике? \n",
    "1. На основе корпусов создаются словари и грамматики. \n",
    "2. Корпусы помогают и в теоретической лингвистике: при исследовании какого-то явления можно быстро собрать нужные данные в их естественном контексте и проанализировать. \n",
    "3. На основе корпусов проводится машинное обучение для самых разных областей прикладной лингвистики. \n",
    "Корпусы могут пригодиться и для любых других задач, связанных с языком - однажды созданный и подготовленный корпус может использоваться многократно, различными исследователями и в разных целях. По сути, корпус - инфраструктруа для существующих и еще не появившихся задач изучения языка.\n",
    "\n",
    "Свойствак корпуса: \n",
    "1. __электронный__ - оформленный в удобном для работе формате\n",
    "2. __репрезентативный__ - корпус должен хорошо «представлять» тот объект, который он моделирует, т.е. язык)\n",
    "    * сбалансированность (balance) корпуса: если корпус - это уменьшенная модель языка, то в нем пропорционально (_какие пропорции?_) должны быть представлены текстов различных периодов, жанров, стилей, авторов и т. д. \n",
    "3. __размеченный__ - разметка заключается в приписывании текстам корпуса и их компонентам дополнительной информации, метаданных\n",
    "4. __прагматически ориентированный__ - создаваемый для определенных целей\n",
    "\n",
    "Специальные виды коропусов:\n",
    "* Параллельные корпусы - корпуса на нескольких языках\n",
    "* Устные корпусы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Типы разметки корпусов: \n",
    "* __морфологическая__ - part-of-speech tagging (POS-tagging), дословно - частеречная разметка. В действительности морфологические метки включают не только признак части речи, но и признаки грамматических категорий, свойственных данной части речи, а также нормальную форму слова, лемму. \n",
    "* __синтаксическая__ - описывает синтаксические связи между лексическими единицами и/или различные синтаксические конструкции (например, придаточное предложение, именное сказуемое и т.д.) \n",
    "* __семантическая__ - обозначают семантические категории, к которым относится данное слово или слово сочетание, и более узкие подкатегории, определяющие его значение. Семантическая разметка корпусов предусматривает спецификацию значений слов, разрешение омонимии и синонимии, категоризацию слов (разряды), выделение тематических классов "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример морфологической разметки корпуса:\n",
    "```xml \n",
    "<s> \n",
    "    <w>Звонили<ana lemma=\"ЗВОНИТЬ\" pos=\"Г\" gram=\"мн,нс,нп,дст,прш,\"/></w> \n",
    "    <w>к<ana lemma=\"К\" p os=\"ПР ЕДЛ\" gram=\"\" /></w> \n",
    "    <w>вечерне\n",
    "        <ana lemma=\"ВЕЧЕРНЯ\" p os=\"С\" gram=\"жр ,ед,дт,пр,но,\" /> \n",
    "        <ana lemma=\"ВЕЧЕРНИЙ\" p os=\"П\" gram=\"ср,ед,кр,\" /></w> \n",
    "    <pun>.</pun> \n",
    "</s> \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Основные корпусы русского языка: \n",
    "* http://ruscorpora.ru - Национальный корпус русского языка (НКРЯ)\n",
    "* http://www.ling.helsinki.fi/projects/hanco - ХЕЛЬСИНКСКИЙ АННОТИРОВАННЫЙ КОРПУС (ХАНКО)\n",
    "* http://corpus.leeds.ac.uk/ruscorpora.html - Russian corpora (Ruscorpora)\n",
    "* http://aot.ru/search1.html - Автоматическая Обработка Текста (АОТ)\n",
    "* http://sketchengine.co.uk \n",
    "\n",
    "Неразмеченные текстовые корпусы:\n",
    "* Project Guttenberg (англоязычный)\n",
    "* Reuters corpora (англоязычный)\n",
    "* lib.ru\n",
    "* Web..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Языковые модели, N-граммы"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели, которые приписывают последовательности слов вероятность ее появления в тексте, называются __языковыми моделями__.\n",
    "\n",
    "Задача языкового моделирования формально может быть сведена к вычислению вероятности появления слова $w_i$ при условии, что до этого появилась цепочка слов $w_1, \\dotso, w_{i−1}$ (история). \n",
    "\n",
    "Например, для изречения _\"Враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\"_ вероятностная языковая модель должна уметь предсказывать, например,  вероятность:\n",
    "\n",
    "_P = (\"преднамеренно\" | \"враг не тот, кто наносит обиду, а тот, кто делает это\")_\n",
    "\n",
    "$P(A|B)=P(AB)/P(B)$\n",
    "\n",
    "Простые методы порождения вероятностных языковых моделей, вероятность $P$ напрямую: \n",
    "\n",
    "_P = F(\"враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\")/F(\"враг не тот, кто наносит обиду, а тот, кто делает это\")_\n",
    "\n",
    "где F – абсолютная частота встречаемости выражения в корпусе текстов. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "N-граммой (n-gram) называется последовательность из $n$ структурных единиц (токенов), на которые сегментирован исходный текст (слова, символы алфавита и т.д.). N-граммы, извлеченные из корпусов, чаще всего _состоят из словоформ, лексем или стем_.\n",
    "\n",
    "Если рассматривать n-граммы как последовательности слов, то:\n",
    "* отдельное слово называется __униграммой__ (\"хозяин\")\n",
    "* два слова - __биграммой__ (\"хозяин ушел\")\n",
    "* три - __триграммой__ (\"хозяин ушел домой\") \n",
    "* и т.д. \n",
    "\n",
    "Слова внутри n-граммы могут не иметь синтаксических связей между собой, единственное, что их связывает – это совместная встречаемость.\n",
    "\n",
    "Кроме n-грамм существуют коллокации:\n",
    "* __n-грамма__ - (статистически устойчивая) последовательность из n соседних слов\n",
    "* __коллокация__ - устойчивое выражение (словосочетание), слова в котором связаны друг с другом, при этом они не обязательно располагаются друг за другом. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наборы данных с N-граммами:\n",
    "\n",
    "* https://books.google.com/ngrams , пример: https://books.google.com/ngrams/graph?content=%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80+%D0%9F%D1%83%D1%88%D0%BA%D0%B8%D0%BD%2C+%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80+%D0%9B%D0%B5%D0%BD%D0%B8%D0%BD%2C+%D0%AE%D1%80%D0%B8%D0%B9+%D0%93%D0%B0%D0%B3%D0%B0%D1%80%D0%B8%D0%BD&year_start=1800&year_end=2000&corpus=25&smoothing=3&share=&direct_url=t1%3B%2C%D0%90%D0%BB%D0%B5%D0%BA%D1%81%D0%B0%D0%BD%D0%B4%D1%80%20%D0%9F%D1%83%D1%88%D0%BA%D0%B8%D0%BD%3B%2Cc0%3B.t1%3B%2C%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80%20%D0%9B%D0%B5%D0%BD%D0%B8%D0%BD%3B%2Cc0%3B.t1%3B%2C%D0%AE%D1%80%D0%B8%D0%B9%20%D0%93%D0%B0%D0%B3%D0%B0%D1%80%D0%B8%D0%BD%3B%2Cc0\n",
    "\n",
    "* http://storage.googleapis.com/books/ngrams/books/datasetsv2.html\n",
    "* http://www.ruscorpora.ru/corpora-freq.html\n",
    "* https://www.sketchengine.eu/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В обучающем корпусе мы можем:\n",
    "* для каждой n-граммы мы можем посчитать __частоту__ - т.е. сколько раз она встретилась в корпусе\n",
    "* на основе полученных данных построить __вероятностную модель__, которая затем может быть использована для оценки вероятности n-грамм в некотором тестовом корпусе\n",
    "\n",
    "Какова вероятность встретить определенное предложение в тексте? Формулировка:\n",
    "\n",
    "_P(\"враг не тот, кто наносит обиду, а тот, кто делает это преднамеренно\") = ?_\n",
    "\n",
    "Вероятность этого события можно расписать через произведение условных вероятностей:\n",
    "$$P(w_1^n) = P(w_1)P(w_2|w_1)P(w_3|w_1^2) \\cdot \\dotso \\cdot P(w_n|w_1^{n−1}) = \\prod_{k=1}^{n}P(w_k|w_1^{k−1})$$\n",
    "где: $w_i$ - i-е слово; $w_i^j$ - последовательность слов с i-го до j-го.\n",
    "\n",
    "Допущение Маркова:\n",
    "$$P(w_k|w_1^{k−1}) \\approx P(w_k|w_{k−1})$$\n",
    "тогда:\n",
    "$$P(w_1^n) \\approx \\prod_{k=1}^{n}P(w_k|w_{k−1})$$\n",
    "\n",
    "Как найти условные вероятности $P(w_k|w_{k−1})$? Используем частоты:\n",
    "$$P(w_k|w_{k−1}) = \\frac{F(w_{k−1}w_k)}{\\sum_{w}F(w_{k−1}w)} = \\frac{F(w_{k−1}w_k)}{F(w_{k−1})}$$\n",
    "т.е. нам нужны частоты биграмм и униграмм в корпусе."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример. Пусть корпус состоит из трех предложений:\n",
    "* ```<s> I am Sam </s>```\n",
    "* ```<s> Sam I am </s>```\n",
    "* ```<s> I do not like green eggs and ham </s>```\n",
    "\n",
    "Тогда: P(I|<s\\>) = 2/3 ; P(am|I)=2/2; P(Sam|am)=1/2; P(</s\\>|Sam)=1/2; P(do|I)=1/3; P(Sam|<s\\>)=1/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel.substring import Substring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ_]*$') # ('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens_ss =  [t.text.lower() for sent in list(sentenize(f.read())) \\\n",
    "    for t in it.chain([Substring(None, None, '_')], tokenize(sent.text), [Substring(None, None, '__')]) \\\n",
    "    if w_regex.search(t.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['_', 'лев', 'николаевич', 'толстой', 'анна', 'каренина', 'мне', 'отмщение', 'и', 'аз', 'воздам', 'часть', 'первая', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', '__', '_', 'все', 'смешалось', 'в', 'доме', 'облонских', '__', '_', 'жена', 'узнала', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'и', 'объявила', 'мужу', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', '__', '_', 'положение', 'это', 'продолжалось', 'уже', 'третий', 'день', 'и', 'мучительно', 'чувствовалось', 'и', 'самими', 'супругами', 'и', 'всеми', 'членами', 'семьи', 'и', 'домочадцами', '__', '_', 'все', 'члены', 'семьи', 'и', 'домочадцы', 'чувствовали', 'что', 'нет', 'смысла', 'в', 'их', 'сожительстве', 'и', 'что', 'на', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди']\n"
     ]
    }
   ],
   "source": [
    "print(book_tokens_ss[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "# from nltk.probability import FreqDist\n",
    "# fdist = FreqDist(b_t)\n",
    "from nltk.util import everygrams, skipgrams\n",
    "from nltk.probability import ConditionalFreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_', 19386), ('и', 12916), ('в', 5720), ('с', 3327), ('я', 3212), ('к', 1983), ('а', 1391), ('о', 1243), ('у', 913), ('ж', 129)]\n",
      "[('было', 1656), ('была', 728), ('кити', 661), ('него', 622), ('быть', 565), ('меня', 533), ('себя', 501), ('анна', 499), ('себе', 499), ('были', 499)]\n"
     ]
    }
   ],
   "source": [
    "cfdist = ConditionalFreqDist((len(word), word) for word in book_tokens_ss)\n",
    "print(cfdist[1].most_common()[:10])\n",
    "print(cfdist[4].most_common()[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна'),\n",
       " ('анна', 'каренина'),\n",
       " ('каренина', 'мне'),\n",
       " ('мне', 'отмщение'),\n",
       " ('отмщение', 'и'),\n",
       " ('и', 'аз'),\n",
       " ('аз', 'воздам')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Биграммы можно строить так:\n",
    "list(it.islice(zip(book_tokens_ss[:-1], book_tokens_ss[1:]), 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_',),\n",
       " ('лев',),\n",
       " ('николаевич',),\n",
       " ('толстой',),\n",
       " ('анна',),\n",
       " ('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна'),\n",
       " ('_', 'лев', 'николаевич'),\n",
       " ('лев', 'николаевич', 'толстой'),\n",
       " ('николаевич', 'толстой', 'анна')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(book_tokens_ss[:5], max_len=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('_', 'лев'),\n",
       " ('лев', 'николаевич'),\n",
       " ('николаевич', 'толстой'),\n",
       " ('толстой', 'анна')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(everygrams(book_tokens_ss[:5], min_len=2, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cfdist2 = ConditionalFreqDist(w_2 for w_2 in  everygrams(book_tokens_ss, min_len=2, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__', 64), ('аркадьевна', 28), ('не', 21), ('и', 20), ('с', 12), ('как', 9), ('павловна', 8), ('я', 6), ('была', 6), ('вышла', 6), ('в', 6), ('уже', 6), ('ни', 4), ('ничего', 4), ('улыбнулась', 4), ('вдруг', 4), ('чувствовала', 4), ('на', 4), ('вошла', 3), ('что', 3), ('все', 3), ('очевидно', 3), ('быстро', 3), ('сказала', 3), ('села', 3), ('встала', 3), ('но', 3), ('говорила', 3), ('ради', 3), ('сказал', 3), ('опять', 3), ('спросил', 2), ('взялась', 2), ('подняла', 2), ('пожимая', 2), ('провела', 2), ('знала', 2), ('улыбаясь', 2), ('вглядываясь', 2), ('сейчас', 2)]\n",
      "\n",
      "[('__', 68), ('и', 28), ('не', 25), ('был', 13), ('с', 12), ('в', 9), ('вошел', 8), ('знал', 7), ('чувствовал', 7), ('взглянул', 6), ('улыбаясь', 5), ('уже', 5), ('еще', 5), ('но', 5), ('сказал', 4), ('подошел', 4), ('никогда', 4), ('мог', 4), ('имел', 4), ('понял', 4), ('остановился', 3), ('вышел', 3), ('глядя', 3), ('оба', 3), ('смотрел', 3), ('поехал', 3), ('видел', 3), ('она', 2), ('подумал', 2), ('встал', 2), ('со', 2), ('внимательно', 2), ('пожимая', 2), ('к', 2), ('взял', 2), ('увидал', 2), ('для', 2), ('ничего', 2), ('закричал', 2), ('весело', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(cfdist2['анна'].most_common()[:40])\n",
    "print()\n",
    "print(cfdist2['вронский'].most_common()[:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ConditionalFreqDist' object has no attribute 'most_common'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-fc9e6bd8bffd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcfdist2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_common\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'ConditionalFreqDist' object has no attribute 'most_common'"
     ]
    }
   ],
   "source": [
    "cfdist2.most_common()[:40]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfdist2['анна'].most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('__', 64), ('аркадьевна', 28), ('павловна', 8), ('вышла', 6), ('улыбнулась', 4), ('чувствовала', 4), ('вошла', 3), ('очевидно', 3), ('быстро', 3), ('сказала', 3), ('села', 3), ('встала', 3), ('говорила', 3), ('ради', 3), ('сказал', 3), ('спросил', 2), ('взялась', 2), ('подняла', 2), ('пожимая', 2), ('провела', 2), ('знала', 2), ('улыбаясь', 2), ('вглядываясь', 2), ('вернулась', 2), ('оглядываясь', 2), ('посмотрела', 2), ('почувствовала', 2), ('очень', 2), ('видела', 2), ('смотрела', 2), ('прочла', 2), ('приехала', 2), ('тотчас', 2), ('краснея', 2), ('рядом', 2), ('заметила', 2), ('глядя', 2), ('спросила', 2), ('сощурившись', 2), ('взяла', 2)]\n",
      "\n",
      "[('__', 68), ('вошел', 8), ('знал', 7), ('чувствовал', 7), ('взглянул', 6), ('улыбаясь', 5), ('сказал', 4), ('подошел', 4), ('мог', 4), ('имел', 4), ('понял', 4), ('остановился', 3), ('вышел', 3), ('глядя', 3), ('оба', 3), ('смотрел', 3), ('поехал', 3), ('видел', 3), ('подумал', 2), ('встал', 2), ('внимательно', 2), ('пожимая', 2), ('взял', 2), ('увидал', 2), ('закричал', 2), ('весело', 2), ('слушал', 2), ('испытывал', 2), ('хмурясь', 2), ('оглянулся', 2), ('хотел', 2), ('выпьешь', 2), ('действительно', 2), ('несмотря', 2), ('получив', 2), ('назвал', 2), ('вспоминая', 2), ('поклонился', 2), ('желал', 2), ('тотчас', 2)]\n"
     ]
    }
   ],
   "source": [
    "print(list(it.islice(((w, f) for w, f in cfdist2['анна'].most_common() if w not in ru_stop_words_s), 40)))\n",
    "print()\n",
    "print(list(it.islice(((w, f) for w, f in cfdist2['вронский'].most_common() if w not in ru_stop_words_s), 40)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cfdist.tabulate(['и', 'за'], ['анна', 'вронский']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cond_prob(s1, s2, cfd):\n",
    "    common_frq = cfd[s1][s2]\n",
    "    s1_frq = cfd[s1].N()\n",
    "    print(f'''2 слова встретились совместно (в данном порядке):{common_frq}, \n",
    "частота предшеcтвовашего слова: {s1_frq}''')    \n",
    "    return common_frq/s1_frq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 слова встретились совместно (в данном порядке):28, \n",
      "частота предшеcтвовашего слова: 499\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.056112224448897796"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cond_prob('анна','аркадьевна', cfdist2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['b', 'c', 'c', 'a', 'b', 'c', 'b', 'c', 'b', 'c']\n"
     ]
    }
   ],
   "source": [
    "# ['a', 'b', 'c']\n",
    "lm = MLE(2)\n",
    "lm.fit([[(\"a\", \"b\"), (\"b\", \"c\")]], vocabulary_text=['a', 'b', 'c'])\n",
    "lm.fit([[(\"a\",), (\"b\",), (\"c\",)]])\n",
    "# print(lm.generate(random_seed=3))\n",
    "print(lm.generate(num_words=10, text_seed=['a']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2 = MLE(2)\n",
    "vals = book_tokens_ss[:]\n",
    "vocabulary_text = list(set(vals))\n",
    "ngram_2 = list(everygrams(vals, min_len=2, max_len=2))\n",
    "# print(vals,text, vocabulary_text, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm2.fit([ngram_2], vocabulary_text)\n",
    "lm2.fit([[(t,) for t in vals]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['левин', 'точно', 'перламутровую', 'раковину', 'которою', 'он', '__', '_', 'опять', 'сложив', 'письмо', '__', '_', 'ну', 'да', 'я', 'хотел', 'вспоминать', 'об', 'удобствах']\n"
     ]
    }
   ],
   "source": [
    "print(lm2.generate(num_words=20, text_seed=['_']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3 = MLE(3)\n",
    "vals = book_tokens_ss[:]\n",
    "vocabulary_text = list(set(vals))\n",
    "ngram_3 = list(everygrams(vals, min_len=3, max_len=3))\n",
    "# print(vals,text, vocabulary_text, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "lm3.fit([ngram_3], vocabulary_text)\n",
    "lm3.fit([[(t,) for t in vals]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ввел', '_', 'в', 'четверг', 'ветер', 'затих', 'и', 'надвинулся', 'густой', 'серый', 'туман', 'как', 'бы', 'снимал', 'с', 'нее', 'глаз', '__', '_', 'тихо', 'и', 'медленно', 'выговаривая', 'слова', 'но', 'с', 'твоим', 'монашеством', 'и', 'строгим', 'лицом', 'приказывавшую', 'слуге', '__', '_', 'нет', 'все', 'более', 'и', 'более', 'озлобляясь', 'кричал', 'громче', 'и', 'громче', '__', '_', 'я', 'все', 'сделаю']\n"
     ]
    }
   ],
   "source": [
    "print(lm3.generate(num_words=50, text_seed=['_']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Необходимость в сглаживании:\n",
    "* Из-за разреженности языка\n",
    "* Огранниченность размера корпуса\n",
    "    * занижена вероятность\n",
    "    * вероятность равна нулю\n",
    "    \n",
    "Сглаживание - повышение вероятности некоторых n-грам, за счет понижения вероятности других.\n",
    "* (*) Сглаживание Лапласа\n",
    "* (*) Откат (backoff)\n",
    "* Сглаживание Кнесера-Нея (Kneser-Ney)\n",
    "* Сглаживание Виттена-Белла (Witten-Bell)\n",
    "* Сглаживание Гуда-Тьюринга (Good-Turing)\n",
    "* Интерполяция"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сглаживание Лапласа:\n",
    "\n",
    "Добавим 1 к встречаемости каждой n-граммы. Пусть в словаре V слов, тогда:\n",
    "\n",
    "$$P(w_k|w_{k−1}) = \\frac{F(w_{k−1}w_k)+1}{F(w_{k−1})+V}$$\n",
    "\n",
    "Практическое применение Сглаживания Лапласа:\n",
    "* Метод провоцирует сильную погрешность в вычислениях \n",
    "* Тесты показываюь, что не сглаженная модель часто показывает более точные результаты\n",
    "* Мтод интересен только с теоретической точки зрения"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Откат (backoff):\n",
    "\n",
    "* Основная идея: можно оценивать вероятности n-грамм с помощью вероятностей (n-k)-грамм (0<k<n).\n",
    "* Особенность: метод можно сочетать с другими алгоритмами сглаживания (Witten-Bell, Good-Turing и т. д.)\n",
    "\n",
    "Оценка вероятности в случае триграмм:\n",
    "\n",
    "$$\\hat{P}(w_k|w_{k−2}w_{k−1}) = \\begin{cases}\n",
    " & P(w_k|w_{k−2}w_{k−1}) \\text{ , } F(w_{k−2}w_{k−1}w_k)>0 \\\\ \n",
    " & \\alpha(w_{k−2}^{k-1})\\hat{P}(w_k|w_{k−1}) \\text{ , } F(w_{k−2}w_{k−1}w_k)=0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "* Коэффициент $\\alpha$ необходим для корректного распределения остаточной вероятности n-грамм в соответствии с распределением вероятности (n-1)-грамм.\n",
    "* Если не вводить $\\alpha$, оценка будет ошибочной, т.к. не будет выполняться равенство:\n",
    "$$\\sum_{i,j} P(w_k|w_i w_j) = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методы оценки качества моделей\n",
    "\n",
    "Как понять, что одна модель лучше другой?\n",
    "* Внешняя оценка (in vivo): как изменение параметра модели влияет на качество решения задачи\n",
    "* Внутренняя оценка (in vitro): коэффициент неопределенности (perplexity)\n",
    "\n",
    "Коэффициент неопределенности\n",
    "* Основан на теории информации\n",
    "* Лучше та модель, которая лучше предсказывает детали тестовой коллекции\n",
    "\n",
    "$$ PP(w) = P(w_1 w_2 \\dotso w_N )^{-1/N}$$\n",
    "\n",
    "Для биграмм:\n",
    "$$ PP(w) = \\left ( \\prod_{i=1}^{N} P(w_i | w_{i-1} )\\right )^{-1/N}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "499"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# http://www.nltk.org/api/nltk.lm.html#module-nltk.lm.models\n",
    "lm2.counts['анна']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.counts[['анна']]['была']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here’s how you get the score for a word given some preceding context. For example we want to know what is the chance that “b” is preceded by “a”.\n",
    "\n",
    "```python\n",
    ">>> lm.score(\"b\", [\"a\"])\n",
    "0.5```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016321804491603593"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# However, the real purpose of training a language model is to have it score how probable words are in certain contexts. \n",
    "# This being MLE, the model returns the item’s relative frequency as its score.\n",
    "lm2.score(\"анна\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-9.258983718334655"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.logscore(\"анна\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.012024048096192385"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here’s how you get the score for a word given some preceding context. \n",
    "# For example we want to know what is the chance that “была” is preceded by [\"анна\"].\n",
    "lm2.score('была', ['анна'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246.0596133731282"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [('анна', 'была'), ('была', 'замужем')]\n",
    "lm2.perplexity(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.948623364641533"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm2.entropy(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "inf"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = [(\"замужем\",'анна'), ('анна', 'была')]\n",
    "lm2.perplexity(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Поиск словосочетаний"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предпосылка:\n",
    "* два (или более) слова встречаются вместе намного более часто, чем это могло бы происходить случайно\n",
    "\n",
    "Подходы к выявлению:\n",
    "* анализ частоты\n",
    "* анализ частоты и дополнительная фильтрация\n",
    "* () математическое ожидание и дисперсия"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с тривиального анализа частоты биграмм:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# еще раз:\n",
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ_]*$') # ('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens_ss2 =  [t.text.lower() for sent in list(sentenize(f.read())) \\\n",
    "    for t in it.chain([Substring(None, None, '_')], tokenize(sent.text), [Substring(None, None, '__')]) \\\n",
    "    if w_regex.search(t.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# определяем частоту биграмм:\n",
    "fdist_bi = FreqDist(w_2 for w_2 in  everygrams(book_tokens_ss2, min_len=2, max_len=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('__', '_'), 19385),\n",
       " (('_', 'я'), 1136),\n",
       " (('_', 'он'), 1090),\n",
       " (('_', 'и'), 936),\n",
       " (('_', 'но'), 850),\n",
       " (('_', 'она'), 785),\n",
       " (('что', 'он'), 642),\n",
       " (('_', 'да'), 634),\n",
       " (('и', 'не'), 518),\n",
       " (('_', 'а'), 506),\n",
       " (('я', 'не'), 495),\n",
       " (('то', 'что'), 472),\n",
       " (('сказал', 'он'), 455),\n",
       " (('он', 'не'), 454),\n",
       " (('_', 'ну'), 433),\n",
       " (('степан', 'аркадьич'), 422),\n",
       " (('алексей', 'александрович'), 395),\n",
       " (('_', 'это'), 367),\n",
       " (('сказала', 'она'), 362),\n",
       " (('что', 'она'), 343),\n",
       " (('_', 'в'), 339),\n",
       " (('_', 'что'), 331),\n",
       " (('_', 'нет'), 327),\n",
       " (('_', 'как'), 325),\n",
       " (('_', 'левин'), 319),\n",
       " (('его', '__'), 295),\n",
       " (('о', 'том'), 279),\n",
       " (('и', 'он'), 266),\n",
       " (('он', '__'), 265),\n",
       " (('_', 'ты'), 257),\n",
       " (('не', 'мог'), 254),\n",
       " (('она', 'не'), 248),\n",
       " (('что', 'я'), 247),\n",
       " (('и', 'в'), 245),\n",
       " (('ничего', 'не'), 236),\n",
       " (('_', 'не'), 235),\n",
       " (('ее', '__'), 232),\n",
       " (('потому', 'что'), 230),\n",
       " (('и', 'что'), 229),\n",
       " (('что', 'это'), 221),\n",
       " (('_', 'все'), 211),\n",
       " (('с', 'ним'), 209),\n",
       " (('том', 'что'), 209),\n",
       " (('она', '__'), 201),\n",
       " (('на', 'него'), 200),\n",
       " (('может', 'быть'), 198),\n",
       " (('к', 'ней'), 193),\n",
       " (('_', 'вот'), 193),\n",
       " (('_', 'вы'), 188),\n",
       " (('сергей', 'иванович'), 188)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fdist_bi.most_common(n=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('что', 'он'), 642),\n",
       " (('и', 'не'), 518),\n",
       " (('я', 'не'), 495),\n",
       " (('то', 'что'), 472),\n",
       " (('сказал', 'он'), 455),\n",
       " (('он', 'не'), 454),\n",
       " (('степан', 'аркадьич'), 422),\n",
       " (('алексей', 'александрович'), 395),\n",
       " (('сказала', 'она'), 362),\n",
       " (('что', 'она'), 343),\n",
       " (('о', 'том'), 279),\n",
       " (('и', 'он'), 266),\n",
       " (('не', 'мог'), 254),\n",
       " (('она', 'не'), 248),\n",
       " (('что', 'я'), 247),\n",
       " (('и', 'в'), 245),\n",
       " (('ничего', 'не'), 236),\n",
       " (('потому', 'что'), 230),\n",
       " (('и', 'что'), 229),\n",
       " (('что', 'это'), 221),\n",
       " (('с', 'ним'), 209),\n",
       " (('том', 'что'), 209),\n",
       " (('на', 'него'), 200),\n",
       " (('может', 'быть'), 198),\n",
       " (('к', 'ней'), 193),\n",
       " (('сергей', 'иванович'), 188),\n",
       " (('как', 'он'), 185),\n",
       " (('к', 'нему'), 180),\n",
       " (('он', 'был'), 180),\n",
       " (('не', 'было'), 179),\n",
       " (('как', 'будто'), 172),\n",
       " (('тотчас', 'же'), 171),\n",
       " (('и', 'с'), 170),\n",
       " (('но', 'он'), 169),\n",
       " (('это', 'было'), 165),\n",
       " (('когда', 'он'), 163),\n",
       " (('не', 'могу'), 161),\n",
       " (('и', 'я'), 161),\n",
       " (('не', 'может'), 159),\n",
       " (('в', 'том'), 157),\n",
       " (('все', 'это'), 156),\n",
       " (('как', 'бы'), 154),\n",
       " (('того', 'что'), 154),\n",
       " (('несмотря', 'на'), 154),\n",
       " (('так', 'же'), 150),\n",
       " (('что', 'же'), 148),\n",
       " (('и', 'все'), 148),\n",
       " (('что', 'не'), 146),\n",
       " (('и', 'она'), 144),\n",
       " (('сказал', 'левин'), 144)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# отфильтруем все биграммы с началом и концом предложения:\n",
    "list(it.islice(((bi, frq) for bi, frq in fdist_bi.most_common(n=500) if len(set(bi) & set(('_', '__'))) == 0), 50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подход к решению проблемы: __фильтрация по комбинациям частей речи__\n",
    "<center> \n",
    "__Пример: набор самых высокочастотных биграмм:__\n",
    "\n",
    "<img src=\"collocation1.png\" alt=\"Саме высокочастотные биграммы\" style=\"width: 200px;\"/>\n",
    "\n",
    "__Пример: набор шаблонов допустимых последовательностей частей речи:__\n",
    "\n",
    "<img src=\"collocation2.png\" alt=\"Набор шаблонов допустимых последовательностей частей речи\" style=\"width: 400px;\"/>\n",
    "\n",
    "__Пример: самые высокочастотные биграммы после фильтрации по шаблонам сочетаний частей речи:__\n",
    "\n",
    "<img src=\"collocation3.png\" alt=\"Самые высокочастотные биграммы после фильтрации\" style=\"width: 400px;\"/>\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовимся тегировать словосочетания:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       "  Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))],\n",
       " OpencorporaTag('VERB,perf,intr plur,past,indc'))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')\n",
    "p, p[0].tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'NOUN': 0.7})"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Counter позволяет удобно складывать значения по совпадающим ключам словарей:\n",
    "Counter({'NOUN':0.5}) + Counter({'NOUN':0.2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_to_pos_frq_dict(word):\n",
    "    res = Counter()\n",
    "    for word_razbor in morph.parse(word):\n",
    "        res += Counter({word_razbor.tag.POS: word_razbor.score})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'VERB': 0.984662, 'NOUN': 0.015335})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_to_pos_frq_dict('стали')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[Counter({'VERB': 0.984662, 'NOUN': 0.015335}), Counter({'VERB': 0.984662, 'NOUN': 0.015335})] end\n",
      "Wall time: 1.35 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "r = [word_to_pos_frq_dict(w) for w in it.repeat('стали', 1000)]\n",
    "print(r[:2], 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ускорим определение частей речи за счет кэширования разбора часто повторяющихся слов\n",
    "from functools import lru_cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "@lru_cache(maxsize=5000) # кэшируем до 5000 различных результатов\n",
    "def word_to_pos_frq_dict2(word):\n",
    "    res = Counter()\n",
    "    for word_razbor in morph.parse(word):\n",
    "        res += Counter({word_razbor.tag.POS: word_razbor.score})\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[Counter({'VERB': 0.984662, 'NOUN': 0.015335}), Counter({'VERB': 0.984662, 'NOUN': 0.015335})] end\n",
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "print('start')\n",
    "r = [word_to_pos_frq_dict2(w) for w in it.repeat('стали', 1000)]\n",
    "print(r[:2], 'end')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создаем список высокочастотных биграмм не включающих символ начала и конца предложения:\n",
    "bi_frq_l = list(it.islice(((bi, frq) for bi, frq in fdist_bi.most_common(n=10000) \\\n",
    "                           if len(set(bi) & set(('_', '__'))) == 0), 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('что', 'он'),\n",
       "  (Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  642),\n",
       " (('и', 'не'),\n",
       "  (Counter({'CONJ': 0.997671,\n",
       "            'INTJ': 0.000436,\n",
       "            'PRCL': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'PRCL': 1.0})),\n",
       "  518),\n",
       " (('я', 'не'), (Counter({'NPRO': 1.0}), Counter({'PRCL': 1.0})), 495),\n",
       " (('то', 'что'),\n",
       "  (Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}),\n",
       "   Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925})),\n",
       "  472),\n",
       " (('сказал', 'он'), (Counter({'VERB': 1.0}), Counter({'NPRO': 1.0})), 455),\n",
       " (('он', 'не'), (Counter({'NPRO': 1.0}), Counter({'PRCL': 1.0})), 454),\n",
       " (('степан', 'аркадьич'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  422),\n",
       " (('алексей', 'александрович'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  395),\n",
       " (('сказала', 'она'), (Counter({'VERB': 1.0}), Counter({'NPRO': 1.0})), 362),\n",
       " (('что', 'она'),\n",
       "  (Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  343),\n",
       " (('о', 'том'),\n",
       "  (Counter({'PREP': 0.990985, 'INTJ': 0.009014}),\n",
       "   Counter({'ADJF': 0.827585, 'NOUN': 0.17241})),\n",
       "  279),\n",
       " (('и', 'он'),\n",
       "  (Counter({'CONJ': 0.997671,\n",
       "            'INTJ': 0.000436,\n",
       "            'PRCL': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  266),\n",
       " (('не', 'мог'), (Counter({'PRCL': 1.0}), Counter({'VERB': 1.0})), 254),\n",
       " (('она', 'не'), (Counter({'NPRO': 1.0}), Counter({'PRCL': 1.0})), 248),\n",
       " (('что', 'я'),\n",
       "  (Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  247),\n",
       " (('и', 'в'),\n",
       "  (Counter({'CONJ': 0.997671,\n",
       "            'INTJ': 0.000436,\n",
       "            'PRCL': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'PREP': 0.999764, 'NOUN': 0.000228})),\n",
       "  245),\n",
       " (('ничего', 'не'),\n",
       "  (Counter({'NPRO': 0.6, 'ADVB': 0.2, 'PRCL': 0.2}), Counter({'PRCL': 1.0})),\n",
       "  236),\n",
       " (('потому', 'что'),\n",
       "  (Counter({'ADVB': 1.0}),\n",
       "   Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925})),\n",
       "  230),\n",
       " (('и', 'что'),\n",
       "  (Counter({'CONJ': 0.997671,\n",
       "            'INTJ': 0.000436,\n",
       "            'PRCL': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925})),\n",
       "  229),\n",
       " (('что', 'это'),\n",
       "  (Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925}),\n",
       "   Counter({'PRCL': 0.35, 'NPRO': 0.44999999999999996, 'ADJF': 0.2})),\n",
       "  221),\n",
       " (('с', 'ним'),\n",
       "  (Counter({'PREP': 0.998363, 'NOUN': 0.0016319999999999996}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  209),\n",
       " (('том', 'что'),\n",
       "  (Counter({'ADJF': 0.827585, 'NOUN': 0.17241}),\n",
       "   Counter({'CONJ': 0.791044,\n",
       "            'NPRO': 0.164178,\n",
       "            'PRCL': 0.02985,\n",
       "            'ADVB': 0.014925})),\n",
       "  209),\n",
       " (('на', 'него'),\n",
       "  (Counter({'PREP': 0.99931, 'PRCL': 0.000477, 'INTJ': 0.000212}),\n",
       "   Counter({'NPRO': 0.9999979999999999})),\n",
       "  200),\n",
       " (('может', 'быть'),\n",
       "  (Counter({'VERB': 0.944444, 'CONJ': 0.055555}), Counter({'INFN': 1.0})),\n",
       "  198),\n",
       " (('к', 'ней'),\n",
       "  (Counter({'PREP': 0.998113,\n",
       "            'ADVB': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  193),\n",
       " (('сергей', 'иванович'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  188),\n",
       " (('как', 'он'),\n",
       "  (Counter({'CONJ': 0.875, 'ADVB': 0.09375, 'PRCL': 0.03125}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  185),\n",
       " (('к', 'нему'),\n",
       "  (Counter({'PREP': 0.998113,\n",
       "            'ADVB': 0.000145,\n",
       "            'NOUN': 0.0017399999999999996}),\n",
       "   Counter({'NPRO': 1.0})),\n",
       "  180),\n",
       " (('он', 'был'), (Counter({'NPRO': 1.0}), Counter({'VERB': 1.0})), 180),\n",
       " (('не', 'было'),\n",
       "  (Counter({'PRCL': 1.0}), Counter({'VERB': 0.963576, 'PRCL': 0.036423})),\n",
       "  179)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# создаем тегированный список:\n",
    "bi_tag_frq_l = [(bi, tuple(map(word_to_pos_frq_dict2, bi)), frq) for bi, frq in bi_frq_l]\n",
    "bi_tag_frq_l[:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# набор шаблонов:\n",
    "masks = [('ADJF', 'NOUN'), ('NOUN', 'NOUN')]\n",
    "# подбор наиболее подходящего шаблона:\n",
    "def get_max_prob_for_mask(counters, masks=masks):\n",
    "    prob_list = []\n",
    "    for mask in masks:\n",
    "        cur_prob = 1.0\n",
    "        for i, pos in enumerate(mask):\n",
    "            cur_prob *= counters[i].get(pos, 0.0)\n",
    "        prob_list.append(cur_prob)\n",
    "    return max(prob_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('степан', 'аркадьич'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  422,\n",
       "  1.0),\n",
       " (('алексей', 'александрович'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  395,\n",
       "  1.0),\n",
       " (('сергей', 'иванович'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  188,\n",
       "  1.0),\n",
       " (('дарья', 'александровна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  141,\n",
       "  1.0),\n",
       " (('алексея', 'александровича'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  117,\n",
       "  1.0),\n",
       " (('степана', 'аркадьича'),\n",
       "  (Counter({'NOUN': 1.0}),\n",
       "   Counter({'NOUN': 0.9830508474576272,\n",
       "            'GRND': 0.011299435028248588,\n",
       "            'ADVB': 0.005649717514124294})),\n",
       "  84,\n",
       "  0.9830508474576272),\n",
       " (('лидия', 'ивановна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  66,\n",
       "  1.0),\n",
       " (('графиня', 'лидия'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  51,\n",
       "  1.0),\n",
       " (('тех', 'пор'),\n",
       "  (Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 1.0})),\n",
       "  48,\n",
       "  0.9999990000000001),\n",
       " (('то', 'время'),\n",
       "  (Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}),\n",
       "   Counter({'NOUN': 0.999999})),\n",
       "  48,\n",
       "  0.3999996),\n",
       " (('агафья', 'михайловна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  43,\n",
       "  1.0),\n",
       " (('ее', 'руку'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  41,\n",
       "  0.8333250000000005),\n",
       " (('первый', 'раз'),\n",
       "  (Counter({'ADJF': 0.9999990000000001}),\n",
       "   Counter({'NOUN': 0.666665, 'ADVB': 0.166666, 'CONJ': 0.166666})),\n",
       "  41,\n",
       "  0.666664333335),\n",
       " (('алексею', 'александровичу'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  40,\n",
       "  1.0),\n",
       " (('сергея', 'ивановича'),\n",
       "  (Counter({'NOUN': 0.9999990000000001}), Counter({'NOUN': 1.0})),\n",
       "  39,\n",
       "  0.9999990000000001),\n",
       " (('последнее', 'время'),\n",
       "  (Counter({'ADJF': 0.6666666666666666, 'COMP': 0.3333333333333333}),\n",
       "   Counter({'NOUN': 0.999999})),\n",
       "  35,\n",
       "  0.666666),\n",
       " (('дарье', 'александровне'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 0.999999})),\n",
       "  31,\n",
       "  0.999999),\n",
       " (('константин', 'левин'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 0.6, 'ADJF': 0.4})),\n",
       "  31,\n",
       "  0.6),\n",
       " (('его', 'руку'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  31,\n",
       "  0.595732),\n",
       " (('первое', 'время'),\n",
       "  (Counter({'NOUN': 0.5, 'ADJF': 0.5}), Counter({'NOUN': 0.999999})),\n",
       "  31,\n",
       "  0.4999995),\n",
       " (('друг', 'друга'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 0.999999})),\n",
       "  29,\n",
       "  0.999999),\n",
       " (('константин', 'дмитрич'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  29,\n",
       "  1.0),\n",
       " (('мадам', 'шталь'),\n",
       "  (Counter({'NOUN': 0.9999989999999999}),\n",
       "   Counter({'NOUN': 0.9965156794425086, 'ADVB': 0.0034843205574912888})),\n",
       "  29,\n",
       "  0.9965146829268291),\n",
       " (('анна', 'аркадьевна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  28,\n",
       "  1.0),\n",
       " (('каждый', 'раз'),\n",
       "  (Counter({'ADJF': 0.999999}),\n",
       "   Counter({'NOUN': 0.666665, 'ADVB': 0.166666, 'CONJ': 0.166666})),\n",
       "  28,\n",
       "  0.666664333335),\n",
       " (('эти', 'слова'),\n",
       "  (Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 0.999999})),\n",
       "  27,\n",
       "  0.999998000001),\n",
       " (('крайней', 'мере'),\n",
       "  (Counter({'ADJF': 0.8333309999999999, 'COMP': 0.166666}),\n",
       "   Counter({'NOUN': 1.333332})),\n",
       "  26,\n",
       "  1.111106888892),\n",
       " (('степану', 'аркадьичу'),\n",
       "  (Counter({'NOUN': 1.0}),\n",
       "   Counter({'NOUN': 0.9354838709677419, 'VERB': 0.06451612903225806})),\n",
       "  24,\n",
       "  0.9354838709677419),\n",
       " (('старый', 'князь'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 1.0})),\n",
       "  23,\n",
       "  0.999999),\n",
       " (('сих', 'пор'),\n",
       "  (Counter({'ADJF': 0.9999979999999999}), Counter({'NOUN': 1.0})),\n",
       "  23,\n",
       "  0.9999979999999999),\n",
       " (('его', 'лицо'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  22,\n",
       "  0.595732),\n",
       " (('слава', 'богу'),\n",
       "  (Counter({'NOUN': 0.999999}), Counter({'NOUN': 1.0})),\n",
       "  22,\n",
       "  0.999999),\n",
       " (('этот', 'день'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})),\n",
       "  22,\n",
       "  0.967740032259),\n",
       " (('другой', 'день'),\n",
       "  (Counter({'ADJF': 0.999998}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})),\n",
       "  21,\n",
       "  0.9677390645180001),\n",
       " (('все', 'время'),\n",
       "  (Counter({'ADJF': 0.857141, 'ADVB': 0.142857}), Counter({'NOUN': 0.999999})),\n",
       "  21,\n",
       "  0.857140142859),\n",
       " (('ее', 'лицо'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  21,\n",
       "  0.8333250000000005),\n",
       " (('ту', 'минуту'), (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})), 21, 1.0),\n",
       " (('дарьи', 'александровны'),\n",
       "  (Counter({'NOUN': 0.999999}), Counter({'NOUN': 1.0})),\n",
       "  20,\n",
       "  0.999999),\n",
       " (('свою', 'жизнь'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999990000000001})),\n",
       "  20,\n",
       "  0.9999990000000001),\n",
       " (('марья', 'николаевна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  20,\n",
       "  1.0),\n",
       " (('лидии', 'ивановны'),\n",
       "  (Counter({'NOUN': 0.9999999999999999}), Counter({'NOUN': 0.999999})),\n",
       "  20,\n",
       "  0.9999989999999999),\n",
       " (('сергею', 'ивановичу'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  20,\n",
       "  1.0),\n",
       " (('другой', 'стороны'),\n",
       "  (Counter({'ADJF': 0.999998}), Counter({'NOUN': 0.9999979999999999})),\n",
       "  19,\n",
       "  0.999996000004),\n",
       " (('агафьи', 'михайловны'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  19,\n",
       "  1.0),\n",
       " (('первую', 'минуту'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  19,\n",
       "  1.0),\n",
       " (('степаном', 'аркадьичем'),\n",
       "  (Counter({'NOUN': 1.0}),\n",
       "   Counter({'NOUN': 0.8969072164948455, 'VERB': 0.10309278350515466})),\n",
       "  18,\n",
       "  0.8969072164948455),\n",
       " (('сергей', 'иваныч'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  18,\n",
       "  1.0),\n",
       " (('алексеем', 'александровичем'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  18,\n",
       "  1.0),\n",
       " (('эту', 'минуту'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  17,\n",
       "  1.0),\n",
       " (('своего', 'положения'),\n",
       "  (Counter({'ADJF': 0.999998}), Counter({'NOUN': 0.9999979999999999})),\n",
       "  17,\n",
       "  0.999996000004),\n",
       " (('каждый', 'день'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})),\n",
       "  17,\n",
       "  0.967740032259),\n",
       " (('графиня', 'нордстон'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  16,\n",
       "  1.0),\n",
       " (('молодой', 'человек'),\n",
       "  (Counter({'NOUN': 0.4, 'ADJF': 0.6}), Counter({'NOUN': 0.9999990000000001})),\n",
       "  16,\n",
       "  0.5999994000000001),\n",
       " (('выражение', 'лица'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 0.9999990000000001})),\n",
       "  16,\n",
       "  0.9999990000000001),\n",
       " (('последний', 'раз'),\n",
       "  (Counter({'ADJF': 0.999999}),\n",
       "   Counter({'NOUN': 0.666665, 'ADVB': 0.166666, 'CONJ': 0.166666})),\n",
       "  16,\n",
       "  0.666664333335),\n",
       " (('расположении', 'духа'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  16,\n",
       "  1.0),\n",
       " (('всех', 'сторон'),\n",
       "  (Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 1.0})),\n",
       "  16,\n",
       "  0.9999990000000001),\n",
       " (('княжна', 'варвара'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  16,\n",
       "  1.0),\n",
       " (('все', 'подробности'),\n",
       "  (Counter({'ADJF': 0.857141, 'ADVB': 0.142857}), Counter({'NOUN': 1.0})),\n",
       "  15,\n",
       "  0.857141),\n",
       " (('ее', 'лице'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 0.999999})),\n",
       "  15,\n",
       "  0.8333241666750005),\n",
       " (('ее', 'лица'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 0.9999990000000001})),\n",
       "  15,\n",
       "  0.8333241666750006),\n",
       " (('этот', 'вопрос'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.999999})),\n",
       "  15,\n",
       "  0.9999980000009999),\n",
       " (('свое', 'положение'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.999999})),\n",
       "  15,\n",
       "  0.9999980000009999),\n",
       " (('то', 'чувство'),\n",
       "  (Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}),\n",
       "   Counter({'NOUN': 0.9999990000000001})),\n",
       "  15,\n",
       "  0.39999960000000007),\n",
       " (('глубине', 'души'),\n",
       "  (Counter({'NOUN': 0.999999}), Counter({'NOUN': 0.979165, 'VERB': 0.020833})),\n",
       "  15,\n",
       "  0.9791640208349999),\n",
       " (('свою', 'руку'), (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})), 14, 1.0),\n",
       " (('николай', 'левин'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 0.6, 'ADJF': 0.4})),\n",
       "  14,\n",
       "  0.6),\n",
       " (('анне', 'аркадьевне'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  14,\n",
       "  1.0),\n",
       " (('графини', 'лидии'),\n",
       "  (Counter({'NOUN': 0.999999}), Counter({'NOUN': 0.9999999999999999})),\n",
       "  14,\n",
       "  0.9999989999999999),\n",
       " (('целый', 'день'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})),\n",
       "  14,\n",
       "  0.967740032259),\n",
       " (('лизавета', 'петровна'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  14,\n",
       "  1.0),\n",
       " (('все', 'дело'),\n",
       "  (Counter({'ADJF': 0.857141, 'ADVB': 0.142857}),\n",
       "   Counter({'NOUN': 0.970587, 'VERB': 0.029411})),\n",
       "  13,\n",
       "  0.831929911767),\n",
       " (('самом', 'деле'),\n",
       "  (Counter({'ADJF': 1.2}), Counter({'NOUN': 1.0})),\n",
       "  13,\n",
       "  1.2),\n",
       " (('полковой', 'командир'),\n",
       "  (Counter({'ADJF': 0.9999999999999999}), Counter({'NOUN': 1.0})),\n",
       "  13,\n",
       "  0.9999999999999999),\n",
       " (('ее', 'руки'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 0.999999})),\n",
       "  13,\n",
       "  0.8333241666750005),\n",
       " (('другую', 'сторону'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  13,\n",
       "  1.0),\n",
       " (('свою', 'мысль'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  13,\n",
       "  1.0),\n",
       " (('губернский', 'предводитель'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  13,\n",
       "  1.0),\n",
       " (('его', 'лица'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 0.9999990000000001})),\n",
       "  12,\n",
       "  0.5957314042680001),\n",
       " (('всю', 'ночь'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999990000000001})),\n",
       "  12,\n",
       "  0.9999990000000001),\n",
       " (('анны', 'аркадьевны'),\n",
       "  (Counter({'NOUN': 0.999999}), Counter({'NOUN': 1.0})),\n",
       "  12,\n",
       "  0.999999),\n",
       " (('его', 'лице'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 0.999999})),\n",
       "  12,\n",
       "  0.595731404268),\n",
       " (('его', 'жизни'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 0.9999960000000001})),\n",
       "  12,\n",
       "  0.5957296170720001),\n",
       " (('высшей', 'степени'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  12,\n",
       "  1.0),\n",
       " (('самое', 'время'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.999999})),\n",
       "  12,\n",
       "  0.9999980000009999),\n",
       " (('ее', 'муж'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  12,\n",
       "  0.8333250000000005),\n",
       " (('ее', 'мужа'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  12,\n",
       "  0.8333250000000005),\n",
       " (('одной', 'стороны'),\n",
       "  (Counter({'ADJF': 0.999998}), Counter({'NOUN': 0.9999979999999999})),\n",
       "  11,\n",
       "  0.999996000004),\n",
       " (('подобное', 'тому'),\n",
       "  (Counter({'ADJF': 1.0}),\n",
       "   Counter({'ADJF': 0.571428, 'NOUN': 0.42857100000000004})),\n",
       "  11,\n",
       "  0.42857100000000004),\n",
       " (('такой', 'степени'),\n",
       "  (Counter({'ADJF': 0.999998}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  0.999998),\n",
       " (('этот', 'взгляд'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.999999})),\n",
       "  11,\n",
       "  0.9999980000009999),\n",
       " (('лидию', 'ивановну'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  1.0),\n",
       " (('знаменитый', 'доктор'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  1.0),\n",
       " (('княгиня', 'бетси'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  1.0),\n",
       " (('дарью', 'александровну'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  1.0),\n",
       " (('василий', 'лукич'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  11,\n",
       "  1.0),\n",
       " (('третий', 'день'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})),\n",
       "  10,\n",
       "  0.967741),\n",
       " (('всю', 'жизнь'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999990000000001})),\n",
       "  10,\n",
       "  0.9999990000000001),\n",
       " (('брат', 'николай'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  10,\n",
       "  1.0),\n",
       " (('сергеем', 'ивановичем'),\n",
       "  (Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})),\n",
       "  10,\n",
       "  1.0),\n",
       " (('его', 'слов'),\n",
       "  (Counter({'NPRO': 0.40425300000000003, 'ADJF': 0.595732}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  10,\n",
       "  0.595732),\n",
       " (('ее', 'душе'),\n",
       "  (Counter({'NPRO': 0.166666, 'ADJF': 0.8333250000000005}),\n",
       "   Counter({'NOUN': 1.0})),\n",
       "  10,\n",
       "  0.8333250000000005),\n",
       " (('семейной', 'жизни'),\n",
       "  (Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999960000000001})),\n",
       "  10,\n",
       "  0.9999960000000001),\n",
       " (('этот', 'вечер'),\n",
       "  (Counter({'ADJF': 0.999999}), Counter({'NOUN': 1.0})),\n",
       "  10,\n",
       "  0.999999)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bi_tag_frq_mask_l = [(bi, bi_count, frq, get_max_prob_for_mask(bi_count)) for bi, bi_count, frq in bi_tag_frq_l[:2000]]\n",
    "bi_tag_frq_mask_l = [(bi, bi_count, frq, prob) for bi, bi_count, frq, prob in bi_tag_frq_mask_l if prob > 0.25]\n",
    "bi_tag_frq_mask_l[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Устойичвые словосочетания не обязательно представляют собой последовательность подряд идущих слов.\n",
    "\n",
    "\n",
    "<center> \n",
    "__Пример: гистограмма частот для слов *strong* и *for*:__\n",
    "\n",
    "Пример употребления: \"*strong* business support *for*\"\n",
    "\n",
    "<img src=\"pos_histogram.png\" alt=\"Саме высокочастотные биграммы\" style=\"width: 500px;\"/>\n",
    "\n",
    "</center>\n",
    "\n",
    "Для построения гистограммы можно пройти скользящим окном по тексту.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Bigram = namedtuple('Bigram', 'bi counters frq prob frq_l')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bigram(bi=('степан', 'аркадьич'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=422, prob=1.0, frq_l=[0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = Bigram(*bi_tag_frq_mask_l[0], [0]*5)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bigram(bi=('степан', 'аркадьич'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=422, prob=1.0, frq_l=[0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# работа с Bigram.frq_l:\n",
    "b.frq_l[1] += 1\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('степан', 'аркадьич'),\n",
       "  Bigram(bi=('степан', 'аркадьич'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=422, prob=1.0, frq_l=[0, 0, 0, 0, 0])),\n",
       " (('алексей', 'александрович'),\n",
       "  Bigram(bi=('алексей', 'александрович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=395, prob=1.0, frq_l=[0, 0, 0, 0, 0])),\n",
       " (('сергей', 'иванович'),\n",
       "  Bigram(bi=('сергей', 'иванович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=188, prob=1.0, frq_l=[0, 0, 0, 0, 0]))]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# словарь для хранения результатов:\n",
    "bigram_l = {b[0]: Bigram(*b, [0]*5) for b in bi_tag_frq_mask_l}\n",
    "list(it.islice(bigram_l.items(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_', 'лев', 'николаевич', 'толстой', 'анна']"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "window = book_tokens_ss2[:5]\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['николаевич', 'толстой', 'анна', 'мне', 'каренина']"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# механизм смещения окна:\n",
    "window.pop(0)\n",
    "window.append(book_tokens_ss2[5])\n",
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sliding_window_frq_count(tokens, bi_dict, window_length=5):\n",
    "    cur_w = [None] + tokens[:window_length-1] # current window\n",
    "    for t in tokens[window_length:]:\n",
    "        # смещаем окно:\n",
    "        cur_w.pop(0) \n",
    "        cur_w.append(t)\n",
    "        for i, w2 in enumerate(cur_w[1:], 1): # второе слово биграммы и индекс его смещения\n",
    "            bigram = bi_dict.get((cur_w[0], w2)) # ищем биграмму в словаре\n",
    "            if bigram:\n",
    "                bigram.frq_l[i] += 1    \n",
    "    return bi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('степан', 'аркадьич'),\n",
       "  Bigram(bi=('степан', 'аркадьич'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=422, prob=1.0, frq_l=[0, 422, 0, 0, 0])),\n",
       " (('алексей', 'александрович'),\n",
       "  Bigram(bi=('алексей', 'александрович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=395, prob=1.0, frq_l=[0, 395, 0, 0, 0])),\n",
       " (('сергей', 'иванович'),\n",
       "  Bigram(bi=('сергей', 'иванович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=188, prob=1.0, frq_l=[0, 188, 0, 0, 0]))]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sliding_window_frq_count(book_tokens_ss2, bigram_l)\n",
    "list(it.islice(bigram_l.items(), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Bigram(bi=('степан', 'аркадьич'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=422, prob=1.0, frq_l=[0, 422, 0, 0, 0]),\n",
       " Bigram(bi=('алексей', 'александрович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=395, prob=1.0, frq_l=[0, 395, 0, 0, 0]),\n",
       " Bigram(bi=('сергей', 'иванович'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=188, prob=1.0, frq_l=[0, 188, 0, 0, 0]),\n",
       " Bigram(bi=('дарья', 'александровна'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=141, prob=1.0, frq_l=[0, 141, 3, 0, 0]),\n",
       " Bigram(bi=('алексея', 'александровича'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=117, prob=1.0, frq_l=[0, 117, 0, 0, 0]),\n",
       " Bigram(bi=('степана', 'аркадьича'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.9830508474576272, 'GRND': 0.011299435028248588, 'ADVB': 0.005649717514124294})), frq=84, prob=0.9830508474576272, frq_l=[0, 84, 0, 0, 0]),\n",
       " Bigram(bi=('то', 'время'), counters=(Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}), Counter({'NOUN': 0.999999})), frq=48, prob=0.3999996, frq_l=[0, 48, 17, 3, 1]),\n",
       " Bigram(bi=('лидия', 'ивановна'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=66, prob=1.0, frq_l=[0, 66, 0, 0, 0]),\n",
       " Bigram(bi=('ее', 'руку'), counters=(Counter({'ADJF': 0.8333250000000005, 'NPRO': 0.166666}), Counter({'NOUN': 1.0})), frq=41, prob=0.8333250000000005, frq_l=[0, 41, 19, 0, 2]),\n",
       " Bigram(bi=('его', 'руку'), counters=(Counter({'ADJF': 0.595732, 'NPRO': 0.40425300000000003}), Counter({'NOUN': 1.0})), frq=31, prob=0.595732, frq_l=[0, 31, 24, 0, 3]),\n",
       " Bigram(bi=('графиня', 'лидия'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=51, prob=1.0, frq_l=[0, 51, 0, 0, 0]),\n",
       " Bigram(bi=('тех', 'пор'), counters=(Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 1.0})), frq=48, prob=0.9999990000000001, frq_l=[0, 48, 0, 0, 0]),\n",
       " Bigram(bi=('агафья', 'михайловна'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=43, prob=1.0, frq_l=[0, 43, 0, 0, 0]),\n",
       " Bigram(bi=('первый', 'раз'), counters=(Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 0.666665, 'ADVB': 0.166666, 'CONJ': 0.166666})), frq=41, prob=0.666664333335, frq_l=[0, 41, 1, 1, 0]),\n",
       " Bigram(bi=('алексею', 'александровичу'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=40, prob=1.0, frq_l=[0, 40, 1, 0, 0]),\n",
       " Bigram(bi=('его', 'лицо'), counters=(Counter({'ADJF': 0.595732, 'NPRO': 0.40425300000000003}), Counter({'NOUN': 1.0})), frq=22, prob=0.595732, frq_l=[0, 22, 11, 5, 3]),\n",
       " Bigram(bi=('сергея', 'ивановича'), counters=(Counter({'NOUN': 0.9999990000000001}), Counter({'NOUN': 1.0})), frq=39, prob=0.9999990000000001, frq_l=[0, 39, 0, 0, 0]),\n",
       " Bigram(bi=('друг', 'друга'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.999999})), frq=29, prob=0.999999, frq_l=[0, 29, 7, 0, 0]),\n",
       " Bigram(bi=('последнее', 'время'), counters=(Counter({'ADJF': 0.6666666666666666, 'COMP': 0.3333333333333333}), Counter({'NOUN': 0.999999})), frq=35, prob=0.666666, frq_l=[0, 35, 0, 0, 0]),\n",
       " Bigram(bi=('константин', 'левин'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.6, 'ADJF': 0.4})), frq=31, prob=0.6, frq_l=[0, 31, 3, 0, 1])]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_cum_frq_l = sorted(bigram_l.values(), key=lambda _:sum(_.frq_l), reverse=True)\n",
    "bigram_cum_frq_l[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Bigram(bi=('его', 'руку'), counters=(Counter({'ADJF': 0.595732, 'NPRO': 0.40425300000000003}), Counter({'NOUN': 1.0})), frq=31, prob=0.595732, frq_l=[0, 31, 24, 0, 3]),\n",
       " Bigram(bi=('ее', 'руку'), counters=(Counter({'ADJF': 0.8333250000000005, 'NPRO': 0.166666}), Counter({'NOUN': 1.0})), frq=41, prob=0.8333250000000005, frq_l=[0, 41, 19, 0, 2]),\n",
       " Bigram(bi=('то', 'время'), counters=(Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}), Counter({'NOUN': 0.999999})), frq=48, prob=0.3999996, frq_l=[0, 48, 17, 3, 1]),\n",
       " Bigram(bi=('его', 'лицо'), counters=(Counter({'ADJF': 0.595732, 'NPRO': 0.40425300000000003}), Counter({'NOUN': 1.0})), frq=22, prob=0.595732, frq_l=[0, 22, 11, 5, 3]),\n",
       " Bigram(bi=('выражение', 'лица'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.9999990000000001})), frq=16, prob=0.9999990000000001, frq_l=[0, 16, 11, 1, 0]),\n",
       " Bigram(bi=('друг', 'друга'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.999999})), frq=29, prob=0.999999, frq_l=[0, 29, 7, 0, 0]),\n",
       " Bigram(bi=('все', 'время'), counters=(Counter({'ADJF': 0.857141, 'ADVB': 0.142857}), Counter({'NOUN': 0.999999})), frq=21, prob=0.857140142859, frq_l=[0, 21, 6, 5, 2]),\n",
       " Bigram(bi=('ту', 'минуту'), counters=(Counter({'ADJF': 1.0}), Counter({'NOUN': 1.0})), frq=21, prob=1.0, frq_l=[0, 21, 6, 0, 0]),\n",
       " Bigram(bi=('то', 'чувство'), counters=(Counter({'CONJ': 0.5, 'ADJF': 0.4, 'PRCL': 0.1}), Counter({'NOUN': 0.9999990000000001})), frq=15, prob=0.39999960000000007, frq_l=[0, 15, 6, 0, 3]),\n",
       " Bigram(bi=('всю', 'жизнь'), counters=(Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999990000000001})), frq=10, prob=0.9999990000000001, frq_l=[0, 10, 6, 0, 0]),\n",
       " Bigram(bi=('ее', 'руки'), counters=(Counter({'ADJF': 0.8333250000000005, 'NPRO': 0.166666}), Counter({'NOUN': 0.999999})), frq=13, prob=0.8333241666750005, frq_l=[0, 13, 5, 4, 0]),\n",
       " Bigram(bi=('самое', 'время'), counters=(Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.999999})), frq=12, prob=0.9999980000009999, frq_l=[0, 12, 4, 1, 1]),\n",
       " Bigram(bi=('дарья', 'александровна'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 1.0})), frq=141, prob=1.0, frq_l=[0, 141, 3, 0, 0]),\n",
       " Bigram(bi=('константин', 'левин'), counters=(Counter({'NOUN': 1.0}), Counter({'NOUN': 0.6, 'ADJF': 0.4})), frq=31, prob=0.6, frq_l=[0, 31, 3, 0, 1]),\n",
       " Bigram(bi=('эти', 'слова'), counters=(Counter({'ADJF': 0.9999990000000001}), Counter({'NOUN': 0.999999})), frq=27, prob=0.999998000001, frq_l=[0, 27, 3, 1, 1]),\n",
       " Bigram(bi=('этот', 'день'), counters=(Counter({'ADJF': 0.999999}), Counter({'NOUN': 0.967741, 'VERB': 0.032258})), frq=22, prob=0.967740032259, frq_l=[0, 22, 3, 1, 0]),\n",
       " Bigram(bi=('свою', 'жизнь'), counters=(Counter({'ADJF': 1.0}), Counter({'NOUN': 0.9999990000000001})), frq=20, prob=0.9999990000000001, frq_l=[0, 20, 3, 2, 1]),\n",
       " Bigram(bi=('ее', 'лица'), counters=(Counter({'ADJF': 0.8333250000000005, 'NPRO': 0.166666}), Counter({'NOUN': 0.9999990000000001})), frq=15, prob=0.8333241666750006, frq_l=[0, 15, 3, 0, 1]),\n",
       " Bigram(bi=('глубине', 'души'), counters=(Counter({'NOUN': 0.999999}), Counter({'NOUN': 0.979165, 'VERB': 0.020833})), frq=15, prob=0.9791640208349999, frq_l=[0, 15, 3, 0, 0]),\n",
       " Bigram(bi=('все', 'дело'), counters=(Counter({'ADJF': 0.857141, 'ADVB': 0.142857}), Counter({'NOUN': 0.970587, 'VERB': 0.029411})), frq=13, prob=0.831929911767, frq_l=[0, 13, 3, 0, 1])]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_cum_frq_l = sorted(bigram_l.values(), key=lambda _:_.frq_l[2], reverse=True)\n",
    "bigram_cum_frq_l[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "$$\\bar{d} = \\frac{\\sum_{i=1}^{n} d_i}{n}$$\n",
    "\n",
    "$$s^2 = \\frac{\\sum_{i=1}^{n} (d_i-\\bar{d})^2}{n-1}$$\n",
    "\n",
    "Где:\n",
    "* $n$ - число раз, когда два слова встретились\n",
    "* $d_i$ - смещение между словами\n",
    "* $\\bar{d}$ - выборочное среднее (sample mean) смещений\n",
    "* $s^2$ - несмещённая выборочная дисперсия (unbiased sample variance) смещений"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_mean_unb_var(frq_l):\n",
    "    n = sum(frq_l)\n",
    "    sm = sum(i * frq for i, frq in enumerate(frq_l)) / n\n",
    "    uv = sum((i - sm) ** 2 * frq for i, frq in enumerate(frq_l)) / (n - 1)\n",
    "    return sm, uv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('его', 'руку'),\n",
       "  (1.5689655172413792, 0.5653357531760437),\n",
       "  [0, 31, 24, 0, 3]),\n",
       " (('ее', 'руку'), (1.403225806451613, 0.44130089899524066), [0, 41, 19, 0, 2]),\n",
       " (('то', 'время'),\n",
       "  (1.3768115942028984, 0.41474850809889174),\n",
       "  [0, 48, 17, 3, 1]),\n",
       " (('его', 'лицо'),\n",
       "  (1.7317073170731707, 0.9012195121951221),\n",
       "  [0, 22, 11, 5, 3]),\n",
       " (('выражение', 'лица'),\n",
       "  (1.4642857142857142, 0.332010582010582),\n",
       "  [0, 16, 11, 1, 0]),\n",
       " (('друг', 'друга'),\n",
       "  (1.1944444444444444, 0.1611111111111111),\n",
       "  [0, 29, 7, 0, 0]),\n",
       " (('все', 'время'),\n",
       "  (1.6470588235294117, 0.9019607843137255),\n",
       "  [0, 21, 6, 5, 2]),\n",
       " (('ту', 'минуту'),\n",
       "  (1.2222222222222223, 0.17948717948717946),\n",
       "  [0, 21, 6, 0, 0]),\n",
       " (('то', 'чувство'), (1.625, 1.0271739130434783), [0, 15, 6, 0, 3]),\n",
       " (('всю', 'жизнь'), (1.375, 0.25), [0, 10, 6, 0, 0]),\n",
       " (('ее', 'руки'), (1.5909090909090908, 0.6341991341991341), [0, 13, 5, 4, 0]),\n",
       " (('самое', 'время'), (1.5, 0.7352941176470589), [0, 12, 4, 1, 1]),\n",
       " (('дарья', 'александровна'),\n",
       "  (1.0208333333333333, 0.02054195804195804),\n",
       "  [0, 141, 3, 0, 0]),\n",
       " (('константин', 'левин'),\n",
       "  (1.1714285714285715, 0.3226890756302521),\n",
       "  [0, 31, 3, 0, 1]),\n",
       " (('эти', 'слова'), (1.25, 0.45161290322580644), [0, 27, 3, 1, 1]),\n",
       " (('этот', 'день'),\n",
       "  (1.1923076923076923, 0.24153846153846154),\n",
       "  [0, 22, 3, 1, 0]),\n",
       " (('свою', 'жизнь'),\n",
       "  (1.3846153846153846, 0.6461538461538461),\n",
       "  [0, 20, 3, 2, 1]),\n",
       " (('ее', 'лица'), (1.3157894736842106, 0.5614035087719298), [0, 15, 3, 0, 1]),\n",
       " (('глубине', 'души'),\n",
       "  (1.1666666666666667, 0.14705882352941177),\n",
       "  [0, 15, 3, 0, 0]),\n",
       " (('все', 'дело'), (1.3529411764705883, 0.6176470588235294), [0, 13, 3, 0, 1])]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram_cum_frq_smuv_l = [(b.bi, sample_mean_unb_var(b.frq_l), b.frq_l) for b in bigram_cum_frq_l]\n",
    "bigram_cum_frq_smuv_l[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
