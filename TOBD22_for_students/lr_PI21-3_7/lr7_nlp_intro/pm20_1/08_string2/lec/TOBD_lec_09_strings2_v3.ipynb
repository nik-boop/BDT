{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Лекция 9: Введение в обработку текста на естественном языке\n",
    "\n",
    "__Автор: Сергей Вячеславович Макрушин__ e-mail: SVMakrushin@fa.ru \n",
    "\n",
    "Финансовый универсиет, 2020 г. \n",
    "\n",
    "При подготовке лекции использованы материалы:\n",
    "* ...\n",
    "\n",
    "V 0.1 18.10.2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Разделы: <a class=\"anchor\" id=\"разделы\"></a>\n",
    "* [Серии (Series) - одномерные массивы в Pandas](#серии)\n",
    "* [Датафрэйм (DataFrame) - двумерные массивы в Pandas](#датафрэйм)\n",
    "    * [Введение](#датафрэйм-введение)\n",
    "    * [Индексация](#датафрэйм-индексация)    \n",
    "* [Обработка данных в библиотеке Pandas](#обработка-данных)\n",
    "    * [Универсальные функции и выравнивание](#обработка-данных-универсальные)\n",
    "    * [Работа с пустыми значениями](#обработка-данных-пустрые-значения)\n",
    "    * [Агрегирование и группировка](#обработка-данных-агрегирование)    \n",
    "* [Обработка нескольких наборов данных](#обработка-нескольких)\n",
    "    * [Объединение наборов данных](#обработка-нескольких-объединение)\n",
    "    * [GroupBy: разбиение, применение, объединение](#обработка-нескольких-групбай)\n",
    " \n",
    "-\n",
    "\n",
    "* [к оглавлению](#разделы)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* расстояние левенштейна\n",
    "\n",
    "* стемминг / лемматизация\n",
    "\n",
    "* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "﻿<style>\r\n",
       "\r\n",
       "\r\n",
       "b.n {\r\n",
       "    font-weight: normal;        \r\n",
       "}\r\n",
       "\r\n",
       "b.grbg {\r\n",
       "    background-color: #a0a0a0;      \r\n",
       "}\r\n",
       "\r\n",
       "b.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "b.b {    \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "b.g {\r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "// add your CSS styling here\r\n",
       "\r\n",
       "list-style: none;\r\n",
       "\r\n",
       "ul.s {\r\n",
       "//    list-style-type: none;\r\n",
       "    list-style: none;\r\n",
       "//    background-color: #ff0000;  \r\n",
       "//    color: #ffff00;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;\r\n",
       "}\r\n",
       "\r\n",
       "li.t {\r\n",
       "    list-style: none;\r\n",
       "//  padding-left: 1.2em;\r\n",
       "//  text-indent: -1.2em;    \r\n",
       "}\r\n",
       "\r\n",
       "\r\n",
       "*.r {\r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "li.t:before {\r\n",
       "    content: \"\\21D2\";    \r\n",
       "//    content: \"►\";\r\n",
       "//    padding-left: -1.2em;    \r\n",
       "    text-indent: -1.2em;    \r\n",
       "    display: block;\r\n",
       "    float: left;\r\n",
       "    \r\n",
       "    \r\n",
       "//    width: 1.2em;\r\n",
       "//    color: #ff0000;\r\n",
       "}\r\n",
       "\r\n",
       "i.m:before {\r\n",
       "    font-style: normal;    \r\n",
       "    content: \"\\21D2\";  \r\n",
       "}\r\n",
       "i.m {\r\n",
       "    font-style: normal; \r\n",
       "}    \r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "/* em {\r\n",
       "    font-style: normal; \r\n",
       "} */\r\n",
       "\r\n",
       "\r\n",
       "em.bl {\r\n",
       "    font-style: normal;     \r\n",
       "    font-weight: bold;        \r\n",
       "}\r\n",
       "\r\n",
       "/* em.grbg {\r\n",
       "    font-style: normal;         \r\n",
       "    background-color: #a0a0a0;      \r\n",
       "} */\r\n",
       "\r\n",
       "em.cr {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #ff0000;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cb {    \r\n",
       "    font-style: normal;         \r\n",
       "    color: #0000ff;    \r\n",
       "}\r\n",
       "\r\n",
       "em.cg {\r\n",
       "    font-style: normal;         \r\n",
       "    color: #00ff00;    \r\n",
       "}\r\n",
       "\r\n",
       "/*--------------------*/\r\n",
       "\r\n",
       "em.qs {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.qs::before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #ff0000;    \r\n",
       "    content: \"Q:\";  \r\n",
       "}\r\n",
       "\r\n",
       "em.an {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.an:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"A:\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.nt {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.nt:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Note:\";  \r\n",
       "}    \r\n",
       "    \r\n",
       "em.ex {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.ex:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #00ff00;    \r\n",
       "    content: \"Ex:\";  \r\n",
       "} \r\n",
       "    \r\n",
       "em.df {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.df:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"Def:\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.pl {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.pl:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"+\";  \r\n",
       "}    \r\n",
       "\r\n",
       "em.mn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.mn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"-\";  \r\n",
       "}        \r\n",
       "\r\n",
       "em.plmn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.plmn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\00B1\";\\\\\"&plusmn;\";  \r\n",
       "}\r\n",
       "    \r\n",
       "em.hn {\r\n",
       "    font-style: normal; \r\n",
       "}\r\n",
       "\r\n",
       "em.hn:before {\r\n",
       "    font-weight: bold;    \r\n",
       "    color: #0000ff;    \r\n",
       "    content: \"\\21D2\";\\\\\"&rArr;\";  \r\n",
       "}     \r\n",
       "    \r\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем стиль для оформления презентации\n",
    "from IPython.display import HTML\n",
    "from urllib.request import urlopen\n",
    "html = urlopen(\"file:./lec_v1.css\")\n",
    "HTML(html.read().decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Метрики расстояния между строками <a class=\"anchor\" id=\"форматирование-строк\"></a>\n",
    "-\n",
    "* [к оглавлению](#разделы)\n",
    "\n",
    "* Расстояние Левенштейна\n",
    "* Задача динамического программирования\n",
    "* Алгоритм поиска расстояния Левинштайна (The minimum edit distance algorithm was named by Wagner and Fischer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Метрики расстояния для строк__\n",
    "\n",
    "Часто требуется понять, насколько близкими являются две не совпадающих строки (слова). Это может потребоваться для:\n",
    "* для сравнения тексктов, предложений\n",
    "* поиска ошибок и опечаток в слове\n",
    "* поиска словоформ слова\n",
    "* в других областях (в биоинформатике для сравнения генов, хромосом и белков)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Расстояние Левенштейна\n",
    "\n",
    "__Расстояние Левенштейна__ (редакционное расстояние, дистанция редактирования) - __минимальное__ количество операций необходимых для превращения одной строки в другую. Рассматриваются следующие операции:\n",
    "* вставка одного символа\n",
    "* удаление одного символа \n",
    "* замена одного символа на другим."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center>         \n",
    "    <img src=\"./img/levinst1.png\" alt=\"Пример выполнения операций вставки, удаления и замены\" style=\"width: 500px;\"/>\n",
    "    <b>Пример выполнения операций вставки, удаления и замены для слова \"intention\"</b>    \n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/levinst2.png\" alt=\"Пример преобразования слова\" style=\"width: 500px;\"/>\n",
    "    <b>Пример преобразования слова \"intention\" в \"execution\" с помощью операций вставки, удаления и замены</b>\n",
    "</center>\n",
    "\n",
    "\n",
    "В общем случае __стоимость различных операций__ может быть различной. Обычно цена отражает __разную вероятность__ событий и может зависеть от:\n",
    "* вида операции (вставка, удаление, замена) \n",
    "* и/или от участвующих в ней символов\n",
    "\n",
    "\n",
    "Если к списку разрешённых операций добавить __транспозицию__ (два соседних символа меняются местами), получается __расстояние Дамерау - Левенштейна__. \n",
    "* Дамерау показал, что 80 % ошибок при наборе текста человеком являются транспозициями.\n",
    "* Кроме того, это расстояние используется и в биоинформатике. \n",
    "\n",
    "Для поиска расстояния Левинштайна и расстояния Дамрау - Левинштайна __существуют эффективные алгоритм__, требующий $O(MN)$ операций ($M$ и $N$ это длины первой и второй строки соответственно)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Пусть $S_1$ и $S_2$ - две строки (длиной $M$ и $N$ соответственно, здесь и далее считается, что элементы строк нумеруются с первого, как принято в математике) над некоторым алфавитом, тогда расстояние Левенштейна $\\operatorname{d}(S_1,S_2)$ можно подсчитать используя вспомогательную функцию $D(M,N)$, находящую редакционное расстояние для подстрок $S_1[0 .. M]$ и $S_2[0 .. N]$\n",
    "\n",
    "по следующей рекуррентной формуле:\n",
    "\n",
    "$$\\ \\operatorname{d}(S_1, S_2) = \\operatorname{D}(M,N)$$\n",
    "\n",
    "$$\\qquad\\operatorname{D}(i,j) = \\begin{cases}\n",
    "  \\max(i,j) & \\text{ if } \\min(i,j)=0, \\\\\n",
    "  \\min \\begin{cases}\n",
    "          \\operatorname{D}(i-1,j) + 1 \\\\\n",
    "          \\operatorname{D}(i,j-1) + 1 \\\\\n",
    "          \\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j])\n",
    "       \\end{cases} & \\text{ otherwise.}\n",
    "\\end{cases}$$\n",
    "\n",
    "$$\\operatorname{D}(i-1,j) + 1 \\text{, операция удаления (цена: 1, на схеме обозначается как: } \\uparrow) $$\n",
    "$$\\operatorname{D}(i,j-1) + 1 \\text{, операция вставки (цена: 1, на схеме обозначается как: } \\leftarrow)$$\n",
    "$$\\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j]) \\text{, операция замены (цена m, на схеме обозначается как: } \\nwarrow)$$\n",
    "\n",
    "Цена операции замены зависит от заменяемых символов:\n",
    "\n",
    "$$\\operatorname{m}(s_1, s_2) = \\begin{cases}\n",
    "0 \\text{ , if } s_1 = s_2 \\\\\n",
    "2 \\text{ , if } s_1 \\neq s_2 \\\\\n",
    "\\end{cases}$$\n",
    "\n",
    "Очевидно, что для расстояния Левинштайна справедливы следующие утверждения:\n",
    "* $\\operatorname{d}(S_1,S_2) \\geqslant \\bigl| |S_1| - |S_2| \\bigr|$\n",
    "* $\\operatorname{d}(S_1,S_2) \\leqslant \\max\\bigl( |S_1| , |S_2| \\bigr)$\n",
    "* $\\operatorname{d}(S_1,S_2) = 0 \\Leftrightarrow S_1 = S_2$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Редакционным предписанием__ называется последовательность действий, необходимых для получения второй строки из первой кратчайшим образом. Обычно действия обозначаются так: `D` (англ. delete) — удалить, `I` (англ. insert) — вставить, `R` (replace) — заменить, `M` (match) — совпадение.\n",
    "\n",
    "По сути редакционное предписание это кратчайшие пути на графе с весами, в котором существует 3 вида ориентированных ребер (D, I, M), а вершинами являются строки (слова). В общем случае для конкретной пары слов может существовать несколько редакционных предписаний (кратчайших путей на графе)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Динамическое программирование\n",
    "\n",
    "__Динамическое программирование__ - способ решения сложных задач путём разбиения их на более простые подзадачи. Он применим к *задачам с оптимальной подструктурой*, выглядящим как *набор перекрывающихся подзадач*, сложность которых чуть меньше исходной. В этом случае время вычислений, по сравнению с «наивными» методами, можно значительно сократить.\n",
    "\n",
    "__Идея динамического программирования:__\n",
    "\n",
    "*Оптимальная подструктура* в динамическом программировании означает, что оптимальное решение подзадач меньшего размера может быть использовано для решения исходной задачи. \n",
    "\n",
    "В общем случае мы можем решить задачу, в которой присутствует оптимальная подструктура, проделывая следующие три шага.\n",
    "\n",
    "1. Разбиение задачи на подзадачи меньшего размера.\n",
    "2. Нахождение оптимального решения подзадач рекурсивно, проделывая такой же трехшаговый алгоритм.\n",
    "3. Использование полученного решения подзадач для конструирования решения исходной задачи.\n",
    "\n",
    "Часто многие из рассматриваемых подзадач одинаковы. Подход динамического программирования состоит в том, чтобы *решить каждую подзадачу только один раз*, сократив тем самым количество вычислений. Это особенно полезно в случаях, когда число повторяющихся подзадач экспоненциально велико.\n",
    "\n",
    "* Метод динамического программирования __сверху-вниз__ (top-down approach) - это простое *запоминание результатов решения тех подзадач*, которые могут повторно встретиться в дальнейшем. \n",
    "* Динамическое программирование __снизу-вверх__ (bottom-up approach) включает в себя переформулирование сложной задачи в виде рекурсивной последовательности более простых подзадач."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Алгоритм Вагнера - Фишера\n",
    "\n",
    "Используя рекурсивное определение расстояния Левинштайна $\\operatorname{D}(i,j)$ через расстояния для слов меньшей длины: $\\operatorname{D}(i-1,j) \\text{ , } \\operatorname{D}(i,j-1) \\text{ , } \\operatorname{D}(i-1,j-1)$ мы применим принцип динамического программирования снизу-вверх, комбинируя решения подзадач, для решения более сложной задачи. \n",
    "\n",
    "1. Для получения базового решения когда конечная строка длины 0 или исходная строка длинны 0:\n",
    "    * $\\operatorname{D}(i, 0) = i $ - используется $i$ операций удаления (на схеме операция удаления обозначается, как: \"$\\uparrow$\")\n",
    "    * $\\operatorname{D}(0, j) = j$ - используется $j$ операций вставки (на схеме операция вставки обозначается, как: \"$\\leftarrow$\")\n",
    "2. После расчета $\\operatorname{D}(i, j)$ для малых $i$ и $j$ мы рассчитываем значения расстояния для бОльших $i$ и $j$ на основе рекурсивной формулы: \n",
    "\n",
    "$$\\qquad\\operatorname{D}(i,j) =  \\min \\begin{cases}\n",
    "          \\operatorname{D}(i-1,j) + 1 \\text{, операция удаления, на схеме обозначается как: } \\uparrow\\\\\n",
    "          \\operatorname{D}(i,j-1) + 1 \\text{, операция вставки, на схеме обозначается как: } \\leftarrow\\\\\n",
    "          \\operatorname{D}(i-1,j-1) + \\operatorname{m}(S_1[i], S_2[j]) \\text{, операция замены, на схеме обозначается как: } \\nwarrow\n",
    "       \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/levinst3.png\" alt=\"Пример поиска расстояния Левинштейна\" style=\"width: 500px;\"/>\n",
    "    <b>Пример поиска расстояния Левинштейна для слов \"intention\" и \"execution\" с помощью алгоритма Вагнера - Фишера</b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/levinst4.png\" alt=\"Алгоритм Вагнера - Фишера для поиска расстояния Левинштейна\" style=\"width: 500px;\"/>\n",
    "    <b>Алгоритм Вагнера - Фишера для поиска расстояния Левинштейна</b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# from nltk.metrics import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.metrics.distance import (\n",
    "    edit_distance,\n",
    "    edit_distance_align,\n",
    "    binary_distance,\n",
    "    jaccard_distance,\n",
    "    masi_distance,\n",
    "    interval_distance,\n",
    "    custom_distance,\n",
    "    presence,\n",
    "    fractional_presence,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('intention', 'execution', substitution_cost=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# результат при substitution_cost=1\n",
    "edit_distance('intention', 'execution')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edit_distance('пирвет', 'привет', substitution_cost=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# расстояние Домрау-Левинштайна:\n",
    "edit_distance('пирвет', 'привет', substitution_cost=2, transpositions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = 'intention'\n",
    "s2 = 'execution'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ed = edit_distance_align(s1, s2, substitution_cost=2)\n",
    "ed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intention'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'n', 't', 'e', 'n', 't', 'i', 'o', 'n']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = list(s1)\n",
    "l1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'intention'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = ''.join(l1)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = l1\n",
    "i = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'int_e_ntion'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sh_res = ''.join('_'+s+'_' if ind==i else s for ind, s in enumerate(res))\n",
    "sh_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_ed_path(as1, as2, ed):\n",
    "    s1 = '#' + as1 # shift index\n",
    "    s2 = '#' + as2 # shift index   \n",
    "    ip,  jp = ed[0]\n",
    "    res = list(s1)\n",
    "    cost = 0\n",
    "    print(f'i:{ip}, j:{jp}; init, cost: {cost}; res: {\"\".join(res)[1:]}')\n",
    "    def sh_res(res, i):\n",
    "        return ''.join(s.upper() if ind==i else s for ind, s in enumerate(res))[1:]\n",
    "        \n",
    "    for i, j in ed[1:]:\n",
    "        if i == ip+1 and j == jp+1:\n",
    "            if s1[i] == s2[j]:\n",
    "                # res = res\n",
    "                cost += 0\n",
    "                print(f'i:{i}, j:{j}; save {s1[i]}, cost: {cost}; res: {sh_res(res, j)}')\n",
    "            else:\n",
    "                res[j] = s2[j]\n",
    "                cost += 2\n",
    "                print(f'i:{i}, j:{j}; change {s1[i]} -> {s2[j]}; cost: {cost}; res: {sh_res(res, j)}')\n",
    "        elif i == ip+1 and j == jp:\n",
    "            cost += 1            \n",
    "            print(f'i:{i}, j:{j}; remove {res[j+1]}, cost: {cost}; res: {sh_res(res, j+1)}')            \n",
    "            rs = res.pop(j+1)\n",
    "        elif i == ip and j == jp+1:\n",
    "            rs = res.insert(j, s2[j])\n",
    "            cost += 1\n",
    "            print(f'i:{i}, j:{j}; insert {s2[j]}, cost: {cost}; res: {sh_res(res, j)}')            \n",
    "        else:\n",
    "            assert False, f'i: {i}, j: {j}; ip: {ip}, jp: {jp}'\n",
    "        ip = i\n",
    "        jp = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 0),\n",
       " (1, 0),\n",
       " (2, 0),\n",
       " (3, 0),\n",
       " (4, 1),\n",
       " (4, 2),\n",
       " (4, 3),\n",
       " (4, 4),\n",
       " (5, 5),\n",
       " (6, 6),\n",
       " (7, 7),\n",
       " (8, 8),\n",
       " (9, 9)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# s1 = 'abcd'\n",
    "# s2 = 'acfg'\n",
    "\n",
    "s1 = 'intention'\n",
    "s2 = 'execution'\n",
    "da = edit_distance_align(s1, s2, substitution_cost=2)\n",
    "da"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i:0, j:0; init, cost: 0; res: intention\n",
      "i:1, j:0; remove i, cost: 1; res: Intention\n",
      "i:2, j:0; remove n, cost: 2; res: Ntention\n",
      "i:3, j:0; remove t, cost: 3; res: Tention\n",
      "i:4, j:1; save e, cost: 3; res: Ention\n",
      "i:4, j:2; insert x, cost: 4; res: eXntion\n",
      "i:4, j:3; insert e, cost: 5; res: exEntion\n",
      "i:4, j:4; insert c, cost: 6; res: exeCntion\n",
      "i:5, j:5; change n -> u; cost: 8; res: execUtion\n",
      "i:6, j:6; save t, cost: 8; res: execuTion\n",
      "i:7, j:7; save i, cost: 8; res: executIon\n",
      "i:8, j:8; save o, cost: 8; res: executiOn\n",
      "i:9, j:9; save n, cost: 8; res: executioN\n"
     ]
    }
   ],
   "source": [
    "show_ed_path(s1, s2, da)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "С точки зрения приложений определение расстояния Левенштейна между словами или строками обладает следующими недостатками:\n",
    "* При перестановке местами слов или частей слов получаются сравнительно большие расстояния.\n",
    "* Расстояния между совершенно разными короткими словами оказываются небольшими, в то время как расстояния между очень похожими длинными словами оказываются значительными.\n",
    "\n",
    "Другие метрики в NLTK: http://www.nltk.org/howto/metrics.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Стемминг и лемматизация"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Часто необходимо обрабатывать разные формы слова одинаково. В этом случае поможет переход от словоформ к их леммам (словарным формам лексем) или основам (ядерным частям слова, за вычетом словоизменительных морфем)\n",
    "\n",
    "Например, при поиске: по запросам “кошками” и “кошкам” ожидаются одинаковые ответы.\n",
    "\n",
    "__Стемминг__ - это процесс нахождения основы слова, которая не обязательно совпадает с корнем слова.\n",
    "\n",
    "__Лемматизация__ - приведение слова к словарной форме."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Морфология__ - это раздел лингвистики, который изучает структуру слов и их морфологические характеристики. Классическая морфология проанализирует слово _собака_ примерно так: это существительное женского рода, оно состоит из _корня_ собак и _окончания_ а, окончание показывает, что слово употреблено в единственном числе и в именительном падеже. \n",
    "\n",
    "__Компьютерная морфология__ анализирует и синтезирует слова программными средствами. В наиболее привычной формулировке под морфологическим анализом слова подразумевается:\n",
    "* определение леммы (базовой, канонической формы слова)\n",
    "* определение грамматических характеристик слова. \n",
    "\n",
    "В области автоматической обработки данных также используется термин __нормализация__, обозначающий постановку слова или словосочетания в __каноническую форму__ (грамматические характеристики исходной формы при этом не выдаются). Обратная задача, т. е . постановка леммы в нужную грамматическую форму, называется __порождением словоформы__. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Стемминг__\n",
    "\n",
    "__Стемминг__ отбрасывает суффиксы и окончания до неизменяемой формы слова \n",
    "\n",
    "Примеры: \n",
    "* кошка -> кошк \n",
    "* кошками -> кошк \n",
    "* пылесосы -> пылесос\n",
    "\n",
    "В школьной грамматике __основой__ считается __часть слова без окончания__. \n",
    "* В большинстве случаев она не меняется при грамматических изменениях самого слова — так ведет себя, например, основа _слон_ в словоформах: _слон, слону, слонами, слонов_. \n",
    "* Но в некоторых словах основа может изменяться. Например, для словоформ _день, дню и дне_ основами будут ден-, дн- и дн-, такое явление называется __чередованием__. \n",
    "Поэтому самый популярный на сегодня подход использует псевдоосновы (или машинные основы). Это неизменяемые начальные части слов. Для слова день такой неизменяемой частью будет _д-_. Формы некоторых слов могут образовываться от разных корней. Например, у слова _ходить_ есть форма _шел_. Это называется _супплетивизмом_. \n",
    "\n",
    "__В русском языке__ супплетивизм и чередования очень распространены, поэтому __псевдоосновы часто получаются очень короткими__. __Для русского языка стемминг работает гораздо хуже, чем лемматизация__. \n",
    "\n",
    "В стемминге есть только правила обрабатывания суффиксов и, возможно, небольшие словари исключений. Существует бесплатный инструмент для написания стеммеров — Snowball. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('arabic',\n",
       " 'danish',\n",
       " 'dutch',\n",
       " 'english',\n",
       " 'finnish',\n",
       " 'french',\n",
       " 'german',\n",
       " 'hungarian',\n",
       " 'italian',\n",
       " 'norwegian',\n",
       " 'porter',\n",
       " 'portuguese',\n",
       " 'romanian',\n",
       " 'russian',\n",
       " 'spanish',\n",
       " 'swedish')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Snowball - Наиболее распространенный стеммер из проекта Apache Lucene \n",
    "# Работает для нескольких языков, включая русский\n",
    "from nltk.stem import SnowballStemmer\n",
    "SnowballStemmer.languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "кошк\n",
      "кошечк\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "snb_stemmer_ru = SnowballStemmer('russian')\n",
    "print(snb_stemmer_ru.stem('кошку'))\n",
    "print(snb_stemmer_ru.stem('кошечки'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "Постгуманизм — рациональное мировоззрение, основанное на представлении, что эволюция человека не завершена и может быть продолжена в будущем. Эволюционное развитие должно привести к становлению постчеловека — гипотетической стадии эволюции человеческого вида, строение и возможности которого стали бы отличными от современных человеческих в результате активного использования передовых технологий преобразования человека. Постгуманизм признаёт неотъемлемыми правами совершенствование человеческих возможностей (физиологических, интеллектуальных и т. п.) и достижение физического бессмертия. В отличие от трансгуманизма, под определением постгуманизма также понимается критика классического гуманизма, подчёркивающая изменение отношения человека к себе, обществу, окружающей среде и бурно развивающимся технологиям, но окончательно разница между транс- и постгуманизмом не определена и остаётся предметом дискуссий.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# загружаем текст:\n",
    "with open('phm.txt ') as f:\n",
    "    lines = [l for l in f]\n",
    "print(len(lines))\n",
    "print(lines[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import sentenize\n",
    "from razdel import tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['постгуманизм',\n",
       " 'рациональн',\n",
       " 'мировоззрен',\n",
       " 'основа',\n",
       " 'на',\n",
       " 'представлен',\n",
       " 'что',\n",
       " 'эволюц',\n",
       " 'человек',\n",
       " 'не',\n",
       " 'заверш',\n",
       " 'и',\n",
       " 'может',\n",
       " 'быт',\n",
       " 'продолж',\n",
       " 'в',\n",
       " 'будущ']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt = list(sentenize(lines[0]))\n",
    "tok = list(tokenize(snt[0].text))\n",
    "w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# предложение превращено в последовательность стем русских слов:\n",
    "[snb_stemmer_ru.stem(t.text) for t in tok if w.search(t.text)] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Snowball использует __систему суффиксов и окончаний__ для предсказания части речи и грамматических параметров. Так как одно и\n",
    "то же окончание может принадлежать разным частям речи или различным парадигмам, его оказывается недостаточно для точного предсказания. Применение суффиксов позволяет повысить точность.\n",
    "\n",
    "Система реализовывается на языке программирования в виде большого количества условных операторов, анализирующих самый длинный постфикс и его контекст. По окончании анализа слову приписывается часть речи и набор параметров, а найденное окончание (или псевдоокончание) отрезается. В итоге, помимо параметров, система возвращает стем."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Лемматизация\n",
    "\n",
    "У разных слов часто совпадает основа: \n",
    "* пол : полу , пола , поле , полю , поля , пол , полем , полях , полям \n",
    "* лев : левый, левая, лев \n",
    "\n",
    "Из-за этого увеличивается многозначность и ухудшаются результаты работы приложений.\n",
    "\n",
    "Лемматизация - приведение слова к словарной форме, например: \n",
    "* кошки -> кошка \n",
    "* кошками -> кошка"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Морфологические анализаторы для русского языка:\n",
    "\n",
    "|Название |Open |Доб. словари |Объем слов. |Скорость | Python? |\n",
    "|------|------|------|------|------|------|\n",
    "|AOT | Y | N | 160 тыс.| 60-90 | N |\n",
    "|MyStem | N | Y/N | >250 тыс.| 100-120 | Есть оболочка на Python |\n",
    "|Pymorphy2 | Y | N | 250 тыс.| 80-100 | Y|\n",
    "|TreeTagger | N | Y | 210 тыс.| 20-25 | N |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__pymorphy2__ \n",
    "\n",
    "* Код проекта: https://github.com/kmike/pymorphy2\n",
    "\n",
    "* Документация проекта: https://pymorphy2.readthedocs.io/en/stable/\n",
    "\n",
    "_pip install pymorphy2_\n",
    "\n",
    "Словари распространяются отдельными пакетами. Для русского языка:\n",
    "\n",
    "_pip install -U pymorphy2-dicts-ru_\n",
    "\n",
    "Есть оптимизированная версия, потребуется настроенное окружение для сборки (компилятор C/C++ и т.д.).\n",
    "\n",
    "Морфологический процессор с открытым исходным кодом, предоставляет все функции полного морфологического анализа и\n",
    "синтеза словоформ. Он умеет:\n",
    "* приводить слово к нормальной форме (например, “люди -> человек”, или “гулял -> гулять”).\n",
    "* ставить слово в нужную форму. Например, ставить слово во множественное число, менять падеж слова и т.д.\n",
    "* возвращать грамматическую информацию о слове (число, род, падеж, часть речи и т.д.)\n",
    "\n",
    "При работе используется словарь OpenCorpora; для незнакомых слов строятся гипотезы. Библиотека достаточно быстрая: в настоящий момент скорость работы - от нескольких тыс слов/сек до > 100тыс слов/сек (в зависимости от выполняемой операции, интерпретатора и установленных пакетов); потребление памяти - 10…20Мб; полностью поддерживается буква ё. Словарь OpenCorpora содержит около 250 тыс. лемм, а также является полностью открытым и регулярно пополняемым.\n",
    "\n",
    "Для анализа неизвестных слов в Pymorphy2 используются несколько методов, которые применяются последовательно. Изначально от слова отсекается префикс из набора известных префиксов и если остаток слова был найден в словаре, то отсеченный префикс приписывается к результатам разбора. Если этот метод не сработал, то аналогичные действия выполняются для префикса слова длиной от 1 до 5, даже если такой префикс является неизвестным. Затем, в случае неудачи, словоформа разбирается по окончанию. Для этого используется дополнительный автомат всех окончаний, встречающихся в словаре с имеющимися разборами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pymorphy2 in c:\\programdata\\anaconda3\\envs\\pytorch_1_6\\lib\\site-packages (0.8)\n",
      "Requirement already satisfied: docopt>=0.6 in c:\\programdata\\anaconda3\\envs\\pytorch_1_6\\lib\\site-packages (from pymorphy2) (0.6.2)\n",
      "Requirement already satisfied: pymorphy2-dicts<3.0,>=2.4 in c:\\programdata\\anaconda3\\envs\\pytorch_1_6\\lib\\site-packages (from pymorphy2) (2.4.393442.3710985)\n",
      "Requirement already satisfied: dawg-python>=0.7 in c:\\programdata\\anaconda3\\envs\\pytorch_1_6\\lib\\site-packages (from pymorphy2) (0.7.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymorphy2\n",
    "\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Parse(word='стали', tag=OpencorporaTag('VERB,perf,intr plur,past,indc'), normal_form='стать', score=0.984662, methods_stack=((<DictionaryAnalyzer>, 'стали', 904, 4),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,gent'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 1),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,datv'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 2),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn sing,loct'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 5),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,nomn'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 6),)),\n",
       " Parse(word='стали', tag=OpencorporaTag('NOUN,inan,femn plur,accs'), normal_form='сталь', score=0.003067, methods_stack=((<DictionaryAnalyzer>, 'стали', 13, 9),))]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = morph.parse('стали')\n",
    "p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OpencorporaTag('VERB,perf,intr plur,past,indc')"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[0].tag"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Метод MorphAnalyzer.parse() возвращает один или несколько объектов типа Parse с информацией о том, как слово может быть разобрано.\n",
    "\n",
    "Тег - это набор граммем, характеризующих данное слово. Например, тег 'VERB,perf,intr plur,past,indc' означает, что слово - глагол (VERB) совершенного вида (perf), непереходный (intr), множественного числа (plur), прошедшего времени (past), изъявительного наклонения (indc). Доступные граммемы описаны тут: https://pymorphy2.readthedocs.io/en/latest/user/grammemes.html#grammeme-docs.\n",
    "\n",
    "Далее: https://pymorphy2.readthedocs.io/en/latest/user/guide.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score - это оценка P(tag|word), оценка вероятности того, что данный разбор правильный.\n",
    "\n",
    "Разборы сортируются по убыванию score, поэтому везде в примерах берется первый вариант разбора из возможных. Оценки P(tag|word) помогают улучшить разбор, но их недостаточно для надежного снятия неоднозначности, как минимум по следующим причинам:\n",
    "\n",
    "то, как нужно разбирать слово, зависит от соседних слов; pymorphy2 работает только на уровне отдельных слов;\n",
    "условная вероятность P(tag|word) оценена на основе сбалансированного набора текстов; в специализированных текстах вероятности могут быть другими - например, возможно, что в металлургических текстах P(NOUN|стали) > P(VERB|стали);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parse(word='стать', tag=OpencorporaTag('INFN,perf,intr'), normal_form='стать', score=1.0, methods_stack=((<DictionaryAnalyzer>, 'стать', 904, 0),))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#у каждого разбора есть нормальная форма, которую можно получить, обратившись к атрибутам normal_form или normalized:\n",
    "p[0].normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['постгуманизм',\n",
       " 'рациональный',\n",
       " 'мировоззрение',\n",
       " 'основать',\n",
       " 'на',\n",
       " 'представление',\n",
       " 'что',\n",
       " 'эволюция',\n",
       " 'человек',\n",
       " 'не',\n",
       " 'завершить',\n",
       " 'и',\n",
       " 'мочь',\n",
       " 'быть',\n",
       " 'продолжить',\n",
       " 'в',\n",
       " 'будущее']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "snt = list(sentenize(lines[0]))\n",
    "tok = list(tokenize(snt[0].text))\n",
    "w = re.compile('^[а-яА-ЯёЁ]*$')\n",
    "# предложение превращено в последовательность нормальных форм русских слов:\n",
    "pt = [morph.parse(t.text) for t in tok if w.search(t.text)] \n",
    "[w[0].normalized.word for w in pt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Закон Ципфа\n",
    "\n",
    "__Закон Ципфа__ (Zipf's law, «ранг-частота») - эмперический закон, наблюдаемый для различных объектов в области физики, социологии, лингвистики и т.д., указывающий на то, что характеристики объектов (в частности, частота появлвения) имеют вид близкий к распределению Ципфа. \n",
    "\n",
    "Распределение Ципфа - это дискретный закон распределения, имеющий степенную природу и близкий (но не идентичный) Дзета-распределени. \n",
    "\n",
    "Пусть:\n",
    "* $N$ - количестов различных объектов (например, различных слов в тексте);\n",
    "* $k$ - ранг, т.е. порядоквый номер объекта (например, слова), в отсортированной по частоте последовательности объектов;\n",
    "* $s$ - параметр распределения, отражающий степень убывания частоты.\n",
    "\n",
    "тогда распрпеделение имеет вид:\n",
    "\n",
    "$$f(k;s,N)=\\frac{1/k^s}{\\sum\\limits_{n=1}^N (1/n^s)}$$\n",
    "\n",
    "Свойство объектов распределенных по этому закону:\n",
    "* $P_k$ - частота встречаемости объекта с рангом k\n",
    "\n",
    "$$P_k=P_1/k$$\n",
    "\n",
    "__Закон Ципфа в лингвистике__ - эмпирическая закономерность распределения частоты слов естественного языка: если все слова языка (или просто достаточно длинного текста) упорядочить по убыванию частоты их использования, то частота n-го слова в таком списке окажется приблизительно обратно пропорциональной его порядковому номеру n (так называемому рангу этого слова. \n",
    "\n",
    "Например: \n",
    "* второе по используемости слово встречается примерно в два раза реже, чем первое\n",
    "* третье - в три раза реже, чем первое (и так далее ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В естественных языках частоты слов имеют очень тяжелые ховосты и могут описываться распределением Ципфа с $s \\to 1$ при $N \\to \\infty$ в случае если $s > 1$:\n",
    "$$\\zeta (s) = \\sum_{n=1}^\\infty \\frac{1}{n^s}<\\infty$$\n",
    "где $\\zeta$ это Дзета-функция Римана.\n",
    "\n",
    "В этом случае распределение Ципфа можно заменить Дзета распределением (дискретным распределением, в котором $k \\in [1, \\infty])$): \n",
    "$$P(x=k; s) = \\frac {k^{-s}} {\\zeta(s)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>         \n",
    "    <img src=\"./img/ZipfsLaw.png\" alt=\"Пример поиска расстояния Левинштейна\" style=\"width: 500px;\"/>\n",
    "    <b>Пример: (распределение частот слов в статьях русской Википедии)</b>\n",
    "</center>\n",
    "\n",
    "<br/>\n",
    "\n",
    "<center>         \n",
    "    <img src=\"./img/ZipfsLaw2.png\" alt=\"Пример поиска расстояния Левинштейна\" style=\"width: 500px;\"/>\n",
    "    <b>Пример (распределение частот слов в крупном художесвтенном произведении)</b>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Стоп-слова\n",
    "\n",
    "* Для крупных текстов большинство слов из головы распределения обычно характеризуют язык, а не текст\n",
    "* Обычно это служебные слова, определяющие стрктуру предложения (например: предлоги, артикли, частицы), местоимения (фактически, универсальные указатели) и  самые общие понятия используемые в письменной речи\n",
    "* Во многих задачах использование наиболее частотных слов создает шум и их выгодно исключать из рассмотрения. За такими словами закрепился теримн __стоп-слова__(stop words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center> \n",
    "<b>Пример стоп-слов русского языка:</b>\n",
    "<br/>\n",
    "(конкретный состав стоп-слов зависит от рассматриваемого корпуса текстов, длинны списка и т.д.)\n",
    "</center>\n",
    "\n",
    "<table>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt; width: 48pt;\" width=\"64\">-</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">еще</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">него</td>   <td style=\"background-color: #eeeeee; width: 48pt;\" width=\"64\">сказать</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">а</td>   <td style=\"background-color: #eeeeee;\">ж</td>   <td style=\"background-color: #eeeeee;\">нее</td>   <td style=\"background-color: #eeeeee;\">со</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">без</td>   <td style=\"background-color: #eeeeee;\">же</td>   <td style=\"background-color: #eeeeee;\">ней</td>   <td style=\"background-color: #eeeeee;\">совсем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">более</td>   <td style=\"background-color: #eeeeee;\">жизнь</td>   <td style=\"background-color: #eeeeee;\">нельзя</td>   <td style=\"background-color: #eeeeee;\">так</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">больше</td>   <td style=\"background-color: #eeeeee;\">за</td>   <td style=\"background-color: #eeeeee;\">нет</td>   <td style=\"background-color: #eeeeee;\">такой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будет</td>   <td style=\"background-color: #eeeeee;\">зачем</td>   <td style=\"background-color: #eeeeee;\">ни</td>   <td style=\"background-color: #eeeeee;\">там</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">будто</td>   <td style=\"background-color: #eeeeee;\">здесь</td>   <td style=\"background-color: #eeeeee;\">нибудь</td>   <td style=\"background-color: #eeeeee;\">тебя</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">бы</td>   <td style=\"background-color: #eeeeee;\">и</td>   <td style=\"background-color: #eeeeee;\">никогда</td>   <td style=\"background-color: #eeeeee;\">тем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">был</td>   <td style=\"background-color: #eeeeee;\">из</td>   <td style=\"background-color: #eeeeee;\">ним</td>   <td style=\"background-color: #eeeeee;\">теперь</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">была</td>   <td style=\"background-color: #eeeeee;\">из-за</td>   <td style=\"background-color: #eeeeee;\">них</td>   <td style=\"background-color: #eeeeee;\">то</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">были</td>   <td style=\"background-color: #eeeeee;\">или</td>   <td style=\"background-color: #eeeeee;\">ничего</td>   <td style=\"background-color: #eeeeee;\">тогда</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">было</td>   <td style=\"background-color: #eeeeee;\">им</td>   <td style=\"background-color: #eeeeee;\">но</td>   <td style=\"background-color: #eeeeee;\">того</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">быть</td>   <td style=\"background-color: #eeeeee;\">иногда</td>   <td style=\"background-color: #eeeeee;\">ну</td>   <td style=\"background-color: #eeeeee;\">тоже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">в</td>   <td style=\"background-color: #eeeeee;\">их</td>   <td style=\"background-color: #eeeeee;\">о</td>   <td style=\"background-color: #eeeeee;\">только</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вам</td>   <td style=\"background-color: #eeeeee;\">к</td>   <td style=\"background-color: #eeeeee;\">об</td>   <td style=\"background-color: #eeeeee;\">том</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вас</td>   <td style=\"background-color: #eeeeee;\">кажется</td>   <td style=\"background-color: #eeeeee;\">один</td>   <td style=\"background-color: #eeeeee;\">тот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вдруг</td>   <td style=\"background-color: #eeeeee;\">как</td>   <td style=\"background-color: #eeeeee;\">он</td>   <td style=\"background-color: #eeeeee;\">три</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ведь</td>   <td style=\"background-color: #eeeeee;\">какая</td>   <td style=\"background-color: #eeeeee;\">она</td>   <td style=\"background-color: #eeeeee;\">тут</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">во</td>   <td style=\"background-color: #eeeeee;\">какой</td>   <td style=\"background-color: #eeeeee;\">они</td>   <td style=\"background-color: #eeeeee;\">ты</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вот</td>   <td style=\"background-color: #eeeeee;\">когда</td>   <td style=\"background-color: #eeeeee;\">опять</td>   <td style=\"background-color: #eeeeee;\">у</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">впрочем</td>   <td style=\"background-color: #eeeeee;\">конечно</td>   <td style=\"background-color: #eeeeee;\">от</td>   <td style=\"background-color: #eeeeee;\">уж</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">все</td>   <td style=\"background-color: #eeeeee;\">которого</td>   <td style=\"background-color: #eeeeee;\">перед</td>   <td style=\"background-color: #eeeeee;\">уже</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всегда</td>   <td style=\"background-color: #eeeeee;\">которые</td>   <td style=\"background-color: #eeeeee;\">по</td>   <td style=\"background-color: #eeeeee;\">хорошо</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всего</td>   <td style=\"background-color: #eeeeee;\">кто</td>   <td style=\"background-color: #eeeeee;\">под</td>   <td style=\"background-color: #eeeeee;\">хоть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всех</td>   <td style=\"background-color: #eeeeee;\">куда</td>   <td style=\"background-color: #eeeeee;\">после</td>   <td style=\"background-color: #eeeeee;\">чего</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">всю</td>   <td style=\"background-color: #eeeeee;\">ли</td>   <td style=\"background-color: #eeeeee;\">потом</td>   <td style=\"background-color: #eeeeee;\">человек</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">вы</td>   <td style=\"background-color: #eeeeee;\">лучше</td>   <td style=\"background-color: #eeeeee;\">потому</td>   <td style=\"background-color: #eeeeee;\">чем</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">г</td>   <td style=\"background-color: #eeeeee;\">между</td>   <td style=\"background-color: #eeeeee;\">почти</td>   <td style=\"background-color: #eeeeee;\">через</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">где</td>   <td style=\"background-color: #eeeeee;\">меня</td>   <td style=\"background-color: #eeeeee;\">при</td>   <td style=\"background-color: #eeeeee;\">что</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">говорил</td>   <td style=\"background-color: #eeeeee;\">мне</td>   <td style=\"background-color: #eeeeee;\">про</td>   <td style=\"background-color: #eeeeee;\">чтоб</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">да</td>   <td style=\"background-color: #eeeeee;\">много</td>   <td style=\"background-color: #eeeeee;\">раз</td>   <td style=\"background-color: #eeeeee;\">чтобы</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">даже</td>   <td style=\"background-color: #eeeeee;\">может</td>   <td style=\"background-color: #eeeeee;\">разве</td>   <td style=\"background-color: #eeeeee;\">чуть</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">два</td>   <td style=\"background-color: #eeeeee;\">можно</td>   <td style=\"background-color: #eeeeee;\">с</td>   <td style=\"background-color: #eeeeee;\">эти</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">для</td>   <td style=\"background-color: #eeeeee;\">мой</td>   <td style=\"background-color: #eeeeee;\">сам</td>   <td style=\"background-color: #eeeeee;\">этого</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">до</td>   <td style=\"background-color: #eeeeee;\">моя</td>   <td style=\"background-color: #eeeeee;\">свое</td>   <td style=\"background-color: #eeeeee;\">этой</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">другой</td>   <td style=\"background-color: #eeeeee;\">мы</td>   <td style=\"background-color: #eeeeee;\">свою</td>   <td style=\"background-color: #eeeeee;\">этом</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">его</td>   <td style=\"background-color: #eeeeee;\">на</td>   <td style=\"background-color: #eeeeee;\">себе</td>   <td style=\"background-color: #eeeeee;\">этот</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ее</td>   <td style=\"background-color: #eeeeee;\">над</td>   <td style=\"background-color: #eeeeee;\">себя</td>   <td style=\"background-color: #eeeeee;\">эту</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ей</td>   <td style=\"background-color: #eeeeee;\">надо</td>   <td style=\"background-color: #eeeeee;\">сегодня</td>   <td style=\"background-color: #eeeeee;\">я</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">ему</td>   <td style=\"background-color: #eeeeee;\">наконец</td>   <td style=\"background-color: #eeeeee;\">сейчас</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">если</td>   <td style=\"background-color: #eeeeee;\">нас</td>   <td style=\"background-color: #eeeeee;\">сказал</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "<tr height=\"20\" style=\"height: 15pt;\">   <td height=\"20\" style=\"background-color: #eeeeee; height: 15pt;\">есть</td>   <td style=\"background-color: #eeeeee;\">не</td>   <td style=\"background-color: #eeeeee;\">сказала</td>   <td style=\"background-color: #eeeeee;\"><br>\n",
    "</td>  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pip install -U nltk\n",
    "import nltk\n",
    "nltk.__version__\n",
    "# # nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.4.5'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Сергей\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Загрузка списков стоп-слов в NLTK:\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['и', 'в', 'во', 'не', 'что', 'он', 'на', 'я', 'с', 'со', 'как', 'а', 'то', 'все', 'она', 'так', 'его', 'но', 'да', 'ты', 'к', 'у', 'же', 'вы', 'за', 'бы', 'по', 'только', 'ее', 'мне', 'было', 'вот', 'от', 'меня', 'еще', 'нет', 'о', 'из', 'ему', 'теперь', 'когда', 'даже', 'ну', 'вдруг', 'ли', 'если', 'уже', 'или', 'ни', 'быть', 'был', 'него', 'до', 'вас', 'нибудь', 'опять', 'уж', 'вам', 'ведь', 'там', 'потом', 'себя', 'ничего', 'ей', 'может', 'они', 'тут', 'где', 'есть', 'надо', 'ней', 'для', 'мы', 'тебя', 'их', 'чем', 'была', 'сам', 'чтоб', 'без', 'будто', 'чего', 'раз', 'тоже', 'себе', 'под', 'будет', 'ж', 'тогда', 'кто', 'этот', 'того', 'потому', 'этого', 'какой', 'совсем', 'ним', 'здесь', 'этом', 'один', 'почти', 'мой', 'тем', 'чтобы', 'нее', 'сейчас', 'были', 'куда', 'зачем', 'всех', 'никогда', 'можно', 'при', 'наконец', 'два', 'об', 'другой', 'хоть', 'после', 'над', 'больше', 'тот', 'через', 'эти', 'нас', 'про', 'всего', 'них', 'какая', 'много', 'разве', 'три', 'эту', 'моя', 'впрочем', 'хорошо', 'свою', 'этой', 'перед', 'иногда', 'лучше', 'чуть', 'том', 'нельзя', 'такой', 'им', 'более', 'всегда', 'конечно', 'всю', 'между']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words = stopwords.words('russian')\n",
    "print(ru_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('phm.txt ') as f:\n",
    "#     lines = [l for l in f]\n",
    "# print(len(lines))\n",
    "# print(lines[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Мешок слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Мешок слов__ (bag-of-words, BoW) – модель, которая используется при обработке естественного языка для представления текста. Для представления текста ведется подсчет того, сколько раз каждое отдельное слово появляется в тексте, таким образом текст преобразуется в вектор, координатами которого являются рассматриваемые слова, а значениями - частоты слов. \n",
    "* Любая информация о порядке или структуре слов в документе отбрасывается. Модель касается только того, встречаются ли в документе известные слова, а не где в документе.\n",
    "    * Интуиция заключается в том, что документы похожи, если они имеют похожее содержание.\n",
    "* Модели мешка слов могут отличаться способами в определении словарного запаса известных слов (или токенов) и в том, как оценивать наличие известных слов.\n",
    "* Перед подсчетом можно применить методы предварительной обработки, описанные в выше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools as it\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# получаем все интересные нам токены:\n",
    "w_regex = re.compile('^[а-яА-ЯёЁ]*$') # re.compile('^[а-яА-ЯёЁ,\\.]*$')\n",
    "with open(\"AnnaKarenina_.txt\", encoding=\"cp1251\") as f:\n",
    "    book_tokens = [t.text.lower() for t in tokenize(f.read()) if w_regex.search(t.text)]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "266954 ['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'мне', 'отмщение', 'и', 'аз', 'воздам', 'часть', 'первая', 'все', 'счастливые', 'семьи', 'похожи', 'друг', 'на', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'все', 'смешалось', 'в', 'доме', 'облонских', 'жена', 'узнала', 'что', 'муж', 'был', 'в', 'связи', 'с', 'бывшею', 'в', 'их', 'доме', 'и', 'объявила', 'мужу', 'что', 'не', 'может', 'жить', 'с', 'ним', 'в', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'уже', 'третий', 'день', 'и', 'мучительно', 'чувствовалось', 'и', 'самими', 'супругами', 'и', 'всеми', 'членами', 'семьи', 'и', 'домочадцами', 'все', 'члены', 'семьи', 'и', 'домочадцы', 'чувствовали', 'что', 'нет', 'смысла', 'в', 'их', 'сожительстве', 'и', 'что', 'на', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'более', 'связаны', 'между', 'собой', 'чем', 'они', 'члены', 'семьи', 'и', 'домочадцы', 'облонских', 'жена', 'не', 'выходила', 'из', 'своих', 'комнат', 'мужа', 'третий', 'день', 'не', 'было', 'дома', 'дети', 'бегали', 'по', 'всему', 'дому', 'как', 'потерянные', 'англичанка', 'поссорилась', 'с', 'экономкой', 'и', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'ей', 'новое', 'место', 'повар', 'ушел', 'еще', 'вчера', 'со', 'двора', 'во', 'время', 'обеда', 'черная', 'кухарка', 'и', 'кучер', 'просили', 'расчета', 'на']\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(print(len(book_tokens), book_tokens[:150]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://www.nltk.org/api/nltk.html#nltk.probability.FreqDist\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist(book_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обработано токенов:266954; найдено различных токенов:32569\n"
     ]
    }
   ],
   "source": [
    "print(f'Обработано токенов:{fdist.N()}; найдено различных токенов:{fdist.B()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Содержимое: [('лев', 1), ('николаевич', 2), ('толстой', 2), ('анна', 499), ('каренина', 45), ('мне', 682), ('отмщение', 1), ('и', 12916), ('аз', 1), ('воздам', 1)]\n"
     ]
    }
   ],
   "source": [
    "print('Содержимое:', list(it.islice(fdist.items(), 10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Самое частое слово:и,  частота слова \"анна\":499\n"
     ]
    }
   ],
   "source": [
    "print(f'Самое частое слово:{fdist.max()},  частота слова \"анна\":{fdist.get(\"анна\")}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('и', 12916), ('не', 6537), ('что', 5765), ('в', 5720), ('он', 5551), ('на', 3594), ('она', 3434), ('с', 3327), ('я', 3212), ('как', 2660), ('но', 2581), ('его', 2578), ('это', 2223), ('к', 1983), ('ее', 1805), ('все', 1671), ('было', 1656), ('так', 1415), ('сказал', 1412), ('а', 1391), ('то', 1388), ('же', 1325), ('ему', 1252), ('о', 1243), ('за', 1139), ('левин', 1135), ('только', 1017), ('ты', 993), ('у', 913), ('был', 901), ('по', 834), ('когда', 831), ('для', 827), ('сказала', 827), ('бы', 822), ('от', 813), ('да', 812), ('теперь', 810), ('вы', 756), ('из', 735), ('была', 728), ('еще', 699), ('ей', 689), ('мне', 682), ('кити', 661), ('они', 646), ('него', 622), ('уже', 601), ('нет', 592), ('очень', 573)]\n"
     ]
    }
   ],
   "source": [
    "print(fdist.most_common(50))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что в мешке слов большинство самых частотных слов - стоп-слова."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['лев', 'николаевич', 'толстой', 'анна', 'каренина', 'отмщение', 'аз', 'воздам', 'часть', 'первая', 'счастливые', 'семьи', 'похожи', 'друг', 'друга', 'каждая', 'несчастливая', 'семья', 'несчастлива', 'смешалось', 'доме', 'облонских', 'жена', 'узнала', 'муж', 'связи', 'бывшею', 'доме', 'объявила', 'мужу', 'жить', 'одном', 'доме', 'положение', 'это', 'продолжалось', 'третий', 'день', 'мучительно', 'чувствовалось', 'самими', 'супругами', 'всеми', 'членами', 'семьи', 'домочадцами', 'члены', 'семьи', 'домочадцы', 'чувствовали', 'смысла', 'сожительстве', 'каждом', 'постоялом', 'дворе', 'случайно', 'сошедшиеся', 'люди', 'связаны', 'собой', 'члены', 'семьи', 'домочадцы', 'облонских', 'жена', 'выходила', 'своих', 'комнат', 'мужа', 'третий', 'день', 'дома', 'дети', 'бегали', 'всему', 'дому', 'потерянные', 'англичанка', 'поссорилась', 'экономкой', 'написала', 'записку', 'приятельнице', 'прося', 'приискать', 'новое', 'место', 'повар', 'ушел', 'вчера', 'двора', 'время', 'обеда', 'черная', 'кухарка', 'кучер', 'просили', 'расчета']\n"
     ]
    }
   ],
   "source": [
    "ru_stop_words_s = set(ru_stop_words)\n",
    "# фильтруем стоп-слова:\n",
    "wtokens_wostw = [w for w in book_tokens[:150] if w not in ru_stop_words_s]\n",
    "print(wtokens_wostw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(12, 'это', 2223), (18, 'сказал', 1412), (25, 'левин', 1135), (33, 'сказала', 827), (44, 'кити', 661), (49, 'очень', 573), (53, 'вронский', 509), (56, 'анна', 499), (66, 'алексей', 429), (68, 'степан', 423), (69, 'аркадьич', 422), (72, 'александрович', 395), (81, 'время', 366), (82, 'мог', 357), (83, 'говорил', 357), (89, 'руку', 309), (90, 'долли', 302), (92, 'которые', 295), (97, 'лицо', 277), (98, 'сказать', 276), (102, 'дело', 272), (103, 'левина', 272), (108, 'который', 263), (111, 'своей', 251), (113, 'знал', 249), (116, 'жизни', 235), (117, 'говорить', 234), (118, 'знаю', 233), (121, 'которое', 231), (124, 'пред', 224), (125, 'хотел', 219), (127, 'сергей', 219), (129, 'нужно', 217), (130, 'человек', 215), (131, 'прежде', 215), (132, 'глаза', 214), (134, 'могу', 214), (135, 'видел', 214), (137, 'тебе', 213), (139, 'тотчас', 211), (141, 'чувствовал', 210), (143, 'вронского', 205), (145, 'одно', 202), (146, 'своего', 199), (147, 'могла', 199), (148, 'свое', 198), (149, 'иванович', 191), (153, 'думал', 189), (154, 'глядя', 189), (156, 'говорила', 184)]\n"
     ]
    }
   ],
   "source": [
    "# самые частотные слова \"Анны Карениной\" после очистки от стоп-слов:\n",
    "print(list(it.islice(((i, w, f) for i, (w, f) in enumerate(fdist.most_common(500)) \\\n",
    "                      if w not in ru_stop_words_s), 50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Векторное представление документа"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все слова (в более общем случае - термы: слова и другие значимые элементы текста) которые встречаются в документах обрабатываемой коллекции, можно упорядочить. Если теперь для некоторого документа выписать по порядку веса всех термов, включая те, которых нет в этом документе, получится вектор, который и будет представлением данного документа в векторном пространстве.\n",
    "* Размерность этого вектора, как и размерность пространства, равна количеству различных термов во всей коллекции, и является одинаковой для всех документов.\n",
    "\n",
    "Записывая формально, документ $j$ описывается вектором:\n",
    "\n",
    "$$d_j = (w_{1j}, w_{2j},\\dotsc, w_{nj})$$\n",
    "\n",
    "где $d_j$ — векторное представление j-го документа, где $w_{ij}$ — вес i-го слова в j-м документе, n — общее количество различных термов во всех документах коллекции.\n",
    "\n",
    "Располагая таким представлением для всех документов, можно, например, находить расстояние между точками пространства и тем самым решать задачу подобия документов — чем ближе расположены точки, тем больше похожи соответствующие документы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Методы взвешивания термов__\n",
    "\n",
    "Для полного определения векторной модели необходимо указать, каким именно образом будет отыскиваться вес терма в документе. Существует несколько стандартных способов задания функции взвешивания:\n",
    "\n",
    "* булевский вес — равен 1, если терм встречается в документе и 0 в противном случае;\n",
    "* tf (term frequency, частота терма) — вес определяется как функция от количества вхождений терма в документе;\n",
    "* tf-idf (term frequency — inverse document frequency, частота терма — обратная частота документа) — вес определяется как произведение функции от количества вхождений терма в документ и функции от величины, обратной количеству документов коллекции, в которых встречается этот терм."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Косинусное сходство__\n",
    "\n",
    "Косинусное сходство — это мера сходства между двумя векторами предгильбертового пространства, которая используется для измерения косинуса угла между ними.\n",
    "\n",
    "Если даны два вектора признаков, A и B, то косинусное сходство, cos(θ), может быть представлено используя скалярное произведение и норму:\n",
    "\n",
    "\n",
    "$$\\text{similarity} = \\cos(\\theta) = {A \\cdot B \\over \\|A\\| \\|B\\|} = \\frac{ \\sum\\limits_{i=1}^{n}{A_i \\times B_i} }{ \\sqrt{\\sum\\limits_{i=1}^{n}{(A_i)^2}} \\times \\sqrt{\\sum\\limits_{i=1}^{n}{(B_i)^2}} }$$\n",
    "\n",
    "косинусное сходство двух документов изменяется в диапазоне от 0 до 1, поскольку частота терма (например, веса tf-idf) не может быть отрицательной. Угол между двумя векторами частоты терма не может быть больше, чем 90°.\n",
    "\n",
    "Одна из причин популярности косинуснуго сходства состоит в том, что __оно эффективно в качестве оценочной меры, особенно для разреженных векторов__, так как необходимо учитывать только ненулевые измерения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример 1: Тривиальный пример с векторизацией на основе подсчета слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools as it\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "\n",
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import (CountVectorizer, TfidfVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# корпус текстов:\n",
    "corpus = ['This is the first document.',\n",
    "          'This document is the second document.',\n",
    "          'And this is the third one.',\n",
    "          'Is this the first document?']\n",
    "\n",
    "# создание векторизатора:\n",
    "cv = CountVectorizer()\n",
    "\n",
    "# векторизуем корпус:\n",
    "corpus_cv = cv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# рассмотренные токены:\n",
    "cv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ar = corpus_cv.toarray()\n",
    "cv_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2.23606798, 2.82842712, 2.44948974, 2.23606798])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "norm(cv_ar, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# нормализация:\n",
    "ca_arn = cv_ar / norm(cv_ar, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.4472136 , 0.4472136 , 0.4472136 , 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.4472136 ],\n",
       "       [0.        , 0.70710678, 0.        , 0.35355339, 0.        ,\n",
       "        0.35355339, 0.35355339, 0.        , 0.35355339],\n",
       "       [0.40824829, 0.        , 0.        , 0.40824829, 0.40824829,\n",
       "        0.        , 0.40824829, 0.40824829, 0.40824829],\n",
       "       [0.        , 0.4472136 , 0.4472136 , 0.4472136 , 0.        ,\n",
       "        0.        , 0.4472136 , 0.        , 0.4472136 ]])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.79056942, 0.54772256, 1.        ],\n",
       "       [0.79056942, 1.        , 0.4330127 , 0.79056942],\n",
       "       [0.54772256, 0.4330127 , 1.        , 0.54772256],\n",
       "       [1.        , 0.79056942, 0.54772256, 1.        ]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ca_arn @ ca_arn.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Использование TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создание векторизатора:\n",
    "tv = TfidfVectorizer()\n",
    "\n",
    "# векторизуем корпус:\n",
    "corpus_tv = tv.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# рассмотренные токены:\n",
    "tv.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524],\n",
       "       [0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867],\n",
       "       [0.51184851, 0.        , 0.        , 0.26710379, 0.51184851,\n",
       "        0.        , 0.26710379, 0.51184851, 0.26710379],\n",
       "       [0.        , 0.46979139, 0.58028582, 0.38408524, 0.        ,\n",
       "        0.        , 0.38408524, 0.        , 0.38408524]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_tv.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.64692568, 0.30777187, 1.        ],\n",
       "       [0.64692568, 1.        , 0.22523955, 0.64692568],\n",
       "       [0.30777187, 0.22523955, 1.        , 0.30777187],\n",
       "       [1.        , 0.64692568, 0.30777187, 1.        ]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pairwise_similarity = corpus_tv * corpus_tv.T \n",
    "pairwise_similarity.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Пример 2: Векторизация данных реального новостного потока\n",
    "\n",
    "* Источник данных: https://webhose.io/free-datasets/russian-news-articles/\n",
    "* альтернатива: https://github.com/RossiyaSegodnya/ria_news_dataset\n",
    "\n",
    "Этап 1: загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['news_0000001.json',\n",
       "  'news_0000002.json',\n",
       "  'news_0000003.json',\n",
       "  'news_0000004.json',\n",
       "  'news_0000005.json'],\n",
       " ['news_0000995.json',\n",
       "  'news_0000996.json',\n",
       "  'news_0000997.json',\n",
       "  'news_0000998.json',\n",
       "  'news_0000999.json'],\n",
       " 999)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# получение имен всех файлов, находящихся по определенному пути:\n",
    "news_path = './news'\n",
    "news_files = [f for f in listdir(news_path) if isfile(join(news_path, f))]\n",
    "news_files[:5], news_files[-5:], len(news_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'organizations': [],\n",
       " 'uuid': '99bbd8fc99f9458417204a7107d21a0e03272d60',\n",
       " 'thread': {'social': {'gplus': {'shares': 0},\n",
       "   'pinterest': {'shares': 0},\n",
       "   'vk': {'shares': 0},\n",
       "   'linkedin': {'shares': 0},\n",
       "   'facebook': {'likes': 1, 'shares': 1, 'comments': 0},\n",
       "   'stumbledupon': {'shares': 0}},\n",
       "  'site_full': 'www.newsru.com',\n",
       "  'main_image': 'http://image.newsru.com/v2/02/2016/10//.jpg',\n",
       "  'site_section': 'http://feeds.newsru.com/com/www/news/main',\n",
       "  'section_title': 'NEWSru.com :: Важные новости',\n",
       "  'url': 'http://www.newsru.com/world/02oct2016/gulens.html',\n",
       "  'country': 'US',\n",
       "  'domain_rank': 3073,\n",
       "  'title': 'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата',\n",
       "  'performance_score': 0,\n",
       "  'site': 'newsru.com',\n",
       "  'participants_count': 0,\n",
       "  'title_full': 'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата',\n",
       "  'spam_score': 0.0,\n",
       "  'site_type': 'news',\n",
       "  'published': '2016-10-02T21:53:00.000+03:00',\n",
       "  'replies_count': 0,\n",
       "  'uuid': '99bbd8fc99f9458417204a7107d21a0e03272d60'},\n",
       " 'author': '',\n",
       " 'url': 'http://www.newsru.com/world/02oct2016/gulens.html',\n",
       " 'ord_in_thread': 0,\n",
       " 'title': 'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата',\n",
       " 'locations': [],\n",
       " 'entities': {'persons': [], 'locations': [], 'organizations': []},\n",
       " 'highlightText': '',\n",
       " 'language': 'russian',\n",
       " 'persons': [],\n",
       " 'text': 'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата   16:53   16:53 \\nТурецкая полиция задержала в городе Измир на западе страны брата оппозиционного исламского проповедника Фетхуллаха Гюлена Ктубеттина. Живущего в США проповедника Анкара считает вдохновителем попытки провалившегося переворота. Кутбеттин Гюлен разыскивался по обвинению в причастности к деятельности организации, возглавляемой его братом. Его доставили на допрос в Управление безопасности и, вероятно, вскоре предъявят обвинение. Операцию по задержанию провела полиция Измира на основе оперативных данных о том, что подозреваемый скрывается в доме своего родственника в районе Газиемир, передает РИА \"Новости\" . ТАСС напоминает, что 23 сентября власти Турции задержали племянницу Гюлена Эмине. Задержание прошло в уезде Эрдемит западной провинции Балыкесир. Выяснилось, что она значительную часть телефонных разговоров вела с одним абонентом в США. Кроме того, у нее изъято большое количество фотографий и книг Гюлена. В августе полиция задержала племянника Гюлена Кемаля Гюлена, телеведущего и адвоката. Он был задержан в одной из деревень в провинции Кастамону, где скрывался с середины июля после провала заговора. Еще раньше был задержан другой племянник проповедника Адбуллах Коруджук. В ночь на 16 июля в Турции группа мятежников совершила попытку военного переворота. Основное противостояние развернулось в Анкаре и Стамбуле. Погибли более 240 турецких граждан, более 2 тысяч человек получили ранения, мятеж был подавлен. Власти Турции обвинили Гюлена в причастности к попытке переворота и потребовали от США его экстрадиции. Сам Гюлен осудил мятеж и заявил о своей непричастности. По обвинению в причастности к организации Гюлена в Турции после мятежа были арестованы около 32 тысяч человек.',\n",
       " 'external_links': [],\n",
       " 'published': '2016-10-02T21:53:00.000+03:00',\n",
       " 'crawled': '2016-10-02T17:00:29.521+03:00',\n",
       " 'highlightTitle': ''}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(join(news_path, news_files[0]), 'r', encoding='utf-8') as f:\n",
    "    news_js = json.load(f)\n",
    "    \n",
    "news_js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата   16:53   16:53 \\nТурецкая полиция задержала в городе Измир на западе страны брата оппозиционного исламского проповедника Фетхуллаха Гюлена Ктубеттина. Живущего в США проповедника Анкара считает вдохновителем попытки провалившегося переворота. Кутбеттин Гюлен разыскивался по обвинению в причастности к деятельности организации, возглавляемой его братом. Его доставили на допрос в Управление безопасности и, вероятно, вскоре предъявят обвинение. Операцию по задержанию провела полиция Измира на основе оперативных данных о том, что подозреваемый скрывается в доме своего родственника в районе Газиемир, передает РИА \"Новости\" . ТАСС напоминает, что 23 сентября власти Турции задержали племянницу Гюлена Эмине. Задержание прошло в уезде Эрдемит западной провинции Балыкесир. Выяснилось, что она значительную часть телефонных разговоров вела с одним абонентом в США. Кроме того, у нее изъято большое количество фотографий и книг Гюлена. В августе полиция задержала племянника Гюлена Кемаля Гюлена, телеведущего и адвоката. Он был задержан в одной из деревень в провинции Кастамону, где скрывался с середины июля после провала заговора. Еще раньше был задержан другой племянник проповедника Адбуллах Коруджук. В ночь на 16 июля в Турции группа мятежников совершила попытку военного переворота. Основное противостояние развернулось в Анкаре и Стамбуле. Погибли более 240 турецких граждан, более 2 тысяч человек получили ранения, мятеж был подавлен. Власти Турции обвинили Гюлена в причастности к попытке переворота и потребовали от США его экстрадиции. Сам Гюлен осудил мятеж и заявил о своей непричастности. По обвинению в причастности к организации Гюлена в Турции после мятежа были арестованы около 32 тысяч человек.'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_js['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Уланова: чтобы охарактеризовать Гамову, достаточно одного слова — великая   22:02. Волейбол Либеро «Динамо Казань» Екатерина Уланова после прощального матча Екатерины Гамовой поделилась эмоциями, связанными с уходом Гамовой из спорта. «Слёзы на глаза накатываются, и мурашки по коже. Грустно, хотя понимаешь, конечно, что все мы рано или поздно будем уходить из спорта. Я очень счастливый человек, потому что мне удалось поиграть с Катей и в сборной, и в клубе. Какими словами я охарактеризовала бы Гамову? Мне достаточно одного слова – великая. И в жизни, и в спорте. Почему у нас не получилось шоу? Не знаю, что ответить на этот вопрос. Не мы решали. Сколько ни пытались сделать шоу в женском волейболе, не получается. Может быть, женский характер не позволяет раскрепоститься и сыграть в своё удовольствие. Да, сегодня была борьба, игра. Просто мы ещё не умеем делать шоу, не готовы к этому», — приводит слова Улановой «Спорт Бизнес Online».',\n",
       " 999)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts_corpus = []\n",
    "for nf in news_files:\n",
    "    with open(join(news_path, nf), 'r', encoding='utf-8') as f:\n",
    "        news_texts_corpus.append(json.load(f)['text'])\n",
    "        \n",
    "news_texts_corpus[42], len(news_texts_corpus)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Этап 2: предподготовка: токенизация, очистка от стоп слов, лемматизация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import itertools as it\n",
    "from razdel import sentenize\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk, string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готовим свой токенизатор (с нормализацией) и список стоп-слов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_regex = re.compile('^[А-а-яЯёЁ]*$')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def n_tokenizer(news_str):\n",
    "    return [(morph.parse(t.text.lower())[0].normalized.word, \n",
    "             w_regex.search(t.text)) for t in tokenize(news_str)]    \n",
    "#     return [morph.parse(t.text.lower())[0].normalized.word for t in tokenize(news_str) \n",
    "#                    if w_regex.search(t.text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата   16:53   16:53 \\nТурецкая полиция задержала в городе Измир на западе страны брата оппозиционного исламского проповедника Фетхуллаха Гюлена Ктубеттина. Живущего в США проповедника Анкара считает вдохновителем попытки провалившегося переворота. Кутбеттин Гюлен разыскивался по обвинению в причастности к деятельности организации, возглавляемой его братом. Его доставили на допрос в Управление безопасности и, вероятно, вскоре предъявят обвинение. Операцию по задержанию провела полиция Измира на основе оперативных данных о том, что подозреваемый скрывается в доме своего родственника в районе Газиемир, передает РИА \"Новости\" . ТАСС напоминает, что 23 сентября власти Турции задержали племянницу Гюлена Эмине. Задержание прошло в уезде Эрдемит западной провинции Балыкесир. Выяснилось, что она значительную часть телефонных разговоров вела с одним абонентом в США. Кроме того, у нее изъято большое количество фотографий и книг Гюлена. В августе полиция задержала племянника Гюлена Кемаля Гюлена, телеведущего и адвоката. Он был задержан в одной из деревень в провинции Кастамону, где скрывался с середины июля после провала заговора. Еще раньше был задержан другой племянник проповедника Адбуллах Коруджук. В ночь на 16 июля в Турции группа мятежников совершила попытку военного переворота. Основное противостояние развернулось в Анкаре и Стамбуле. Погибли более 240 турецких граждан, более 2 тысяч человек получили ранения, мятеж был подавлен. Власти Турции обвинили Гюлена в причастности к попытке переворота и потребовали от США его экстрадиции. Сам Гюлен осудил мятеж и заявил о своей непричастности. По обвинению в причастности к организации Гюлена в Турции после мятежа были арестованы около 32 тысяч человек.'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('в', <re.Match object; span=(0, 1), match='В'>),\n",
       " ('турция', None),\n",
       " ('задержать', None),\n",
       " ('очередной', None),\n",
       " ('родственник', None),\n",
       " ('фетхуллах', None),\n",
       " ('гюлена', None),\n",
       " ('-', <re.Match object; span=(0, 1), match='-'>),\n",
       " ('он', None),\n",
       " ('брат', None),\n",
       " ('16', None),\n",
       " (':', None),\n",
       " ('53', None),\n",
       " ('16', None),\n",
       " (':', None),\n",
       " ('53', None),\n",
       " ('турецкий', None),\n",
       " ('полиция', None),\n",
       " ('задержать', None),\n",
       " ('в', None),\n",
       " ('город', None),\n",
       " ('измир', None),\n",
       " ('на', None),\n",
       " ('запад', None),\n",
       " ('страна', None),\n",
       " ('брат', None),\n",
       " ('оппозиционный', None),\n",
       " ('исламский', None),\n",
       " ('проповедник', None),\n",
       " ('фетхуллах', None),\n",
       " ('гюлена', None),\n",
       " ('ктубеттин', None),\n",
       " ('.', None),\n",
       " ('жить', None),\n",
       " ('в', None),\n",
       " ('сша', <re.Match object; span=(0, 3), match='США'>),\n",
       " ('проповедник', None),\n",
       " ('анкара', None),\n",
       " ('считать', None),\n",
       " ('вдохновитель', None),\n",
       " ('попытка', None),\n",
       " ('провалиться', None),\n",
       " ('переворот', None),\n",
       " ('.', None),\n",
       " ('кутбеттин', None),\n",
       " ('гюлена', None),\n",
       " ('разыскиваться', None),\n",
       " ('по', None),\n",
       " ('обвинение', None),\n",
       " ('в', None),\n",
       " ('причастность', None),\n",
       " ('к', None),\n",
       " ('деятельность', None),\n",
       " ('организация', None),\n",
       " (',', None),\n",
       " ('возглавлять', None),\n",
       " ('он', None),\n",
       " ('брат', None),\n",
       " ('.', None),\n",
       " ('он', None),\n",
       " ('доставить', None),\n",
       " ('на', None),\n",
       " ('допрос', None),\n",
       " ('в', None),\n",
       " ('управление', None),\n",
       " ('безопасность', None),\n",
       " ('и', None),\n",
       " (',', None),\n",
       " ('вероятно', None),\n",
       " (',', None),\n",
       " ('вскоре', None),\n",
       " ('предъявить', None),\n",
       " ('обвинение', None),\n",
       " ('.', None),\n",
       " ('операция', None),\n",
       " ('по', None),\n",
       " ('задержание', None),\n",
       " ('провести', None),\n",
       " ('полиция', None),\n",
       " ('измир', None),\n",
       " ('на', None),\n",
       " ('основа', None),\n",
       " ('оперативный', None),\n",
       " ('дать', None),\n",
       " ('о', None),\n",
       " ('тот', None),\n",
       " (',', None),\n",
       " ('что', None),\n",
       " ('подозревать', None),\n",
       " ('скрываться', None),\n",
       " ('в', None),\n",
       " ('дом', None),\n",
       " ('свой', None),\n",
       " ('родственник', None),\n",
       " ('в', None),\n",
       " ('район', None),\n",
       " ('газиемиро', None),\n",
       " (',', None),\n",
       " ('передавать', None),\n",
       " ('риа', <re.Match object; span=(0, 3), match='РИА'>),\n",
       " ('\"', None),\n",
       " ('новость', None),\n",
       " ('\"', None),\n",
       " ('.', None),\n",
       " ('тасс', <re.Match object; span=(0, 4), match='ТАСС'>),\n",
       " ('напоминать', None),\n",
       " (',', None),\n",
       " ('что', None),\n",
       " ('23', None),\n",
       " ('сентябрь', None),\n",
       " ('власть', None),\n",
       " ('турция', None),\n",
       " ('задержать', None),\n",
       " ('племянница', None),\n",
       " ('гюлена', None),\n",
       " ('эмин', None),\n",
       " ('.', None),\n",
       " ('задержание', None),\n",
       " ('прошлый', None),\n",
       " ('в', None),\n",
       " ('уезд', None),\n",
       " ('эрдеметь', None),\n",
       " ('западный', None),\n",
       " ('провинция', None),\n",
       " ('балыкесир', None),\n",
       " ('.', None),\n",
       " ('выясниться', None),\n",
       " (',', None),\n",
       " ('что', None),\n",
       " ('она', None),\n",
       " ('значительный', None),\n",
       " ('часть', None),\n",
       " ('телефонный', None),\n",
       " ('разговор', None),\n",
       " ('вести', None),\n",
       " ('с', None),\n",
       " ('один', None),\n",
       " ('абонент', None),\n",
       " ('в', None),\n",
       " ('сша', <re.Match object; span=(0, 3), match='США'>),\n",
       " ('.', None),\n",
       " ('кроме', None),\n",
       " ('тот', None),\n",
       " (',', None),\n",
       " ('у', None),\n",
       " ('нея', None),\n",
       " ('изъять', None),\n",
       " ('большой', None),\n",
       " ('количество', None),\n",
       " ('фотография', None),\n",
       " ('и', None),\n",
       " ('книга', None),\n",
       " ('гюлена', None),\n",
       " ('.', None),\n",
       " ('в', <re.Match object; span=(0, 1), match='В'>),\n",
       " ('август', None),\n",
       " ('полиция', None),\n",
       " ('задержать', None),\n",
       " ('племянник', None),\n",
       " ('гюлена', None),\n",
       " ('кемаля', None),\n",
       " ('гюлена', None),\n",
       " (',', None),\n",
       " ('телеведущий', None),\n",
       " ('и', None),\n",
       " ('адвокат', None),\n",
       " ('.', None),\n",
       " ('он', None),\n",
       " ('быть', None),\n",
       " ('задержать', None),\n",
       " ('в', None),\n",
       " ('один', None),\n",
       " ('из', None),\n",
       " ('деревня', None),\n",
       " ('в', None),\n",
       " ('провинция', None),\n",
       " ('кастамон', None),\n",
       " (',', None),\n",
       " ('где', None),\n",
       " ('скрываться', None),\n",
       " ('с', None),\n",
       " ('середина', None),\n",
       " ('июль', None),\n",
       " ('после', None),\n",
       " ('провал', None),\n",
       " ('заговор', None),\n",
       " ('.', None),\n",
       " ('ещё', None),\n",
       " ('ранний', None),\n",
       " ('быть', None),\n",
       " ('задержать', None),\n",
       " ('другой', None),\n",
       " ('племянник', None),\n",
       " ('проповедник', None),\n",
       " ('адбуллах', None),\n",
       " ('коруджук', None),\n",
       " ('.', None),\n",
       " ('в', <re.Match object; span=(0, 1), match='В'>),\n",
       " ('ночь', None),\n",
       " ('на', None),\n",
       " ('16', None),\n",
       " ('июль', None),\n",
       " ('в', None),\n",
       " ('турция', None),\n",
       " ('группа', None),\n",
       " ('мятежник', None),\n",
       " ('совершить', None),\n",
       " ('попытка', None),\n",
       " ('военный', None),\n",
       " ('переворот', None),\n",
       " ('.', None),\n",
       " ('основный', None),\n",
       " ('противостояние', None),\n",
       " ('развернуться', None),\n",
       " ('в', None),\n",
       " ('анкара', None),\n",
       " ('и', None),\n",
       " ('стамбул', None),\n",
       " ('.', None),\n",
       " ('погибнуть', None),\n",
       " ('более', None),\n",
       " ('240', None),\n",
       " ('турецкий', None),\n",
       " ('гражданин', None),\n",
       " (',', None),\n",
       " ('более', None),\n",
       " ('2', None),\n",
       " ('тысяча', None),\n",
       " ('человек', None),\n",
       " ('получить', None),\n",
       " ('ранение', None),\n",
       " (',', None),\n",
       " ('мятеж', None),\n",
       " ('быть', None),\n",
       " ('подавить', None),\n",
       " ('.', None),\n",
       " ('власть', None),\n",
       " ('турция', None),\n",
       " ('обвинить', None),\n",
       " ('гюлена', None),\n",
       " ('в', None),\n",
       " ('причастность', None),\n",
       " ('к', None),\n",
       " ('попытка', None),\n",
       " ('переворот', None),\n",
       " ('и', None),\n",
       " ('потребовать', None),\n",
       " ('от', None),\n",
       " ('сша', <re.Match object; span=(0, 3), match='США'>),\n",
       " ('он', None),\n",
       " ('экстрадиция', None),\n",
       " ('.', None),\n",
       " ('сам', None),\n",
       " ('гюлена', None),\n",
       " ('осудить', None),\n",
       " ('мятеж', None),\n",
       " ('и', None),\n",
       " ('заявить', None),\n",
       " ('о', None),\n",
       " ('свой', None),\n",
       " ('непричастность', None),\n",
       " ('.', None),\n",
       " ('по', None),\n",
       " ('обвинение', None),\n",
       " ('в', None),\n",
       " ('причастность', None),\n",
       " ('к', None),\n",
       " ('организация', None),\n",
       " ('гюлена', None),\n",
       " ('в', None),\n",
       " ('турция', None),\n",
       " ('после', None),\n",
       " ('мятеж', None),\n",
       " ('быть', None),\n",
       " ('арестовать', None),\n",
       " ('около', None),\n",
       " ('32', None),\n",
       " ('тысяча', None),\n",
       " ('человек', None),\n",
       " ('.', None)]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test:\n",
    "n_tokenizer(news_texts_corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между']"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_stop_words = stopwords.words('russian')\n",
    "\n",
    "n_stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['В Турции задержали очередного родственника Фетхуллаха Гюлена - его брата   16:53   16:53 \\nТурецкая полиция задержала в городе Измир на западе страны брата оппозиционного исламского проповедника Фетхуллаха Гюлена Ктубеттина. Живущего в США проповедника Анкара считает вдохновителем попытки провалившегося переворота. Кутбеттин Гюлен разыскивался по обвинению в причастности к деятельности организации, возглавляемой его братом. Его доставили на допрос в Управление безопасности и, вероятно, вскоре предъявят обвинение. Операцию по задержанию провела полиция Измира на основе оперативных данных о том, что подозреваемый скрывается в доме своего родственника в районе Газиемир, передает РИА \"Новости\" . ТАСС напоминает, что 23 сентября власти Турции задержали племянницу Гюлена Эмине. Задержание прошло в уезде Эрдемит западной провинции Балыкесир. Выяснилось, что она значительную часть телефонных разговоров вела с одним абонентом в США. Кроме того, у нее изъято большое количество фотографий и книг Гюлена. В августе полиция задержала племянника Гюлена Кемаля Гюлена, телеведущего и адвоката. Он был задержан в одной из деревень в провинции Кастамону, где скрывался с середины июля после провала заговора. Еще раньше был задержан другой племянник проповедника Адбуллах Коруджук. В ночь на 16 июля в Турции группа мятежников совершила попытку военного переворота. Основное противостояние развернулось в Анкаре и Стамбуле. Погибли более 240 турецких граждан, более 2 тысяч человек получили ранения, мятеж был подавлен. Власти Турции обвинили Гюлена в причастности к попытке переворота и потребовали от США его экстрадиции. Сам Гюлен осудил мятеж и заявил о своей непричастности. По обвинению в причастности к организации Гюлена в Турции после мятежа были арестованы около 32 тысяч человек.',\n",
       " 'Aizvērt karti Высокие потолки, нужен ремонт-всё подготовлено для капитального ремонта, окна на одну сторону но шума нет, газовое отопление-колонка Viessmann-выравненный платёж 68 euro в месяц, городские коммуникации, вода по счетчикам только за холодную, пластиковые окна, камины действующие, два сарая во дворе c занесением в земельную книгу (22, 77m2), земля в собственности. Pilsēta:',\n",
       " 'Теги Локомотив Руслан Пименов Юрий Семин Премьер-лига Россия Арсенал Тула Бывший нападающий «Локомотива» Руслан Пименов после матча 9-го тура премьер-лиги с «Арсеналом» (1:1) выразил мнение, что многие футболисты «железнодорожников» не отвечают требованиям главного тренера Юрия Семина. Футболист не стал конкретизировать. – Главный тренер дал указание футболистам подумать, как им играть лучше. Ожидаете какие-то меры со стороны Семина? — вопрос Пименову. – Команду нужно встряхнуть. Многие футболисты не отвечают требованиями Юрия Павловича. – Кто именно?']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts_corpus[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 316 ms\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "# создание векторизатора:\n",
    "# vectorizer = TfidfVectorizer(tokenizer=n_tokenizer, stop_words=n_stop_words)\n",
    "cv_news = CountVectorizer(tokenizer=n_tokenizer, stop_words=n_stop_words)\n",
    "\n",
    "# векторизуем корпус:\n",
    "news_corpus_cv = cv_news.fit_transform(news_texts_corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['-', '--'], ['-', '--'], 2)"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_fn = cv_news.get_feature_names()\n",
    "news_fn[:20], news_fn[-20:], len(news_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1, 0], dtype=int64), 2)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_ar = news_corpus_cv.toarray()\n",
    "news_ar[0][:40], len(news_ar[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'-': 1, '--': 0}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(zip(news_fn, news_ar[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0, 1, 1, 0, 1],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 0, 1, 0, 1]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_ar = corpus_cv.toarray()\n",
    "cv_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\envs\\pyTorch_1_6\\lib\\site-packages\\ipykernel_launcher.py:2: RuntimeWarning: invalid value encountered in true_divide\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "# нормализация:\n",
    "news_arn = news_ar / norm(news_ar, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1., nan, nan, ...,  1., nan,  1.],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       ...,\n",
       "       [ 1., nan, nan, ...,  1., nan,  1.],\n",
       "       [nan, nan, nan, ..., nan, nan, nan],\n",
       "       [ 1., nan, nan, ...,  1., nan,  1.]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sim_mx = news_arn @ news_arn.T\n",
    "news_sim_mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Уланова: чтобы охарактеризовать Гамову, достаточно одного слова — великая   22:02. Волейбол Либеро «Динамо Казань» Екатерина Уланова после прощального матча Екатерины Гамовой поделилась эмоциями, связанными с уходом Гамовой из спорта. «Слёзы на глаза накатываются, и мурашки по коже. Грустно, хотя понимаешь, конечно, что все мы рано или поздно будем уходить из спорта. Я очень счастливый человек, потому что мне удалось поиграть с Катей и в сборной, и в клубе. Какими словами я охарактеризовала бы Гамову? Мне достаточно одного слова – великая. И в жизни, и в спорте. Почему у нас не получилось шоу? Не знаю, что ответить на этот вопрос. Не мы решали. Сколько ни пытались сделать шоу в женском волейболе, не получается. Может быть, женский характер не позволяет раскрепоститься и сыграть в своё удовольствие. Да, сегодня была борьба, игра. Просто мы ещё не умеем делать шоу, не готовы к этому», — приводит слова Улановой «Спорт Бизнес Online».'"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_idx = 42\n",
    "news_texts_corpus[n_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_sim_mx[n_idx, :].argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0, 72, 71, 70, 69, 68, 67, 66, 65, 64, 63, 62, 61, 60, 59, 58, 57,\n",
       "       56, 55, 54, 53, 52, 73, 51, 74, 76, 97, 96, 95, 94, 93, 92, 91, 90,\n",
       "       89, 88, 87, 86, 85, 84, 83, 82, 81, 80, 79, 78, 77, 75, 50, 49, 48,\n",
       "       21, 20, 19, 18, 17, 16, 15, 14, 13, 12, 11, 10,  9,  8,  7,  6,  5,\n",
       "        4,  3,  2,  1, 22, 23, 24, 25, 47, 46, 45, 44, 43, 42, 41, 40, 39,\n",
       "       38, 98, 37, 35, 34, 33, 32, 31, 30, 29, 28, 27, 26, 36, 99],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sim_news_idx = news_sim_mx[n_idx, :].argsort()\n",
    "sim_news_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!-- rb: 6032?_SITEZONE=4 --><!-- /rb: 6032?_SITEZONE=4 --> <!-- rb: 6030?_SITEZONE=4 --><!-- /rb: 6030?_SITEZONE=4 --> <!-- rb: 6029?_SITEZONE=4 --><!-- /rb: 6029?_SITEZONE=4   \\nТеперь вы можете обсуждать последние новости и высказывать свое отношение к происходящему. 13:47 (мск) , источник: 112.ua В Черновцах водитель маршрутки, которому стало плохо, сбил пешехода и врезался в дом \\nПогиб пожилой человек, пострадали пассажиры.'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_texts_corpus[sim_news_idx[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
